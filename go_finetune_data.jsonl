{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: act\nSource File Path(s): ['act\\\\auth_test.go\\\\source_files\\\\auth.go']\nTest File Path: act\\auth_test.go\\auth_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: act\\auth_test.go\\source_files\\auth.go\n// Copyright 2024 The Gitea Authors. All rights reserved.\n// SPDX-License-Identifier: MIT\n\npackage common\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"net/http\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/golang-jwt/jwt/v5\"\n\tlog \"github.com/sirupsen/logrus\"\n)\n\ntype actionsClaims struct {\n\tjwt.RegisteredClaims\n\tScp    string `json:\"scp\"`\n\tTaskID int64\n\tRunID  int64\n\tJobID  int64\n\tAc     string `json:\"ac\"`\n}\n\ntype actionsCacheScope struct {\n\tScope      string\n\tPermission actionsCachePermission\n}\n\ntype actionsCachePermission int\n\nconst (\n\tactionsCachePermissionRead = 1 << iota\n\tactionsCachePermissionWrite\n)\n\nfunc CreateAuthorizationToken(taskID, runID, jobID int64) (string, error) {\n\tnow := time.Now()\n\n\tac, err := json.Marshal(&[]actionsCacheScope{\n\t\t{\n\t\t\tScope:      \"\",\n\t\t\tPermission: actionsCachePermissionWrite,\n\t\t},\n\t})\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\tclaims := actionsClaims{\n\t\tRegisteredClaims: jwt.RegisteredClaims{\n\t\t\tExpiresAt: jwt.NewNumericDate(now.Add(24 * time.Hour)),\n\t\t\tNotBefore: jwt.NewNumericDate(now),\n\t\t},\n\t\tScp:    fmt.Sprintf(\"Actions.Results:%d:%d\", runID, jobID),\n\t\tTaskID: taskID,\n\t\tRunID:  runID,\n\t\tJobID:  jobID,\n\t\tAc:     string(ac),\n\t}\n\ttoken := jwt.NewWithClaims(jwt.SigningMethodHS256, claims)\n\n\ttokenString, err := token.SignedString([]byte{})\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\treturn tokenString, nil\n}\n\nfunc ParseAuthorizationToken(req *http.Request) (int64, error) {\n\th := req.Header.Get(\"Authorization\")\n\tif h == \"\" {\n\t\treturn 0, nil\n\t}\n\n\tparts := strings.SplitN(h, \" \", 2)\n\tif len(parts) != 2 {\n\t\tlog.Errorf(\"split token failed: %s\", h)\n\t\treturn 0, fmt.Errorf(\"split token failed\")\n\t}\n\n\ttoken, err := jwt.ParseWithClaims(parts[1], &actionsClaims{}, func(t *jwt.Token) (any, error) {\n\t\tif _, ok := t.Method.(*jwt.SigningMethodHMAC); !ok {\n\t\t\treturn nil, fmt.Errorf(\"unexpected signing method: %v\", t.Header[\"alg\"])\n\t\t}\n\t\treturn []byte{}, nil\n\t})\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\tc, ok := token.Claims.(*actionsClaims)\n\tif !token.Valid || !ok {\n\t\treturn 0, fmt.Errorf(\"invalid token claim\")\n\t}\n\n\treturn c.TaskID, nil\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:47:29"}, {"role": "assistant", "content": "```go\n// Copyright 2024 The Gitea Authors. All rights reserved.\n// SPDX-License-Identifier: MIT\n\npackage common\n\nimport (\n\t\"encoding/json\"\n\t\"net/http\"\n\t\"testing\"\n\n\t\"github.com/golang-jwt/jwt/v5\"\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestCreateAuthorizationToken(t *testing.T) {\n\tvar taskID int64 = 23\n\ttoken, err := CreateAuthorizationToken(taskID, 1, 2)\n\tassert.Nil(t, err)\n\tassert.NotEqual(t, \"\", token)\n\tclaims := jwt.MapClaims{}\n\t_, err = jwt.ParseWithClaims(token, claims, func(_ *jwt.Token) (interface{}, error) {\n\t\treturn []byte{}, nil\n\t})\n\tassert.Nil(t, err)\n\tscp, ok := claims[\"scp\"]\n\tassert.True(t, ok, \"Has scp claim in jwt token\")\n\tassert.Contains(t, scp, \"Actions.Results:1:2\")\n\ttaskIDClaim, ok := claims[\"TaskID\"]\n\tassert.True(t, ok, \"Has TaskID claim in jwt token\")\n\tassert.Equal(t, float64(taskID), taskIDClaim, \"Supplied taskid must match stored one\")\n\tacClaim, ok := claims[\"ac\"]\n\tassert.True(t, ok, \"Has ac claim in jwt token\")\n\tac, ok := acClaim.(string)\n\tassert.True(t, ok, \"ac claim is a string for buildx gha cache\")\n\tscopes := []actionsCacheScope{}\n\terr = json.Unmarshal([]byte(ac), &scopes)\n\tassert.NoError(t, err, \"ac claim is a json list for buildx gha cache\")\n\tassert.GreaterOrEqual(t, len(scopes), 1, \"Expected at least one action cache scope for buildx gha cache\")\n}\n\nfunc TestParseAuthorizationToken(t *testing.T) {\n\tvar taskID int64 = 23\n\ttoken, err := CreateAuthorizationToken(taskID, 1, 2)\n\tassert.Nil(t, err)\n\tassert.NotEqual(t, \"\", token)\n\theaders := http.Header{}\n\theaders.Set(\"Authorization\", \"Bearer \"+token)\n\trTaskID, err := ParseAuthorizationToken(&http.Request{\n\t\tHeader: headers,\n\t})\n\tassert.Nil(t, err)\n\tassert.Equal(t, taskID, rTaskID)\n}\n\nfunc TestParseAuthorizationTokenNoAuthHeader(t *testing.T) {\n\theaders := http.Header{}\n\trTaskID, err := ParseAuthorizationToken(&http.Request{\n\t\tHeader: headers,\n\t})\n\tassert.Nil(t, err)\n\tassert.Equal(t, int64(0), rTaskID)\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: act\nSource File Path(s): ['act\\\\cartesian_test.go\\\\source_files\\\\cartesian.go']\nTest File Path: act\\cartesian_test.go\\cartesian_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: act\\cartesian_test.go\\source_files\\cartesian.go\npackage common\n\n// CartesianProduct takes map of lists and returns list of unique tuples\nfunc CartesianProduct(mapOfLists map[string][]interface{}) []map[string]interface{} {\n\tlistNames := make([]string, 0)\n\tlists := make([][]interface{}, 0)\n\tfor k, v := range mapOfLists {\n\t\tlistNames = append(listNames, k)\n\t\tlists = append(lists, v)\n\t}\n\n\tlistCart := cartN(lists...)\n\n\trtn := make([]map[string]interface{}, 0)\n\tfor _, list := range listCart {\n\t\tvMap := make(map[string]interface{})\n\t\tfor i, v := range list {\n\t\t\tvMap[listNames[i]] = v\n\t\t}\n\t\trtn = append(rtn, vMap)\n\t}\n\treturn rtn\n}\n\nfunc cartN(a ...[]interface{}) [][]interface{} {\n\tc := 1\n\tfor _, a := range a {\n\t\tc *= len(a)\n\t}\n\tif c == 0 || len(a) == 0 {\n\t\treturn nil\n\t}\n\tp := make([][]interface{}, c)\n\tb := make([]interface{}, c*len(a))\n\tn := make([]int, len(a))\n\ts := 0\n\tfor i := range p {\n\t\te := s + len(a)\n\t\tpi := b[s:e]\n\t\tp[i] = pi\n\t\ts = e\n\t\tfor j, n := range n {\n\t\t\tpi[j] = a[j][n]\n\t\t}\n\t\tfor j := len(n) - 1; j >= 0; j-- {\n\t\t\tn[j]++\n\t\t\tif n[j] < len(a[j]) {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tn[j] = 0\n\t\t}\n\t}\n\treturn p\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:47:29"}, {"role": "assistant", "content": "```go\npackage common\n\nimport (\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestCartesianProduct(t *testing.T) {\n\tassert := assert.New(t)\n\tinput := map[string][]interface{}{\n\t\t\"foo\": {1, 2, 3, 4},\n\t\t\"bar\": {\"a\", \"b\", \"c\"},\n\t\t\"baz\": {false, true},\n\t}\n\n\toutput := CartesianProduct(input)\n\tassert.Len(output, 24)\n\n\tfor _, v := range output {\n\t\tassert.Len(v, 3)\n\n\t\tassert.Contains(v, \"foo\")\n\t\tassert.Contains(v, \"bar\")\n\t\tassert.Contains(v, \"baz\")\n\t}\n\n\tinput = map[string][]interface{}{\n\t\t\"foo\": {1, 2, 3, 4},\n\t\t\"bar\": {},\n\t\t\"baz\": {false, true},\n\t}\n\toutput = CartesianProduct(input)\n\tassert.Len(output, 0)\n\n\tinput = map[string][]interface{}{}\n\toutput = CartesianProduct(input)\n\tassert.Len(output, 0)\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: act\nSource File Path(s): ['act\\\\executor_test.go\\\\source_files\\\\executor.go']\nTest File Path: act\\executor_test.go\\executor_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: act\\executor_test.go\\source_files\\executor.go\npackage common\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\n\tlog \"github.com/sirupsen/logrus\"\n)\n\n// Warning that implements `error` but safe to ignore\ntype Warning struct {\n\tMessage string\n}\n\n// Error the contract for error\nfunc (w Warning) Error() string {\n\treturn w.Message\n}\n\n// Warningf create a warning\nfunc Warningf(format string, args ...interface{}) Warning {\n\tw := Warning{\n\t\tMessage: fmt.Sprintf(format, args...),\n\t}\n\treturn w\n}\n\n// Executor define contract for the steps of a workflow\ntype Executor func(ctx context.Context) error\n\n// Conditional define contract for the conditional predicate\ntype Conditional func(ctx context.Context) bool\n\n// NewInfoExecutor is an executor that logs messages\nfunc NewInfoExecutor(format string, args ...interface{}) Executor {\n\treturn func(ctx context.Context) error {\n\t\tlogger := Logger(ctx)\n\t\tlogger.Infof(format, args...)\n\t\treturn nil\n\t}\n}\n\n// NewDebugExecutor is an executor that logs messages\nfunc NewDebugExecutor(format string, args ...interface{}) Executor {\n\treturn func(ctx context.Context) error {\n\t\tlogger := Logger(ctx)\n\t\tlogger.Debugf(format, args...)\n\t\treturn nil\n\t}\n}\n\n// NewPipelineExecutor creates a new executor from a series of other executors\nfunc NewPipelineExecutor(executors ...Executor) Executor {\n\tif len(executors) == 0 {\n\t\treturn func(_ context.Context) error {\n\t\t\treturn nil\n\t\t}\n\t}\n\tvar rtn Executor\n\tfor _, executor := range executors {\n\t\tif rtn == nil {\n\t\t\trtn = executor\n\t\t} else {\n\t\t\trtn = rtn.Then(executor)\n\t\t}\n\t}\n\treturn rtn\n}\n\n// NewConditionalExecutor creates a new executor based on conditions\nfunc NewConditionalExecutor(conditional Conditional, trueExecutor Executor, falseExecutor Executor) Executor {\n\treturn func(ctx context.Context) error {\n\t\tif conditional(ctx) {\n\t\t\tif trueExecutor != nil {\n\t\t\t\treturn trueExecutor(ctx)\n\t\t\t}\n\t\t} else {\n\t\t\tif falseExecutor != nil {\n\t\t\t\treturn falseExecutor(ctx)\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t}\n}\n\n// NewErrorExecutor creates a new executor that always errors out\nfunc NewErrorExecutor(err error) Executor {\n\treturn func(_ context.Context) error {\n\t\treturn err\n\t}\n}\n\n// NewParallelExecutor creates a new executor from a parallel of other executors\nfunc NewParallelExecutor(parallel int, executors ...Executor) Executor {\n\treturn func(ctx context.Context) error {\n\t\twork := make(chan Executor, len(executors))\n\t\terrs := make(chan error, len(executors))\n\n\t\tif 1 > parallel {\n\t\t\tlog.Debugf(\"Parallel tasks (%d) below minimum, setting to 1\", parallel)\n\t\t\tparallel = 1\n\t\t}\n\n\t\tfor i := 0; i < parallel; i++ {\n\t\t\tgo func(work <-chan Executor, errs chan<- error) {\n\t\t\t\tfor executor := range work {\n\t\t\t\t\terrs <- executor(ctx)\n\t\t\t\t}\n\t\t\t}(work, errs)\n\t\t}\n\n\t\tfor i := 0; i < len(executors); i++ {\n\t\t\twork <- executors[i]\n\t\t}\n\t\tclose(work)\n\n\t\t// Executor waits all executors to cleanup these resources.\n\t\tvar firstErr error\n\t\tfor i := 0; i < len(executors); i++ {\n\t\t\terr := <-errs\n\t\t\tif firstErr == nil {\n\t\t\t\tfirstErr = err\n\t\t\t}\n\t\t}\n\n\t\tif err := ctx.Err(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn firstErr\n\t}\n}\n\nfunc NewFieldExecutor(name string, value interface{}, exec Executor) Executor {\n\treturn func(ctx context.Context) error {\n\t\treturn exec(WithLogger(ctx, Logger(ctx).WithField(name, value)))\n\t}\n}\n\n// Then runs another executor if this executor succeeds\nfunc (e Executor) ThenError(then func(ctx context.Context, err error) error) Executor {\n\treturn func(ctx context.Context) error {\n\t\terr := e(ctx)\n\t\tif err != nil {\n\t\t\tswitch err.(type) {\n\t\t\tcase Warning:\n\t\t\t\tLogger(ctx).Warning(err.Error())\n\t\t\tdefault:\n\t\t\t\treturn then(ctx, err)\n\t\t\t}\n\t\t}\n\t\tif ctx.Err() != nil {\n\t\t\treturn ctx.Err()\n\t\t}\n\t\treturn then(ctx, err)\n\t}\n}\n\n// Then runs another executor if this executor succeeds\nfunc (e Executor) Then(then Executor) Executor {\n\treturn func(ctx context.Context) error {\n\t\terr := e(ctx)\n\t\tif err != nil {\n\t\t\tswitch err.(type) {\n\t\t\tcase Warning:\n\t\t\t\tLogger(ctx).Warning(err.Error())\n\t\t\tdefault:\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tif ctx.Err() != nil {\n\t\t\treturn ctx.Err()\n\t\t}\n\t\treturn then(ctx)\n\t}\n}\n\n// Then runs another executor if this executor succeeds\nfunc (e Executor) OnError(then Executor) Executor {\n\treturn func(ctx context.Context) error {\n\t\terr := e(ctx)\n\t\tif err != nil {\n\t\t\tswitch err.(type) {\n\t\t\tcase Warning:\n\t\t\t\tLogger(ctx).Warning(err.Error())\n\t\t\tdefault:\n\t\t\t\treturn errors.Join(err, then(ctx))\n\t\t\t}\n\t\t}\n\t\tif ctx.Err() != nil {\n\t\t\treturn ctx.Err()\n\t\t}\n\t\treturn nil\n\t}\n}\n\n// If only runs this executor if conditional is true\nfunc (e Executor) If(conditional Conditional) Executor {\n\treturn func(ctx context.Context) error {\n\t\tif conditional(ctx) {\n\t\t\treturn e(ctx)\n\t\t}\n\t\treturn nil\n\t}\n}\n\n// IfNot only runs this executor if conditional is true\nfunc (e Executor) IfNot(conditional Conditional) Executor {\n\treturn func(ctx context.Context) error {\n\t\tif !conditional(ctx) {\n\t\t\treturn e(ctx)\n\t\t}\n\t\treturn nil\n\t}\n}\n\n// IfBool only runs this executor if conditional is true\nfunc (e Executor) IfBool(conditional bool) Executor {\n\treturn e.If(func(_ context.Context) bool {\n\t\treturn conditional\n\t})\n}\n\n// Finally adds an executor to run after other executor\nfunc (e Executor) Finally(finally Executor) Executor {\n\treturn func(ctx context.Context) error {\n\t\terr := e(ctx)\n\t\terr2 := finally(ctx)\n\t\tif err2 != nil {\n\t\t\treturn fmt.Errorf(\"Error occurred running finally: %v (original error: %v)\", err2, err)\n\t\t}\n\t\treturn err\n\t}\n}\n\n// Not return an inverted conditional\nfunc (c Conditional) Not() Conditional {\n\treturn func(ctx context.Context) bool {\n\t\treturn !c(ctx)\n\t}\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:47:29"}, {"role": "assistant", "content": "```go\npackage common\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestNewWorkflow(t *testing.T) {\n\tassert := assert.New(t)\n\n\tctx := context.Background()\n\n\t// empty\n\temptyWorkflow := NewPipelineExecutor()\n\tassert.Nil(emptyWorkflow(ctx))\n\n\t// error case\n\terrorWorkflow := NewErrorExecutor(fmt.Errorf(\"test error\"))\n\tassert.NotNil(errorWorkflow(ctx))\n\n\t// multiple success case\n\truncount := 0\n\tsuccessWorkflow := NewPipelineExecutor(\n\t\tfunc(_ context.Context) error {\n\t\t\truncount++\n\t\t\treturn nil\n\t\t},\n\t\tfunc(_ context.Context) error {\n\t\t\truncount++\n\t\t\treturn nil\n\t\t})\n\tassert.Nil(successWorkflow(ctx))\n\tassert.Equal(2, runcount)\n}\n\nfunc TestNewConditionalExecutor(t *testing.T) {\n\tassert := assert.New(t)\n\n\tctx := context.Background()\n\n\ttrueCount := 0\n\tfalseCount := 0\n\n\terr := NewConditionalExecutor(func(_ context.Context) bool {\n\t\treturn false\n\t}, func(_ context.Context) error {\n\t\ttrueCount++\n\t\treturn nil\n\t}, func(_ context.Context) error {\n\t\tfalseCount++\n\t\treturn nil\n\t})(ctx)\n\n\tassert.Nil(err)\n\tassert.Equal(0, trueCount)\n\tassert.Equal(1, falseCount)\n\n\terr = NewConditionalExecutor(func(_ context.Context) bool {\n\t\treturn true\n\t}, func(_ context.Context) error {\n\t\ttrueCount++\n\t\treturn nil\n\t}, func(_ context.Context) error {\n\t\tfalseCount++\n\t\treturn nil\n\t})(ctx)\n\n\tassert.Nil(err)\n\tassert.Equal(1, trueCount)\n\tassert.Equal(1, falseCount)\n}\n\nfunc TestNewParallelExecutor(t *testing.T) {\n\tassert := assert.New(t)\n\n\tctx := context.Background()\n\n\tcount := 0\n\tactiveCount := 0\n\tmaxCount := 0\n\temptyWorkflow := NewPipelineExecutor(func(_ context.Context) error {\n\t\tcount++\n\n\t\tactiveCount++\n\t\tif activeCount > maxCount {\n\t\t\tmaxCount = activeCount\n\t\t}\n\t\ttime.Sleep(2 * time.Second)\n\t\tactiveCount--\n\n\t\treturn nil\n\t})\n\n\terr := NewParallelExecutor(2, emptyWorkflow, emptyWorkflow, emptyWorkflow)(ctx)\n\n\tassert.Equal(3, count, \"should run all 3 executors\")\n\tassert.Equal(2, maxCount, \"should run at most 2 executors in parallel\")\n\tassert.Nil(err)\n\n\t// Reset to test running the executor with 0 parallelism\n\tcount = 0\n\tactiveCount = 0\n\tmaxCount = 0\n\n\terrSingle := NewParallelExecutor(0, emptyWorkflow, emptyWorkflow, emptyWorkflow)(ctx)\n\n\tassert.Equal(3, count, \"should run all 3 executors\")\n\tassert.Equal(1, maxCount, \"should run at most 1 executors in parallel\")\n\tassert.Nil(errSingle)\n}\n\nfunc TestNewParallelExecutorFailed(t *testing.T) {\n\tassert := assert.New(t)\n\n\tctx, cancel := context.WithCancel(context.Background())\n\tcancel()\n\n\tcount := 0\n\terrorWorkflow := NewPipelineExecutor(func(_ context.Context) error {\n\t\tcount++\n\t\treturn fmt.Errorf(\"fake error\")\n\t})\n\terr := NewParallelExecutor(1, errorWorkflow)(ctx)\n\tassert.Equal(1, count)\n\tassert.ErrorIs(context.Canceled, err)\n}\n\nfunc TestNewParallelExecutorCanceled(t *testing.T) {\n\tassert := assert.New(t)\n\n\tctx, cancel := context.WithCancel(context.Background())\n\tcancel()\n\n\terrExpected := fmt.Errorf(\"fake error\")\n\n\tcount := 0\n\tsuccessWorkflow := NewPipelineExecutor(func(_ context.Context) error {\n\t\tcount++\n\t\treturn nil\n\t})\n\terrorWorkflow := NewPipelineExecutor(func(_ context.Context) error {\n\t\tcount++\n\t\treturn errExpected\n\t})\n\terr := NewParallelExecutor(3, errorWorkflow, successWorkflow, successWorkflow)(ctx)\n\tassert.Equal(3, count)\n\tassert.Error(errExpected, err)\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: act\nSource File Path(s): ['act\\\\file_collector_test.go\\\\source_files\\\\file_collector.go']\nTest File Path: act\\file_collector_test.go\\file_collector_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: act\\file_collector_test.go\\source_files\\file_collector.go\npackage filecollector\n\nimport (\n\t\"archive/tar\"\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"io/fs\"\n\t\"os\"\n\t\"path\"\n\t\"path/filepath\"\n\t\"strings\"\n\n\tgit \"github.com/go-git/go-git/v5\"\n\t\"github.com/go-git/go-git/v5/plumbing/filemode\"\n\t\"github.com/go-git/go-git/v5/plumbing/format/gitignore\"\n\t\"github.com/go-git/go-git/v5/plumbing/format/index\"\n)\n\ntype Handler interface {\n\tWriteFile(path string, fi fs.FileInfo, linkName string, f io.Reader) error\n}\n\ntype TarCollector struct {\n\tTarWriter *tar.Writer\n\tUID       int\n\tGID       int\n\tDstDir    string\n}\n\nfunc (tc TarCollector) WriteFile(fpath string, fi fs.FileInfo, linkName string, f io.Reader) error {\n\t// create a new dir/file header\n\theader, err := tar.FileInfoHeader(fi, linkName)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// update the name to correctly reflect the desired destination when untaring\n\theader.Name = path.Join(tc.DstDir, fpath)\n\theader.Mode = int64(fi.Mode())\n\theader.ModTime = fi.ModTime()\n\theader.Uid = tc.UID\n\theader.Gid = tc.GID\n\n\t// write the header\n\tif err := tc.TarWriter.WriteHeader(header); err != nil {\n\t\treturn err\n\t}\n\n\t// this is a symlink no reader provided\n\tif f == nil {\n\t\treturn nil\n\t}\n\n\t// copy file data into tar writer\n\tif _, err := io.Copy(tc.TarWriter, f); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\ntype CopyCollector struct {\n\tDstDir string\n}\n\nfunc (cc *CopyCollector) WriteFile(fpath string, fi fs.FileInfo, linkName string, f io.Reader) error {\n\tfdestpath := filepath.Join(cc.DstDir, fpath)\n\tif err := os.MkdirAll(filepath.Dir(fdestpath), 0o777); err != nil {\n\t\treturn err\n\t}\n\tif linkName != \"\" {\n\t\treturn os.Symlink(linkName, fdestpath)\n\t}\n\tdf, err := os.OpenFile(fdestpath, os.O_CREATE|os.O_WRONLY, fi.Mode())\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer df.Close()\n\tif _, err := io.Copy(df, f); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\ntype FileCollector struct {\n\tIgnorer   gitignore.Matcher\n\tSrcPath   string\n\tSrcPrefix string\n\tFs        Fs\n\tHandler   Handler\n}\n\ntype Fs interface {\n\tWalk(root string, fn filepath.WalkFunc) error\n\tOpenGitIndex(path string) (*index.Index, error)\n\tOpen(path string) (io.ReadCloser, error)\n\tReadlink(path string) (string, error)\n}\n\ntype DefaultFs struct {\n}\n\nfunc (*DefaultFs) Walk(root string, fn filepath.WalkFunc) error {\n\treturn filepath.Walk(root, fn)\n}\n\nfunc (*DefaultFs) OpenGitIndex(path string) (*index.Index, error) {\n\tr, err := git.PlainOpen(path)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ti, err := r.Storer.Index()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn i, nil\n}\n\nfunc (*DefaultFs) Open(path string) (io.ReadCloser, error) {\n\treturn os.Open(path)\n}\n\nfunc (*DefaultFs) Readlink(path string) (string, error) {\n\treturn os.Readlink(path)\n}\n\n//nolint:gocyclo\nfunc (fc *FileCollector) CollectFiles(ctx context.Context, submodulePath []string) filepath.WalkFunc {\n\ti, _ := fc.Fs.OpenGitIndex(path.Join(fc.SrcPath, path.Join(submodulePath...)))\n\treturn func(file string, fi os.FileInfo, err error) error {\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif ctx != nil {\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn fmt.Errorf(\"copy cancelled\")\n\t\t\tdefault:\n\t\t\t}\n\t\t}\n\n\t\tsansPrefix := strings.TrimPrefix(file, fc.SrcPrefix)\n\t\tsplit := strings.Split(sansPrefix, string(filepath.Separator))\n\t\t// The root folders should be skipped, submodules only have the last path component set to \".\" by filepath.Walk\n\t\tif fi.IsDir() && len(split) > 0 && split[len(split)-1] == \".\" {\n\t\t\treturn nil\n\t\t}\n\t\tvar entry *index.Entry\n\t\tif i != nil {\n\t\t\tentry, err = i.Entry(strings.Join(split[len(submodulePath):], \"/\"))\n\t\t} else {\n\t\t\terr = index.ErrEntryNotFound\n\t\t}\n\t\tif err != nil && fc.Ignorer != nil && fc.Ignorer.Match(split, fi.IsDir()) {\n\t\t\tif fi.IsDir() {\n\t\t\t\tif i != nil {\n\t\t\t\t\tms, err := i.Glob(strings.Join(append(split[len(submodulePath):], \"**\"), \"/\"))\n\t\t\t\t\tif err != nil || len(ms) == 0 {\n\t\t\t\t\t\treturn filepath.SkipDir\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\treturn filepath.SkipDir\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\t\tif err == nil && entry.Mode == filemode.Submodule {\n\t\t\terr = fc.Fs.Walk(file, fc.CollectFiles(ctx, split))\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\treturn filepath.SkipDir\n\t\t}\n\t\tpath := filepath.ToSlash(sansPrefix)\n\n\t\t// return on non-regular files (thanks to [kumo](https://medium.com/@komuw/just-like-you-did-fbdd7df829d3) for this suggested update)\n\t\tif fi.Mode()&os.ModeSymlink == os.ModeSymlink {\n\t\t\tlinkName, err := fc.Fs.Readlink(file)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"unable to readlink '%s': %w\", file, err)\n\t\t\t}\n\t\t\treturn fc.Handler.WriteFile(path, fi, linkName, nil)\n\t\t} else if !fi.Mode().IsRegular() {\n\t\t\treturn nil\n\t\t}\n\n\t\t// open file\n\t\tf, err := fc.Fs.Open(file)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdefer f.Close()\n\n\t\tif ctx != nil {\n\t\t\t// make io.Copy cancellable by closing the file\n\t\t\tcpctx, cpfinish := context.WithCancel(ctx)\n\t\t\tdefer cpfinish()\n\t\t\tgo func() {\n\t\t\t\tselect {\n\t\t\t\tcase <-cpctx.Done():\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\tf.Close()\n\t\t\t\t}\n\t\t\t}()\n\t\t}\n\n\t\treturn fc.Handler.WriteFile(path, fi, \"\", f)\n\t}\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:47:29"}, {"role": "assistant", "content": "```go\npackage filecollector\n\nimport (\n\t\"archive/tar\"\n\t\"context\"\n\t\"io\"\n\t\"path/filepath\"\n\t\"strings\"\n\t\"testing\"\n\n\t\"github.com/go-git/go-billy/v5\"\n\t\"github.com/go-git/go-billy/v5/memfs\"\n\tgit \"github.com/go-git/go-git/v5\"\n\t\"github.com/go-git/go-git/v5/plumbing/cache\"\n\t\"github.com/go-git/go-git/v5/plumbing/format/gitignore\"\n\t\"github.com/go-git/go-git/v5/plumbing/format/index\"\n\t\"github.com/go-git/go-git/v5/storage/filesystem\"\n\t\"github.com/stretchr/testify/assert\"\n)\n\ntype memoryFs struct {\n\tbilly.Filesystem\n}\n\nfunc (mfs *memoryFs) walk(root string, fn filepath.WalkFunc) error {\n\tdir, err := mfs.ReadDir(root)\n\tif err != nil {\n\t\treturn err\n\t}\n\tfor i := 0; i < len(dir); i++ {\n\t\tfilename := filepath.Join(root, dir[i].Name())\n\t\terr = fn(filename, dir[i], nil)\n\t\tif dir[i].IsDir() {\n\t\t\tif err == filepath.SkipDir {\n\t\t\t\terr = nil\n\t\t\t} else if err := mfs.walk(filename, fn); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (mfs *memoryFs) Walk(root string, fn filepath.WalkFunc) error {\n\tstat, err := mfs.Lstat(root)\n\tif err != nil {\n\t\treturn err\n\t}\n\terr = fn(strings.Join([]string{root, \".\"}, string(filepath.Separator)), stat, nil)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn mfs.walk(root, fn)\n}\n\nfunc (mfs *memoryFs) OpenGitIndex(path string) (*index.Index, error) {\n\tf, _ := mfs.Filesystem.Chroot(filepath.Join(path, \".git\"))\n\tstorage := filesystem.NewStorage(f, cache.NewObjectLRUDefault())\n\ti, err := storage.Index()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn i, nil\n}\n\nfunc (mfs *memoryFs) Open(path string) (io.ReadCloser, error) {\n\treturn mfs.Filesystem.Open(path)\n}\n\nfunc (mfs *memoryFs) Readlink(path string) (string, error) {\n\treturn mfs.Filesystem.Readlink(path)\n}\n\nfunc TestIgnoredTrackedfile(t *testing.T) {\n\tfs := memfs.New()\n\t_ = fs.MkdirAll(\"mygitrepo/.git\", 0o777)\n\tdotgit, _ := fs.Chroot(\"mygitrepo/.git\")\n\tworktree, _ := fs.Chroot(\"mygitrepo\")\n\trepo, _ := git.Init(filesystem.NewStorage(dotgit, cache.NewObjectLRUDefault()), worktree)\n\tf, _ := worktree.Create(\".gitignore\")\n\t_, _ = f.Write([]byte(\".*\\n\"))\n\tf.Close()\n\t// This file shouldn't be in the tar\n\tf, _ = worktree.Create(\".env\")\n\t_, _ = f.Write([]byte(\"test=val1\\n\"))\n\tf.Close()\n\tw, _ := repo.Worktree()\n\t// .gitignore is in the tar after adding it to the index\n\t_, _ = w.Add(\".gitignore\")\n\n\ttmpTar, _ := fs.Create(\"temp.tar\")\n\ttw := tar.NewWriter(tmpTar)\n\tps, _ := gitignore.ReadPatterns(worktree, []string{})\n\tignorer := gitignore.NewMatcher(ps)\n\tfc := &FileCollector{\n\t\tFs:        &memoryFs{Filesystem: fs},\n\t\tIgnorer:   ignorer,\n\t\tSrcPath:   \"mygitrepo\",\n\t\tSrcPrefix: \"mygitrepo\" + string(filepath.Separator),\n\t\tHandler: &TarCollector{\n\t\t\tTarWriter: tw,\n\t\t},\n\t}\n\terr := fc.Fs.Walk(\"mygitrepo\", fc.CollectFiles(context.Background(), []string{}))\n\tassert.NoError(t, err, \"successfully collect files\")\n\ttw.Close()\n\t_, _ = tmpTar.Seek(0, io.SeekStart)\n\ttr := tar.NewReader(tmpTar)\n\th, err := tr.Next()\n\tassert.NoError(t, err, \"tar must not be empty\")\n\tassert.Equal(t, \".gitignore\", h.Name)\n\t_, err = tr.Next()\n\tassert.ErrorIs(t, err, io.EOF, \"tar must only contain one element\")\n}\n\nfunc TestSymlinks(t *testing.T) {\n\tfs := memfs.New()\n\t_ = fs.MkdirAll(\"mygitrepo/.git\", 0o777)\n\tdotgit, _ := fs.Chroot(\"mygitrepo/.git\")\n\tworktree, _ := fs.Chroot(\"mygitrepo\")\n\trepo, _ := git.Init(filesystem.NewStorage(dotgit, cache.NewObjectLRUDefault()), worktree)\n\t// This file shouldn't be in the tar\n\tf, err := worktree.Create(\".env\")\n\tassert.NoError(t, err)\n\t_, err = f.Write([]byte(\"test=val1\\n\"))\n\tassert.NoError(t, err)\n\tf.Close()\n\terr = worktree.Symlink(\".env\", \"test.env\")\n\tassert.NoError(t, err)\n\n\tw, err := repo.Worktree()\n\tassert.NoError(t, err)\n\n\t// .gitignore is in the tar after adding it to the index\n\t_, err = w.Add(\".env\")\n\tassert.NoError(t, err)\n\t_, err = w.Add(\"test.env\")\n\tassert.NoError(t, err)\n\n\ttmpTar, _ := fs.Create(\"temp.tar\")\n\ttw := tar.NewWriter(tmpTar)\n\tps, _ := gitignore.ReadPatterns(worktree, []string{})\n\tignorer := gitignore.NewMatcher(ps)\n\tfc := &FileCollector{\n\t\tFs:        &memoryFs{Filesystem: fs},\n\t\tIgnorer:   ignorer,\n\t\tSrcPath:   \"mygitrepo\",\n\t\tSrcPrefix: \"mygitrepo\" + string(filepath.Separator),\n\t\tHandler: &TarCollector{\n\t\t\tTarWriter: tw,\n\t\t},\n\t}\n\terr = fc.Fs.Walk(\"mygitrepo\", fc.CollectFiles(context.Background(), []string{}))\n\tassert.NoError(t, err, \"successfully collect files\")\n\ttw.Close()\n\t_, _ = tmpTar.Seek(0, io.SeekStart)\n\ttr := tar.NewReader(tmpTar)\n\th, err := tr.Next()\n\tfiles := map[string]tar.Header{}\n\tfor err == nil {\n\t\tfiles[h.Name] = *h\n\t\th, err = tr.Next()\n\t}\n\n\tassert.Equal(t, \".env\", files[\".env\"].Name)\n\tassert.Equal(t, \"test.env\", files[\"test.env\"].Name)\n\tassert.Equal(t, \".env\", files[\"test.env\"].Linkname)\n\tassert.ErrorIs(t, err, io.EOF, \"tar must be read cleanly to EOF\")\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: act\nSource File Path(s): ['act\\\\line_writer_test.go\\\\source_files\\\\line_writer.go']\nTest File Path: act\\line_writer_test.go\\line_writer_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: act\\line_writer_test.go\\source_files\\line_writer.go\npackage common\n\nimport (\n\t\"bytes\"\n\t\"io\"\n)\n\n// LineHandler is a callback function for handling a line\ntype LineHandler func(line string) bool\n\ntype lineWriter struct {\n\tbuffer   bytes.Buffer\n\thandlers []LineHandler\n}\n\n// NewLineWriter creates a new instance of a line writer\nfunc NewLineWriter(handlers ...LineHandler) io.Writer {\n\tw := new(lineWriter)\n\tw.handlers = handlers\n\treturn w\n}\n\nfunc (lw *lineWriter) Write(p []byte) (n int, err error) {\n\tpBuf := bytes.NewBuffer(p)\n\twritten := 0\n\tfor {\n\t\tline, err := pBuf.ReadString('\\n')\n\t\tw, _ := lw.buffer.WriteString(line)\n\t\twritten += w\n\t\tif err == nil {\n\t\t\tlw.handleLine(lw.buffer.String())\n\t\t\tlw.buffer.Reset()\n\t\t} else if err == io.EOF {\n\t\t\tbreak\n\t\t} else {\n\t\t\treturn written, err\n\t\t}\n\t}\n\n\treturn written, nil\n}\n\nfunc (lw *lineWriter) handleLine(line string) {\n\tfor _, h := range lw.handlers {\n\t\tok := h(line)\n\t\tif !ok {\n\t\t\tbreak\n\t\t}\n\t}\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:47:29"}, {"role": "assistant", "content": "```go\npackage common\n\nimport (\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestLineWriter(t *testing.T) {\n\tlines := make([]string, 0)\n\tlineHandler := func(s string) bool {\n\t\tlines = append(lines, s)\n\t\treturn true\n\t}\n\n\tlineWriter := NewLineWriter(lineHandler)\n\n\tassert := assert.New(t)\n\twrite := func(s string) {\n\t\tn, err := lineWriter.Write([]byte(s))\n\t\tassert.NoError(err)\n\t\tassert.Equal(len(s), n, s)\n\t}\n\n\twrite(\"hello\")\n\twrite(\" \")\n\twrite(\"world!!\\nextra\")\n\twrite(\" line\\n and another\\nlast\")\n\twrite(\" line\\n\")\n\twrite(\"no newline here...\")\n\n\tassert.Len(lines, 4)\n\tassert.Equal(\"hello world!!\\n\", lines[0])\n\tassert.Equal(\"extra line\\n\", lines[1])\n\tassert.Equal(\" and another\\n\", lines[2])\n\tassert.Equal(\"last line\\n\", lines[3])\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: act\nSource File Path(s): ['act\\\\planner_test.go\\\\source_files\\\\planner.go']\nTest File Path: act\\planner_test.go\\planner_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: act\\planner_test.go\\source_files\\planner.go\npackage model\n\nimport (\n\t\"fmt\"\n\t\"io\"\n\t\"io/fs\"\n\t\"math\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"regexp\"\n\t\"sort\"\n\n\tlog \"github.com/sirupsen/logrus\"\n)\n\n// WorkflowPlanner contains methods for creating plans\ntype WorkflowPlanner interface {\n\tPlanEvent(eventName string) (*Plan, error)\n\tPlanJob(jobName string) (*Plan, error)\n\tPlanAll() (*Plan, error)\n\tGetEvents() []string\n}\n\n// Plan contains a list of stages to run in series\ntype Plan struct {\n\tStages []*Stage\n}\n\n// Stage contains a list of runs to execute in parallel\ntype Stage struct {\n\tRuns []*Run\n}\n\n// Run represents a job from a workflow that needs to be run\ntype Run struct {\n\tWorkflow *Workflow\n\tJobID    string\n}\n\nfunc (r *Run) String() string {\n\tjobName := r.Job().Name\n\tif jobName == \"\" {\n\t\tjobName = r.JobID\n\t}\n\treturn jobName\n}\n\n// Job returns the job for this Run\nfunc (r *Run) Job() *Job {\n\treturn r.Workflow.GetJob(r.JobID)\n}\n\ntype WorkflowFiles struct {\n\tworkflowDirEntry os.DirEntry\n\tdirPath          string\n}\n\n// NewWorkflowPlanner will load a specific workflow, all workflows from a directory or all workflows from a directory and its subdirectories\nfunc NewWorkflowPlanner(path string, noWorkflowRecurse bool) (WorkflowPlanner, error) {\n\tpath, err := filepath.Abs(path)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfi, err := os.Stat(path)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar workflows []WorkflowFiles\n\n\tif fi.IsDir() {\n\t\tlog.Debugf(\"Loading workflows from '%s'\", path)\n\t\tif noWorkflowRecurse {\n\t\t\tfiles, err := os.ReadDir(path)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\tfor _, v := range files {\n\t\t\t\tworkflows = append(workflows, WorkflowFiles{\n\t\t\t\t\tdirPath:          path,\n\t\t\t\t\tworkflowDirEntry: v,\n\t\t\t\t})\n\t\t\t}\n\t\t} else {\n\t\t\tlog.Debug(\"Loading workflows recursively\")\n\t\t\tif err := filepath.Walk(path,\n\t\t\t\tfunc(p string, f os.FileInfo, err error) error {\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn err\n\t\t\t\t\t}\n\n\t\t\t\t\tif !f.IsDir() {\n\t\t\t\t\t\tlog.Debugf(\"Found workflow '%s' in '%s'\", f.Name(), p)\n\t\t\t\t\t\tworkflows = append(workflows, WorkflowFiles{\n\t\t\t\t\t\t\tdirPath:          filepath.Dir(p),\n\t\t\t\t\t\t\tworkflowDirEntry: fs.FileInfoToDirEntry(f),\n\t\t\t\t\t\t})\n\t\t\t\t\t}\n\n\t\t\t\t\treturn nil\n\t\t\t\t}); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t} else {\n\t\tlog.Debugf(\"Loading workflow '%s'\", path)\n\t\tdirname := filepath.Dir(path)\n\n\t\tworkflows = append(workflows, WorkflowFiles{\n\t\t\tdirPath:          dirname,\n\t\t\tworkflowDirEntry: fs.FileInfoToDirEntry(fi),\n\t\t})\n\t}\n\n\twp := new(workflowPlanner)\n\tfor _, wf := range workflows {\n\t\text := filepath.Ext(wf.workflowDirEntry.Name())\n\t\tif ext == \".yml\" || ext == \".yaml\" {\n\t\t\tf, err := os.Open(filepath.Join(wf.dirPath, wf.workflowDirEntry.Name()))\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\tlog.Debugf(\"Reading workflow '%s'\", f.Name())\n\t\t\tworkflow, err := ReadWorkflow(f)\n\t\t\tif err != nil {\n\t\t\t\t_ = f.Close()\n\t\t\t\tif err == io.EOF {\n\t\t\t\t\treturn nil, fmt.Errorf(\"unable to read workflow '%s': file is empty: %w\", wf.workflowDirEntry.Name(), err)\n\t\t\t\t}\n\t\t\t\treturn nil, fmt.Errorf(\"workflow is not valid. '%s': %w\", wf.workflowDirEntry.Name(), err)\n\t\t\t}\n\t\t\t_, err = f.Seek(0, 0)\n\t\t\tif err != nil {\n\t\t\t\t_ = f.Close()\n\t\t\t\treturn nil, fmt.Errorf(\"error occurring when resetting io pointer in '%s': %w\", wf.workflowDirEntry.Name(), err)\n\t\t\t}\n\n\t\t\tworkflow.File = wf.workflowDirEntry.Name()\n\t\t\tif workflow.Name == \"\" {\n\t\t\t\tworkflow.Name = wf.workflowDirEntry.Name()\n\t\t\t}\n\n\t\t\terr = validateJobName(workflow)\n\t\t\tif err != nil {\n\t\t\t\t_ = f.Close()\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\twp.workflows = append(wp.workflows, workflow)\n\t\t\t_ = f.Close()\n\t\t}\n\t}\n\n\treturn wp, nil\n}\n\nfunc NewSingleWorkflowPlanner(name string, f io.Reader) (WorkflowPlanner, error) {\n\twp := new(workflowPlanner)\n\n\tlog.Debugf(\"Reading workflow %s\", name)\n\tworkflow, err := ReadWorkflow(f)\n\tif err != nil {\n\t\tif err == io.EOF {\n\t\t\treturn nil, fmt.Errorf(\"unable to read workflow '%s': file is empty: %w\", name, err)\n\t\t}\n\t\treturn nil, fmt.Errorf(\"workflow is not valid. '%s': %w\", name, err)\n\t}\n\tworkflow.File = name\n\tif workflow.Name == \"\" {\n\t\tworkflow.Name = name\n\t}\n\n\terr = validateJobName(workflow)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\twp.workflows = append(wp.workflows, workflow)\n\n\treturn wp, nil\n}\n\nfunc validateJobName(workflow *Workflow) error {\n\tjobNameRegex := regexp.MustCompile(`^([[:alpha:]_][[:alnum:]_\\-]*)$`)\n\tfor k := range workflow.Jobs {\n\t\tif ok := jobNameRegex.MatchString(k); !ok {\n\t\t\treturn fmt.Errorf(\"workflow is not valid. '%s': Job name '%s' is invalid. Names must start with a letter or '_' and contain only alphanumeric characters, '-', or '_'\", workflow.Name, k)\n\t\t}\n\t}\n\treturn nil\n}\n\ntype workflowPlanner struct {\n\tworkflows []*Workflow\n}\n\n// PlanEvent builds a new list of runs to execute in parallel for an event name\nfunc (wp *workflowPlanner) PlanEvent(eventName string) (*Plan, error) {\n\tplan := new(Plan)\n\tif len(wp.workflows) == 0 {\n\t\tlog.Debug(\"no workflows found by planner\")\n\t\treturn plan, nil\n\t}\n\tvar lastErr error\n\n\tfor _, w := range wp.workflows {\n\t\tevents := w.On()\n\t\tif len(events) == 0 {\n\t\t\tlog.Debugf(\"no events found for workflow: %s\", w.File)\n\t\t\tcontinue\n\t\t}\n\n\t\tfor _, e := range events {\n\t\t\tif e == eventName {\n\t\t\t\tstages, err := createStages(w, w.GetJobIDs()...)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlog.Warn(err)\n\t\t\t\t\tlastErr = err\n\t\t\t\t} else {\n\t\t\t\t\tplan.mergeStages(stages)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn plan, lastErr\n}\n\n// PlanJob builds a new run to execute in parallel for a job name\nfunc (wp *workflowPlanner) PlanJob(jobName string) (*Plan, error) {\n\tplan := new(Plan)\n\tif len(wp.workflows) == 0 {\n\t\tlog.Debugf(\"no jobs found for workflow: %s\", jobName)\n\t}\n\tvar lastErr error\n\n\tfor _, w := range wp.workflows {\n\t\tstages, err := createStages(w, jobName)\n\t\tif err != nil {\n\t\t\tlog.Warn(err)\n\t\t\tlastErr = err\n\t\t} else {\n\t\t\tplan.mergeStages(stages)\n\t\t}\n\t}\n\treturn plan, lastErr\n}\n\n// PlanAll builds a new run to execute in parallel all\nfunc (wp *workflowPlanner) PlanAll() (*Plan, error) {\n\tplan := new(Plan)\n\tif len(wp.workflows) == 0 {\n\t\tlog.Debug(\"no workflows found by planner\")\n\t\treturn plan, nil\n\t}\n\tvar lastErr error\n\n\tfor _, w := range wp.workflows {\n\t\tstages, err := createStages(w, w.GetJobIDs()...)\n\t\tif err != nil {\n\t\t\tlog.Warn(err)\n\t\t\tlastErr = err\n\t\t} else {\n\t\t\tplan.mergeStages(stages)\n\t\t}\n\t}\n\n\treturn plan, lastErr\n}\n\n// GetEvents gets all the events in the workflows file\nfunc (wp *workflowPlanner) GetEvents() []string {\n\tevents := make([]string, 0)\n\tfor _, w := range wp.workflows {\n\t\tfound := false\n\t\tfor _, e := range events {\n\t\t\tfor _, we := range w.On() {\n\t\t\t\tif e == we {\n\t\t\t\t\tfound = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tif found {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\n\t\tif !found {\n\t\t\tevents = append(events, w.On()...)\n\t\t}\n\t}\n\n\t// sort the list based on depth of dependencies\n\tsort.Slice(events, func(i, j int) bool {\n\t\treturn events[i] < events[j]\n\t})\n\n\treturn events\n}\n\n// MaxRunNameLen determines the max name length of all jobs\nfunc (p *Plan) MaxRunNameLen() int {\n\tmaxRunNameLen := 0\n\tfor _, stage := range p.Stages {\n\t\tfor _, run := range stage.Runs {\n\t\t\trunNameLen := len(run.String())\n\t\t\tif runNameLen > maxRunNameLen {\n\t\t\t\tmaxRunNameLen = runNameLen\n\t\t\t}\n\t\t}\n\t}\n\treturn maxRunNameLen\n}\n\n// GetJobIDs will get all the job names in the stage\nfunc (s *Stage) GetJobIDs() []string {\n\tnames := make([]string, 0)\n\tfor _, r := range s.Runs {\n\t\tnames = append(names, r.JobID)\n\t}\n\treturn names\n}\n\n// Merge stages with existing stages in plan\nfunc (p *Plan) mergeStages(stages []*Stage) {\n\tnewStages := make([]*Stage, int(math.Max(float64(len(p.Stages)), float64(len(stages)))))\n\tfor i := 0; i < len(newStages); i++ {\n\t\tnewStages[i] = new(Stage)\n\t\tif i >= len(p.Stages) {\n\t\t\tnewStages[i].Runs = append(newStages[i].Runs, stages[i].Runs...)\n\t\t} else if i >= len(stages) {\n\t\t\tnewStages[i].Runs = append(newStages[i].Runs, p.Stages[i].Runs...)\n\t\t} else {\n\t\t\tnewStages[i].Runs = append(newStages[i].Runs, p.Stages[i].Runs...)\n\t\t\tnewStages[i].Runs = append(newStages[i].Runs, stages[i].Runs...)\n\t\t}\n\t}\n\tp.Stages = newStages\n}\n\nfunc createStages(w *Workflow, jobIDs ...string) ([]*Stage, error) {\n\t// first, build a list of all the necessary jobs to run, and their dependencies\n\tjobDependencies := make(map[string][]string)\n\tfor len(jobIDs) > 0 {\n\t\tnewJobIDs := make([]string, 0)\n\t\tfor _, jID := range jobIDs {\n\t\t\t// make sure we haven't visited this job yet\n\t\t\tif _, ok := jobDependencies[jID]; !ok {\n\t\t\t\tif job := w.GetJob(jID); job != nil {\n\t\t\t\t\tjobDependencies[jID] = job.Needs()\n\t\t\t\t\tnewJobIDs = append(newJobIDs, job.Needs()...)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tjobIDs = newJobIDs\n\t}\n\n\t// next, build an execution graph\n\tstages := make([]*Stage, 0)\n\tfor len(jobDependencies) > 0 {\n\t\tstage := new(Stage)\n\t\tfor jID, jDeps := range jobDependencies {\n\t\t\t// make sure all deps are in the graph already\n\t\t\tif listInStages(jDeps, stages...) {\n\t\t\t\tstage.Runs = append(stage.Runs, &Run{\n\t\t\t\t\tWorkflow: w,\n\t\t\t\t\tJobID:    jID,\n\t\t\t\t})\n\t\t\t\tdelete(jobDependencies, jID)\n\t\t\t}\n\t\t}\n\t\tif len(stage.Runs) == 0 {\n\t\t\treturn nil, fmt.Errorf(\"unable to build dependency graph for %s (%s)\", w.Name, w.File)\n\t\t}\n\t\tstages = append(stages, stage)\n\t}\n\n\treturn stages, nil\n}\n\n// return true iff all strings in srcList exist in at least one of the stages\nfunc listInStages(srcList []string, stages ...*Stage) bool {\n\tfor _, src := range srcList {\n\t\tfound := false\n\t\tfor _, stage := range stages {\n\t\t\tfor _, search := range stage.GetJobIDs() {\n\t\t\t\tif src == search {\n\t\t\t\t\tfound = true\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif !found {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:47:29"}, {"role": "assistant", "content": "```go\npackage model\n\nimport (\n\t\"path/filepath\"\n\t\"testing\"\n\n\tlog \"github.com/sirupsen/logrus\"\n\t\"github.com/stretchr/testify/assert\"\n)\n\ntype WorkflowPlanTest struct {\n\tworkflowPath      string\n\terrorMessage      string\n\tnoWorkflowRecurse bool\n}\n\nfunc TestPlanner(t *testing.T) {\n\tlog.SetLevel(log.DebugLevel)\n\n\ttables := []WorkflowPlanTest{\n\t\t{\"invalid-job-name/invalid-1.yml\", \"workflow is not valid. 'invalid-job-name-1': Job name 'invalid-JOB-Name-v1.2.3-docker_hub' is invalid. Names must start with a letter or '_' and contain only alphanumeric characters, '-', or '_'\", false},\n\t\t{\"invalid-job-name/invalid-2.yml\", \"workflow is not valid. 'invalid-job-name-2': Job name '1234invalid-JOB-Name-v123-docker_hub' is invalid. Names must start with a letter or '_' and contain only alphanumeric characters, '-', or '_'\", false},\n\t\t{\"invalid-job-name/valid-1.yml\", \"\", false},\n\t\t{\"invalid-job-name/valid-2.yml\", \"\", false},\n\t\t{\"empty-workflow\", \"unable to read workflow 'push.yml': file is empty: EOF\", false},\n\t\t{\"nested\", \"unable to read workflow 'fail.yml': file is empty: EOF\", false},\n\t\t{\"nested\", \"\", true},\n\t}\n\n\tworkdir, err := filepath.Abs(\"testdata\")\n\tassert.NoError(t, err, workdir)\n\tfor _, table := range tables {\n\t\tfullWorkflowPath := filepath.Join(workdir, table.workflowPath)\n\t\t_, err = NewWorkflowPlanner(fullWorkflowPath, table.noWorkflowRecurse)\n\t\tif table.errorMessage == \"\" {\n\t\t\tassert.NoError(t, err, \"WorkflowPlanner should exit without any error\")\n\t\t} else {\n\t\t\tassert.EqualError(t, err, table.errorMessage)\n\t\t}\n\t}\n}\n\nfunc TestWorkflow(t *testing.T) {\n\tlog.SetLevel(log.DebugLevel)\n\n\tworkflow := Workflow{\n\t\tJobs: map[string]*Job{\n\t\t\t\"valid_job\": {\n\t\t\t\tName: \"valid_job\",\n\t\t\t},\n\t\t},\n\t}\n\n\t// Check that a valid job id returns non-error\n\tresult, err := createStages(&workflow, \"valid_job\")\n\tassert.Nil(t, err)\n\tassert.NotNil(t, result)\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: act\nSource File Path(s): ['act\\\\workflow_pattern_test.go\\\\source_files\\\\workflow_pattern.go']\nTest File Path: act\\workflow_pattern_test.go\\workflow_pattern_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: act\\workflow_pattern_test.go\\source_files\\workflow_pattern.go\npackage workflowpattern\n\nimport (\n\t\"fmt\"\n\t\"regexp\"\n\t\"strings\"\n)\n\ntype WorkflowPattern struct {\n\tPattern  string\n\tNegative bool\n\tRegex    *regexp.Regexp\n}\n\nfunc CompilePattern(rawpattern string) (*WorkflowPattern, error) {\n\tnegative := false\n\tpattern := rawpattern\n\tif strings.HasPrefix(rawpattern, \"!\") {\n\t\tnegative = true\n\t\tpattern = rawpattern[1:]\n\t}\n\trpattern, err := PatternToRegex(pattern)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tregex, err := regexp.Compile(rpattern)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &WorkflowPattern{\n\t\tPattern:  pattern,\n\t\tNegative: negative,\n\t\tRegex:    regex,\n\t}, nil\n}\n\n//nolint:gocyclo\nfunc PatternToRegex(pattern string) (string, error) {\n\tvar rpattern strings.Builder\n\trpattern.WriteString(\"^\")\n\tpos := 0\n\terrors := map[int]string{}\n\tfor pos < len(pattern) {\n\t\tswitch pattern[pos] {\n\t\tcase '*':\n\t\t\tif pos+1 < len(pattern) && pattern[pos+1] == '*' {\n\t\t\t\tif pos+2 < len(pattern) && pattern[pos+2] == '/' {\n\t\t\t\t\trpattern.WriteString(\"(.+/)?\")\n\t\t\t\t\tpos += 3\n\t\t\t\t} else {\n\t\t\t\t\trpattern.WriteString(\".*\")\n\t\t\t\t\tpos += 2\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\trpattern.WriteString(\"[^/]*\")\n\t\t\t\tpos++\n\t\t\t}\n\t\tcase '+', '?':\n\t\t\tif pos > 0 {\n\t\t\t\trpattern.WriteByte(pattern[pos])\n\t\t\t} else {\n\t\t\t\trpattern.WriteString(regexp.QuoteMeta(string([]byte{pattern[pos]})))\n\t\t\t}\n\t\t\tpos++\n\t\tcase '[':\n\t\t\trpattern.WriteByte(pattern[pos])\n\t\t\tpos++\n\t\t\tif pos < len(pattern) && pattern[pos] == ']' {\n\t\t\t\terrors[pos] = \"Unexpected empty brackets '[]'\"\n\t\t\t\tpos++\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tvalidChar := func(a, b, test byte) bool {\n\t\t\t\treturn test >= a && test <= b\n\t\t\t}\n\t\t\tstartPos := pos\n\t\t\tfor pos < len(pattern) && pattern[pos] != ']' {\n\t\t\t\tswitch pattern[pos] {\n\t\t\t\tcase '-':\n\t\t\t\t\tif pos <= startPos || pos+1 >= len(pattern) {\n\t\t\t\t\t\terrors[pos] = \"Invalid range\"\n\t\t\t\t\t\tpos++\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t\tvalidRange := func(a, b byte) bool {\n\t\t\t\t\t\treturn validChar(a, b, pattern[pos-1]) && validChar(a, b, pattern[pos+1]) && pattern[pos-1] <= pattern[pos+1]\n\t\t\t\t\t}\n\t\t\t\t\tif !validRange('A', 'z') && !validRange('0', '9') {\n\t\t\t\t\t\terrors[pos] = \"Ranges can only include a-z, A-Z, A-z, and 0-9\"\n\t\t\t\t\t\tpos++\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t\trpattern.WriteString(pattern[pos : pos+2])\n\t\t\t\t\tpos += 2\n\t\t\t\tdefault:\n\t\t\t\t\tif !validChar('A', 'z', pattern[pos]) && !validChar('0', '9', pattern[pos]) {\n\t\t\t\t\t\terrors[pos] = \"Ranges can only include a-z, A-Z and 0-9\"\n\t\t\t\t\t\tpos++\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t\trpattern.WriteString(regexp.QuoteMeta(string([]byte{pattern[pos]})))\n\t\t\t\t\tpos++\n\t\t\t\t}\n\t\t\t}\n\t\t\tif pos >= len(pattern) || pattern[pos] != ']' {\n\t\t\t\terrors[pos] = \"Missing closing bracket ']' after '['\"\n\t\t\t\tpos++\n\t\t\t}\n\t\t\trpattern.WriteString(\"]\")\n\t\t\tpos++\n\t\tcase '\\\\':\n\t\t\tif pos+1 >= len(pattern) {\n\t\t\t\terrors[pos] = \"Missing symbol after \\\\\"\n\t\t\t\tpos++\n\t\t\t\tbreak\n\t\t\t}\n\t\t\trpattern.WriteString(regexp.QuoteMeta(string([]byte{pattern[pos+1]})))\n\t\t\tpos += 2\n\t\tdefault:\n\t\t\trpattern.WriteString(regexp.QuoteMeta(string([]byte{pattern[pos]})))\n\t\t\tpos++\n\t\t}\n\t}\n\tif len(errors) > 0 {\n\t\tvar errorMessage strings.Builder\n\t\tfor position, err := range errors {\n\t\t\tif errorMessage.Len() > 0 {\n\t\t\t\terrorMessage.WriteString(\", \")\n\t\t\t}\n\t\t\terrorMessage.WriteString(fmt.Sprintf(\"Position: %d Error: %s\", position, err))\n\t\t}\n\t\treturn \"\", fmt.Errorf(\"invalid Pattern '%s': %s\", pattern, errorMessage.String())\n\t}\n\trpattern.WriteString(\"$\")\n\treturn rpattern.String(), nil\n}\n\nfunc CompilePatterns(patterns ...string) ([]*WorkflowPattern, error) {\n\tret := []*WorkflowPattern{}\n\tfor _, pattern := range patterns {\n\t\tcp, err := CompilePattern(pattern)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tret = append(ret, cp)\n\t}\n\treturn ret, nil\n}\n\n// returns true if the workflow should be skipped paths/branches\nfunc Skip(sequence []*WorkflowPattern, input []string, traceWriter TraceWriter) bool {\n\tif len(sequence) == 0 {\n\t\treturn false\n\t}\n\tfor _, file := range input {\n\t\tmatched := false\n\t\tfor _, item := range sequence {\n\t\t\tif item.Regex.MatchString(file) {\n\t\t\t\tpattern := item.Pattern\n\t\t\t\tif item.Negative {\n\t\t\t\t\tmatched = false\n\t\t\t\t\ttraceWriter.Info(\"%s excluded by pattern %s\", file, pattern)\n\t\t\t\t} else {\n\t\t\t\t\tmatched = true\n\t\t\t\t\ttraceWriter.Info(\"%s included by pattern %s\", file, pattern)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif matched {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// returns true if the workflow should be skipped paths-ignore/branches-ignore\nfunc Filter(sequence []*WorkflowPattern, input []string, traceWriter TraceWriter) bool {\n\tif len(sequence) == 0 {\n\t\treturn false\n\t}\n\tfor _, file := range input {\n\t\tmatched := false\n\t\tfor _, item := range sequence {\n\t\t\tif item.Regex.MatchString(file) == !item.Negative {\n\t\t\t\tpattern := item.Pattern\n\t\t\t\ttraceWriter.Info(\"%s ignored by pattern %s\", file, pattern)\n\t\t\t\tmatched = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif !matched {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:47:29"}, {"role": "assistant", "content": "```go\npackage workflowpattern\n\nimport (\n\t\"strings\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestMatchPattern(t *testing.T) {\n\tkases := []struct {\n\t\tinputs       []string\n\t\tpatterns     []string\n\t\tskipResult   bool\n\t\tfilterResult bool\n\t}{\n\t\t{\n\t\t\tpatterns:     []string{\"*\"},\n\t\t\tinputs:       []string{\"path/with/slash\"},\n\t\t\tskipResult:   true,\n\t\t\tfilterResult: false,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"path/a\", \"path/b\", \"path/c\"},\n\t\t\tinputs:       []string{\"meta\", \"path/b\", \"otherfile\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: false,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"path/a\", \"path/b\", \"path/c\"},\n\t\t\tinputs:       []string{\"path/b\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"path/a\", \"path/b\", \"path/c\"},\n\t\t\tinputs:       []string{\"path/c\", \"path/b\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"path/a\", \"path/b\", \"path/c\"},\n\t\t\tinputs:       []string{\"path/c\", \"path/b\", \"path/a\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"path/a\", \"path/b\", \"path/c\"},\n\t\t\tinputs:       []string{\"path/c\", \"path/b\", \"path/d\", \"path/a\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: false,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{},\n\t\t\tinputs:       []string{},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: false,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"\\\\!file\"},\n\t\t\tinputs:       []string{\"!file\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"escape\\\\\\\\backslash\"},\n\t\t\tinputs:       []string{\"escape\\\\backslash\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\".yml\"},\n\t\t\tinputs:       []string{\"fyml\"},\n\t\t\tskipResult:   true,\n\t\t\tfilterResult: false,\n\t\t},\n\t\t// https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#patterns-to-match-branches-and-tags\n\t\t{\n\t\t\tpatterns:     []string{\"feature/*\"},\n\t\t\tinputs:       []string{\"feature/my-branch\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"feature/*\"},\n\t\t\tinputs:       []string{\"feature/your-branch\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"feature/**\"},\n\t\t\tinputs:       []string{\"feature/beta-a/my-branch\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"feature/**\"},\n\t\t\tinputs:       []string{\"feature/beta-a/my-branch\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"feature/**\"},\n\t\t\tinputs:       []string{\"feature/mona/the/octocat\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"main\", \"releases/mona-the-octocat\"},\n\t\t\tinputs:       []string{\"main\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"main\", \"releases/mona-the-octocat\"},\n\t\t\tinputs:       []string{\"releases/mona-the-octocat\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"*\"},\n\t\t\tinputs:       []string{\"main\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"*\"},\n\t\t\tinputs:       []string{\"releases\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"**\"},\n\t\t\tinputs:       []string{\"all/the/branches\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"**\"},\n\t\t\tinputs:       []string{\"every/tag\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"*feature\"},\n\t\t\tinputs:       []string{\"mona-feature\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"*feature\"},\n\t\t\tinputs:       []string{\"feature\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"*feature\"},\n\t\t\tinputs:       []string{\"ver-10-feature\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"v2*\"},\n\t\t\tinputs:       []string{\"v2\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"v2*\"},\n\t\t\tinputs:       []string{\"v2.0\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"v2*\"},\n\t\t\tinputs:       []string{\"v2.9\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"v[12].[0-9]+.[0-9]+\"},\n\t\t\tinputs:       []string{\"v1.10.1\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"v[12].[0-9]+.[0-9]+\"},\n\t\t\tinputs:       []string{\"v2.0.0\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t// https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#patterns-to-match-file-paths\n\t\t{\n\t\t\tpatterns:     []string{\"*\"},\n\t\t\tinputs:       []string{\"README.md\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"*\"},\n\t\t\tinputs:       []string{\"server.rb\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"*.jsx?\"},\n\t\t\tinputs:       []string{\"page.js\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"*.jsx?\"},\n\t\t\tinputs:       []string{\"page.jsx\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"**\"},\n\t\t\tinputs:       []string{\"all/the/files.md\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"*.js\"},\n\t\t\tinputs:       []string{\"app.js\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"*.js\"},\n\t\t\tinputs:       []string{\"index.js\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"**.js\"},\n\t\t\tinputs:       []string{\"index.js\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"**.js\"},\n\t\t\tinputs:       []string{\"js/index.js\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"**.js\"},\n\t\t\tinputs:       []string{\"src/js/app.js\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"docs/*\"},\n\t\t\tinputs:       []string{\"docs/README.md\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"docs/*\"},\n\t\t\tinputs:       []string{\"docs/file.txt\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"docs/**\"},\n\t\t\tinputs:       []string{\"docs/README.md\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"docs/**\"},\n\t\t\tinputs:       []string{\"docs/mona/octocat.txt\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"docs/**/*.md\"},\n\t\t\tinputs:       []string{\"docs/README.md\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"docs/**/*.md\"},\n\t\t\tinputs:       []string{\"docs/mona/hello-world.md\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"docs/**/*.md\"},\n\t\t\tinputs:       []string{\"docs/a/markdown/file.md\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"**/docs/**\"},\n\t\t\tinputs:       []string{\"docs/hello.md\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"**/docs/**\"},\n\t\t\tinputs:       []string{\"dir/docs/my-file.txt\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"**/docs/**\"},\n\t\t\tinputs:       []string{\"space/docs/plan/space.doc\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"**/README.md\"},\n\t\t\tinputs:       []string{\"README.md\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"**/README.md\"},\n\t\t\tinputs:       []string{\"js/README.md\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"**/*src/**\"},\n\t\t\tinputs:       []string{\"a/src/app.js\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"**/*src/**\"},\n\t\t\tinputs:       []string{\"my-src/code/js/app.js\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"**/*-post.md\"},\n\t\t\tinputs:       []string{\"my-post.md\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"**/*-post.md\"},\n\t\t\tinputs:       []string{\"path/their-post.md\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"**/migrate-*.sql\"},\n\t\t\tinputs:       []string{\"migrate-10909.sql\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"**/migrate-*.sql\"},\n\t\t\tinputs:       []string{\"db/migrate-v1.0.sql\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"**/migrate-*.sql\"},\n\t\t\tinputs:       []string{\"db/sept/migrate-v1.sql\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"*.md\", \"!README.md\"},\n\t\t\tinputs:       []string{\"hello.md\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"*.md\", \"!README.md\"},\n\t\t\tinputs:       []string{\"README.md\"},\n\t\t\tskipResult:   true,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"*.md\", \"!README.md\"},\n\t\t\tinputs:       []string{\"docs/hello.md\"},\n\t\t\tskipResult:   true,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"*.md\", \"!README.md\", \"README*\"},\n\t\t\tinputs:       []string{\"hello.md\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"*.md\", \"!README.md\", \"README*\"},\n\t\t\tinputs:       []string{\"README.md\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t\t{\n\t\t\tpatterns:     []string{\"*.md\", \"!README.md\", \"README*\"},\n\t\t\tinputs:       []string{\"README.doc\"},\n\t\t\tskipResult:   false,\n\t\t\tfilterResult: true,\n\t\t},\n\t}\n\n\tfor _, kase := range kases {\n\t\tt.Run(strings.Join(kase.patterns, \",\"), func(t *testing.T) {\n\t\t\tpatterns, err := CompilePatterns(kase.patterns...)\n\t\t\tassert.NoError(t, err)\n\n\t\t\tassert.EqualValues(t, kase.skipResult, Skip(patterns, kase.inputs, &StdOutTraceWriter{}), \"skipResult\")\n\t\t\tassert.EqualValues(t, kase.filterResult, Filter(patterns, kase.inputs, &StdOutTraceWriter{}), \"filterResult\")\n\t\t})\n\t}\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: prometheus\nSource File Path(s): ['prometheus\\\\bstream_test.go\\\\source_files\\\\bstream.go']\nTest File Path: prometheus\\bstream_test.go\\bstream_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: prometheus\\bstream_test.go\\source_files\\bstream.go\n// Copyright 2017 The Prometheus Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// The code in this file was largely written by Damian Gryski as part of\n// https://github.com/dgryski/go-tsz and published under the license below.\n// It received minor modifications to suit Prometheus's needs.\n\n// Copyright (c) 2015,2016 Damian Gryski <damian@gryski.com>\n// All rights reserved.\n\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are met:\n\n// * Redistributions of source code must retain the above copyright notice,\n// this list of conditions and the following disclaimer.\n//\n// * Redistributions in binary form must reproduce the above copyright notice,\n// this list of conditions and the following disclaimer in the documentation\n// and/or other materials provided with the distribution.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n// ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n// WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n// FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n// DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n// SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n// CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n// OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\npackage chunkenc\n\nimport (\n\t\"encoding/binary\"\n\t\"io\"\n)\n\n// bstream is a stream of bits.\ntype bstream struct {\n\tstream []byte // The data stream.\n\tcount  uint8  // How many right-most bits are available for writing in the current byte (the last byte of the stream).\n}\n\n// Reset resets b around stream.\nfunc (b *bstream) Reset(stream []byte) {\n\tb.stream = stream\n\tb.count = 0\n}\n\nfunc (b *bstream) bytes() []byte {\n\treturn b.stream\n}\n\ntype bit bool\n\nconst (\n\tzero bit = false\n\tone  bit = true\n)\n\nfunc (b *bstream) writeBit(bit bit) {\n\tif b.count == 0 {\n\t\tb.stream = append(b.stream, 0)\n\t\tb.count = 8\n\t}\n\n\ti := len(b.stream) - 1\n\n\tif bit {\n\t\tb.stream[i] |= 1 << (b.count - 1)\n\t}\n\n\tb.count--\n}\n\nfunc (b *bstream) writeByte(byt byte) {\n\tif b.count == 0 {\n\t\tb.stream = append(b.stream, byt)\n\t\treturn\n\t}\n\n\ti := len(b.stream) - 1\n\n\t// Complete the last byte with the leftmost b.count bits from byt.\n\tb.stream[i] |= byt >> (8 - b.count)\n\n\t// Write the remainder, if any.\n\tb.stream = append(b.stream, byt<<b.count)\n}\n\n// writeBits writes the nbits right-most bits of u to the stream\n// in left-to-right order.\nfunc (b *bstream) writeBits(u uint64, nbits int) {\n\tu <<= 64 - uint(nbits)\n\tfor nbits >= 8 {\n\t\tbyt := byte(u >> 56)\n\t\tb.writeByte(byt)\n\t\tu <<= 8\n\t\tnbits -= 8\n\t}\n\n\tfor nbits > 0 {\n\t\tb.writeBit((u >> 63) == 1)\n\t\tu <<= 1\n\t\tnbits--\n\t}\n}\n\ntype bstreamReader struct {\n\tstream       []byte\n\tstreamOffset int // The offset from which read the next byte from the stream.\n\n\tbuffer uint64 // The current buffer, filled from the stream, containing up to 8 bytes from which read bits.\n\tvalid  uint8  // The number of right-most bits valid to read (from left) in the current 8 byte buffer.\n\tlast   byte   // A copy of the last byte of the stream.\n}\n\nfunc newBReader(b []byte) bstreamReader {\n\t// The last byte of the stream can be updated later, so we take a copy.\n\tvar last byte\n\tif len(b) > 0 {\n\t\tlast = b[len(b)-1]\n\t}\n\treturn bstreamReader{\n\t\tstream: b,\n\t\tlast:   last,\n\t}\n}\n\nfunc (b *bstreamReader) readBit() (bit, error) {\n\tif b.valid == 0 {\n\t\tif !b.loadNextBuffer(1) {\n\t\t\treturn false, io.EOF\n\t\t}\n\t}\n\n\treturn b.readBitFast()\n}\n\n// readBitFast is like readBit but can return io.EOF if the internal buffer is empty.\n// If it returns io.EOF, the caller should retry reading bits calling readBit().\n// This function must be kept small and a leaf in order to help the compiler inlining it\n// and further improve performances.\nfunc (b *bstreamReader) readBitFast() (bit, error) {\n\tif b.valid == 0 {\n\t\treturn false, io.EOF\n\t}\n\n\tb.valid--\n\tbitmask := uint64(1) << b.valid\n\treturn (b.buffer & bitmask) != 0, nil\n}\n\n// readBits constructs a uint64 with the nbits right-most bits\n// read from the stream, and any other bits 0.\nfunc (b *bstreamReader) readBits(nbits uint8) (uint64, error) {\n\tif b.valid == 0 {\n\t\tif !b.loadNextBuffer(nbits) {\n\t\t\treturn 0, io.EOF\n\t\t}\n\t}\n\n\tif nbits <= b.valid {\n\t\treturn b.readBitsFast(nbits)\n\t}\n\n\t// We have to read all remaining valid bits from the current buffer and a part from the next one.\n\tbitmask := (uint64(1) << b.valid) - 1\n\tnbits -= b.valid\n\tv := (b.buffer & bitmask) << nbits\n\tb.valid = 0\n\n\tif !b.loadNextBuffer(nbits) {\n\t\treturn 0, io.EOF\n\t}\n\n\tbitmask = (uint64(1) << nbits) - 1\n\tv |= ((b.buffer >> (b.valid - nbits)) & bitmask)\n\tb.valid -= nbits\n\n\treturn v, nil\n}\n\n// readBitsFast is like readBits but can return io.EOF if the internal buffer is empty.\n// If it returns io.EOF, the caller should retry reading bits calling readBits().\n// This function must be kept small and a leaf in order to help the compiler inlining it\n// and further improve performances.\nfunc (b *bstreamReader) readBitsFast(nbits uint8) (uint64, error) {\n\tif nbits > b.valid {\n\t\treturn 0, io.EOF\n\t}\n\n\tbitmask := (uint64(1) << nbits) - 1\n\tb.valid -= nbits\n\n\treturn (b.buffer >> b.valid) & bitmask, nil\n}\n\nfunc (b *bstreamReader) ReadByte() (byte, error) {\n\tv, err := b.readBits(8)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn byte(v), nil\n}\n\n// loadNextBuffer loads the next bytes from the stream into the internal buffer.\n// The input nbits is the minimum number of bits that must be read, but the implementation\n// can read more (if possible) to improve performances.\nfunc (b *bstreamReader) loadNextBuffer(nbits uint8) bool {\n\tif b.streamOffset >= len(b.stream) {\n\t\treturn false\n\t}\n\n\t// Handle the case there are more then 8 bytes in the buffer (most common case)\n\t// in a optimized way. It's guaranteed that this branch will never read from the\n\t// very last byte of the stream (which suffers race conditions due to concurrent\n\t// writes).\n\tif b.streamOffset+8 < len(b.stream) {\n\t\tb.buffer = binary.BigEndian.Uint64(b.stream[b.streamOffset:])\n\t\tb.streamOffset += 8\n\t\tb.valid = 64\n\t\treturn true\n\t}\n\n\t// We're here if there are 8 or less bytes left in the stream.\n\t// The following code is slower but called less frequently.\n\tnbytes := int((nbits / 8) + 1)\n\tif b.streamOffset+nbytes > len(b.stream) {\n\t\tnbytes = len(b.stream) - b.streamOffset\n\t}\n\n\tbuffer := uint64(0)\n\tskip := 0\n\tif b.streamOffset+nbytes == len(b.stream) {\n\t\t// There can be concurrent writes happening on the very last byte\n\t\t// of the stream, so use the copy we took at initialization time.\n\t\tbuffer |= uint64(b.last)\n\t\t// Read up to the byte before\n\t\tskip = 1\n\t}\n\n\tfor i := 0; i < nbytes-skip; i++ {\n\t\tbuffer |= (uint64(b.stream[b.streamOffset+i]) << uint(8*(nbytes-i-1)))\n\t}\n\n\tb.buffer = buffer\n\tb.streamOffset += nbytes\n\tb.valid = uint8(nbytes * 8)\n\n\treturn true\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:47:45"}, {"role": "assistant", "content": "```go\n// Copyright 2020 The Prometheus Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage chunkenc\n\nimport (\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestBstream_Reset(t *testing.T) {\n\tbs := bstream{\n\t\tstream: []byte(\"test\"),\n\t\tcount:  10,\n\t}\n\tbs.Reset([]byte(\"was reset\"))\n\n\trequire.Equal(t, bstream{\n\t\tstream: []byte(\"was reset\"),\n\t\tcount:  0,\n\t}, bs)\n}\n\nfunc TestBstreamReader(t *testing.T) {\n\t// Write to the bit stream.\n\tw := bstream{}\n\tfor _, bit := range []bit{true, false} {\n\t\tw.writeBit(bit)\n\t}\n\tfor nbits := 1; nbits <= 64; nbits++ {\n\t\tw.writeBits(uint64(nbits), nbits)\n\t}\n\tfor v := 1; v < 10000; v += 123 {\n\t\tw.writeBits(uint64(v), 29)\n\t}\n\n\t// Read back.\n\tr := newBReader(w.bytes())\n\tfor _, bit := range []bit{true, false} {\n\t\tv, err := r.readBitFast()\n\t\tif err != nil {\n\t\t\tv, err = r.readBit()\n\t\t}\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, bit, v)\n\t}\n\tfor nbits := uint8(1); nbits <= 64; nbits++ {\n\t\tv, err := r.readBitsFast(nbits)\n\t\tif err != nil {\n\t\t\tv, err = r.readBits(nbits)\n\t\t}\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, uint64(nbits), v, \"nbits=%d\", nbits)\n\t}\n\tfor v := 1; v < 10000; v += 123 {\n\t\tactual, err := r.readBitsFast(29)\n\t\tif err != nil {\n\t\t\tactual, err = r.readBits(29)\n\t\t}\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, uint64(v), actual, \"v=%d\", v)\n\t}\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: prometheus\nSource File Path(s): ['prometheus\\\\dedicated_server_test.go\\\\source_files\\\\dedicated_server.go']\nTest File Path: prometheus\\dedicated_server_test.go\\dedicated_server_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: prometheus\\dedicated_server_test.go\\source_files\\dedicated_server.go\n// Copyright 2021 The Prometheus Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage ovhcloud\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"net/netip\"\n\t\"net/url\"\n\t\"path\"\n\t\"strconv\"\n\n\t\"github.com/ovh/go-ovh/ovh\"\n\t\"github.com/prometheus/common/model\"\n\n\t\"github.com/prometheus/prometheus/discovery/refresh\"\n\t\"github.com/prometheus/prometheus/discovery/targetgroup\"\n)\n\nconst (\n\tdedicatedServerAPIPath     = \"/dedicated/server\"\n\tdedicatedServerLabelPrefix = metaLabelPrefix + \"dedicated_server_\"\n)\n\n// dedicatedServer struct from API. Also contains IP addresses that are fetched\n// independently.\ntype dedicatedServer struct {\n\tState           string `json:\"state\"`\n\tips             []netip.Addr\n\tCommercialRange string `json:\"commercialRange\"`\n\tLinkSpeed       int    `json:\"linkSpeed\"`\n\tRack            string `json:\"rack\"`\n\tNoIntervention  bool   `json:\"noIntervention\"`\n\tOs              string `json:\"os\"`\n\tSupportLevel    string `json:\"supportLevel\"`\n\tServerID        int64  `json:\"serverId\"`\n\tReverse         string `json:\"reverse\"`\n\tDatacenter      string `json:\"datacenter\"`\n\tName            string `json:\"name\"`\n}\n\ntype dedicatedServerDiscovery struct {\n\t*refresh.Discovery\n\tconfig *SDConfig\n\tlogger *slog.Logger\n}\n\nfunc newDedicatedServerDiscovery(conf *SDConfig, logger *slog.Logger) *dedicatedServerDiscovery {\n\treturn &dedicatedServerDiscovery{config: conf, logger: logger}\n}\n\nfunc getDedicatedServerList(client *ovh.Client) ([]string, error) {\n\tvar dedicatedListName []string\n\terr := client.Get(dedicatedServerAPIPath, &dedicatedListName)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn dedicatedListName, nil\n}\n\nfunc getDedicatedServerDetails(client *ovh.Client, serverName string) (*dedicatedServer, error) {\n\tvar dedicatedServerDetails dedicatedServer\n\terr := client.Get(path.Join(dedicatedServerAPIPath, url.QueryEscape(serverName)), &dedicatedServerDetails)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar ips []string\n\terr = client.Get(path.Join(dedicatedServerAPIPath, url.QueryEscape(serverName), \"ips\"), &ips)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tparsedIPs, err := parseIPList(ips)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tdedicatedServerDetails.ips = parsedIPs\n\treturn &dedicatedServerDetails, nil\n}\n\nfunc (d *dedicatedServerDiscovery) getService() string {\n\treturn \"dedicated_server\"\n}\n\nfunc (d *dedicatedServerDiscovery) getSource() string {\n\treturn fmt.Sprintf(\"%s_%s\", d.config.Name(), d.getService())\n}\n\nfunc (d *dedicatedServerDiscovery) refresh(context.Context) ([]*targetgroup.Group, error) {\n\tclient, err := createClient(d.config)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tvar dedicatedServerDetailedList []dedicatedServer\n\tdedicatedServerList, err := getDedicatedServerList(client)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tfor _, dedicatedServerName := range dedicatedServerList {\n\t\tdedicatedServer, err := getDedicatedServerDetails(client, dedicatedServerName)\n\t\tif err != nil {\n\t\t\td.logger.Warn(fmt.Sprintf(\"%s: Could not get details of %s\", d.getSource(), dedicatedServerName), \"err\", err.Error())\n\t\t\tcontinue\n\t\t}\n\t\tdedicatedServerDetailedList = append(dedicatedServerDetailedList, *dedicatedServer)\n\t}\n\tvar targets []model.LabelSet\n\n\tfor _, server := range dedicatedServerDetailedList {\n\t\tvar ipv4, ipv6 string\n\t\tfor _, ip := range server.ips {\n\t\t\tif ip.Is4() {\n\t\t\t\tipv4 = ip.String()\n\t\t\t}\n\t\t\tif ip.Is6() {\n\t\t\t\tipv6 = ip.String()\n\t\t\t}\n\t\t}\n\t\tdefaultIP := ipv4\n\t\tif defaultIP == \"\" {\n\t\t\tdefaultIP = ipv6\n\t\t}\n\t\tlabels := model.LabelSet{\n\t\t\tmodel.AddressLabel:                              model.LabelValue(defaultIP),\n\t\t\tmodel.InstanceLabel:                             model.LabelValue(server.Name),\n\t\t\tdedicatedServerLabelPrefix + \"state\":            model.LabelValue(server.State),\n\t\t\tdedicatedServerLabelPrefix + \"commercial_range\": model.LabelValue(server.CommercialRange),\n\t\t\tdedicatedServerLabelPrefix + \"link_speed\":       model.LabelValue(strconv.Itoa(server.LinkSpeed)),\n\t\t\tdedicatedServerLabelPrefix + \"rack\":             model.LabelValue(server.Rack),\n\t\t\tdedicatedServerLabelPrefix + \"no_intervention\":  model.LabelValue(strconv.FormatBool(server.NoIntervention)),\n\t\t\tdedicatedServerLabelPrefix + \"os\":               model.LabelValue(server.Os),\n\t\t\tdedicatedServerLabelPrefix + \"support_level\":    model.LabelValue(server.SupportLevel),\n\t\t\tdedicatedServerLabelPrefix + \"server_id\":        model.LabelValue(strconv.FormatInt(server.ServerID, 10)),\n\t\t\tdedicatedServerLabelPrefix + \"reverse\":          model.LabelValue(server.Reverse),\n\t\t\tdedicatedServerLabelPrefix + \"datacenter\":       model.LabelValue(server.Datacenter),\n\t\t\tdedicatedServerLabelPrefix + \"name\":             model.LabelValue(server.Name),\n\t\t\tdedicatedServerLabelPrefix + \"ipv4\":             model.LabelValue(ipv4),\n\t\t\tdedicatedServerLabelPrefix + \"ipv6\":             model.LabelValue(ipv6),\n\t\t}\n\t\ttargets = append(targets, labels)\n\t}\n\n\treturn []*targetgroup.Group{{Source: d.getSource(), Targets: targets}}, nil\n}\n\n### Source File Dependency Files Content\n### Dependency File: prometheus\\dedicated_server_test.go\\dependent_files\\refresh.go\n// Copyright 2019 The Prometheus Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage refresh\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"log/slog\"\n\t\"time\"\n\n\t\"github.com/prometheus/common/promslog\"\n\n\t\"github.com/prometheus/prometheus/discovery\"\n\t\"github.com/prometheus/prometheus/discovery/targetgroup\"\n)\n\ntype Options struct {\n\tLogger              *slog.Logger\n\tMech                string\n\tInterval            time.Duration\n\tRefreshF            func(ctx context.Context) ([]*targetgroup.Group, error)\n\tMetricsInstantiator discovery.RefreshMetricsInstantiator\n}\n\n// Discovery implements the Discoverer interface.\ntype Discovery struct {\n\tlogger   *slog.Logger\n\tinterval time.Duration\n\trefreshf func(ctx context.Context) ([]*targetgroup.Group, error)\n\tmetrics  *discovery.RefreshMetrics\n}\n\n// NewDiscovery returns a Discoverer function that calls a refresh() function at every interval.\nfunc NewDiscovery(opts Options) *Discovery {\n\tm := opts.MetricsInstantiator.Instantiate(opts.Mech)\n\n\tvar logger *slog.Logger\n\tif opts.Logger == nil {\n\t\tlogger = promslog.NewNopLogger()\n\t} else {\n\t\tlogger = opts.Logger\n\t}\n\n\td := Discovery{\n\t\tlogger:   logger,\n\t\tinterval: opts.Interval,\n\t\trefreshf: opts.RefreshF,\n\t\tmetrics:  m,\n\t}\n\n\treturn &d\n}\n\n// Run implements the Discoverer interface.\nfunc (d *Discovery) Run(ctx context.Context, ch chan<- []*targetgroup.Group) {\n\t// Get an initial set right away.\n\ttgs, err := d.refresh(ctx)\n\tif err != nil {\n\t\tif !errors.Is(ctx.Err(), context.Canceled) {\n\t\t\td.logger.Error(\"Unable to refresh target groups\", \"err\", err.Error())\n\t\t}\n\t} else {\n\t\tselect {\n\t\tcase ch <- tgs:\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\t}\n\t}\n\n\tticker := time.NewTicker(d.interval)\n\tdefer ticker.Stop()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ticker.C:\n\t\t\ttgs, err := d.refresh(ctx)\n\t\t\tif err != nil {\n\t\t\t\tif !errors.Is(ctx.Err(), context.Canceled) {\n\t\t\t\t\td.logger.Error(\"Unable to refresh target groups\", \"err\", err.Error())\n\t\t\t\t}\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tselect {\n\t\t\tcase ch <- tgs:\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn\n\t\t\t}\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (d *Discovery) refresh(ctx context.Context) ([]*targetgroup.Group, error) {\n\tnow := time.Now()\n\tdefer func() {\n\t\td.metrics.Duration.Observe(time.Since(now).Seconds())\n\t}()\n\n\ttgs, err := d.refreshf(ctx)\n\tif err != nil {\n\t\td.metrics.Failures.Inc()\n\t}\n\treturn tgs, err\n}\n\n\n### Dependency File: prometheus\\dedicated_server_test.go\\dependent_files\\targetgroup.go\n// Copyright 2013 The Prometheus Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage targetgroup\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\n\t\"github.com/prometheus/common/model\"\n)\n\n// Group is a set of targets with a common label set(production , test, staging etc.).\ntype Group struct {\n\t// Targets is a list of targets identified by a label set. Each target is\n\t// uniquely identifiable in the group by its address label.\n\tTargets []model.LabelSet\n\t// Labels is a set of labels that is common across all targets in the group.\n\tLabels model.LabelSet\n\n\t// Source is an identifier that describes a group of targets.\n\tSource string\n}\n\nfunc (tg Group) String() string {\n\treturn tg.Source\n}\n\n// UnmarshalYAML implements the yaml.Unmarshaler interface.\nfunc (tg *Group) UnmarshalYAML(unmarshal func(interface{}) error) error {\n\tg := struct {\n\t\tTargets []string       `yaml:\"targets\"`\n\t\tLabels  model.LabelSet `yaml:\"labels\"`\n\t}{}\n\tif err := unmarshal(&g); err != nil {\n\t\treturn err\n\t}\n\ttg.Targets = make([]model.LabelSet, 0, len(g.Targets))\n\tfor _, t := range g.Targets {\n\t\ttg.Targets = append(tg.Targets, model.LabelSet{\n\t\t\tmodel.AddressLabel: model.LabelValue(t),\n\t\t})\n\t}\n\ttg.Labels = g.Labels\n\treturn nil\n}\n\n// MarshalYAML implements the yaml.Marshaler interface.\nfunc (tg Group) MarshalYAML() (interface{}, error) {\n\tg := &struct {\n\t\tTargets []string       `yaml:\"targets\"`\n\t\tLabels  model.LabelSet `yaml:\"labels,omitempty\"`\n\t}{\n\t\tTargets: make([]string, 0, len(tg.Targets)),\n\t\tLabels:  tg.Labels,\n\t}\n\tfor _, t := range tg.Targets {\n\t\tg.Targets = append(g.Targets, string(t[model.AddressLabel]))\n\t}\n\treturn g, nil\n}\n\n// UnmarshalJSON implements the json.Unmarshaler interface.\nfunc (tg *Group) UnmarshalJSON(b []byte) error {\n\tg := struct {\n\t\tTargets []string       `json:\"targets\"`\n\t\tLabels  model.LabelSet `json:\"labels\"`\n\t}{}\n\n\tdec := json.NewDecoder(bytes.NewReader(b))\n\tdec.DisallowUnknownFields()\n\tif err := dec.Decode(&g); err != nil {\n\t\treturn err\n\t}\n\ttg.Targets = make([]model.LabelSet, 0, len(g.Targets))\n\tfor _, t := range g.Targets {\n\t\ttg.Targets = append(tg.Targets, model.LabelSet{\n\t\t\tmodel.AddressLabel: model.LabelValue(t),\n\t\t})\n\t}\n\ttg.Labels = g.Labels\n\treturn nil\n}\n\n// MarshalJSON implements the json.Marshaler interface.\nfunc (tg Group) MarshalJSON() ([]byte, error) {\n\tg := &struct {\n\t\tTargets []string       `json:\"targets\"`\n\t\tLabels  model.LabelSet `json:\"labels,omitempty\"`\n\t}{\n\t\tTargets: make([]string, 0, len(tg.Targets)),\n\t\tLabels:  tg.Labels,\n\t}\n\tfor _, t := range tg.Targets {\n\t\tg.Targets = append(g.Targets, string(t[model.AddressLabel]))\n\t}\n\treturn json.Marshal(g)\n}\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:47:45"}, {"role": "assistant", "content": "```go\n// Copyright 2021 The Prometheus Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage ovhcloud\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net/http\"\n\t\"net/http/httptest\"\n\t\"os\"\n\t\"testing\"\n\n\t\"github.com/prometheus/common/model\"\n\t\"github.com/prometheus/common/promslog\"\n\t\"github.com/stretchr/testify/require\"\n\t\"gopkg.in/yaml.v2\"\n)\n\nfunc TestOvhcloudDedicatedServerRefresh(t *testing.T) {\n\tvar cfg SDConfig\n\n\tmock := httptest.NewServer(http.HandlerFunc(MockDedicatedAPI))\n\tdefer mock.Close()\n\tcfgString := fmt.Sprintf(`\n---\nservice: dedicated_server\nendpoint: %s\napplication_key: %s\napplication_secret: %s\nconsumer_key: %s`, mock.URL, ovhcloudApplicationKeyTest, ovhcloudApplicationSecretTest, ovhcloudConsumerKeyTest)\n\n\trequire.NoError(t, yaml.UnmarshalStrict([]byte(cfgString), &cfg))\n\td, err := newRefresher(&cfg, promslog.NewNopLogger())\n\trequire.NoError(t, err)\n\tctx := context.Background()\n\ttargetGroups, err := d.refresh(ctx)\n\trequire.NoError(t, err)\n\n\trequire.Len(t, targetGroups, 1)\n\ttargetGroup := targetGroups[0]\n\trequire.NotNil(t, targetGroup)\n\trequire.NotNil(t, targetGroup.Targets)\n\trequire.Len(t, targetGroup.Targets, 1)\n\n\tfor i, lbls := range []model.LabelSet{\n\t\t{\n\t\t\t\"__address__\": \"1.2.3.4\",\n\t\t\t\"__meta_ovhcloud_dedicated_server_commercial_range\": \"Advance-1 Gen 2\",\n\t\t\t\"__meta_ovhcloud_dedicated_server_datacenter\":       \"gra3\",\n\t\t\t\"__meta_ovhcloud_dedicated_server_ipv4\":             \"1.2.3.4\",\n\t\t\t\"__meta_ovhcloud_dedicated_server_ipv6\":             \"\",\n\t\t\t\"__meta_ovhcloud_dedicated_server_link_speed\":       \"123\",\n\t\t\t\"__meta_ovhcloud_dedicated_server_name\":             \"abcde\",\n\t\t\t\"__meta_ovhcloud_dedicated_server_no_intervention\":  \"false\",\n\t\t\t\"__meta_ovhcloud_dedicated_server_os\":               \"debian11_64\",\n\t\t\t\"__meta_ovhcloud_dedicated_server_rack\":             \"TESTRACK\",\n\t\t\t\"__meta_ovhcloud_dedicated_server_reverse\":          \"abcde-rev\",\n\t\t\t\"__meta_ovhcloud_dedicated_server_server_id\":        \"1234\",\n\t\t\t\"__meta_ovhcloud_dedicated_server_state\":            \"test\",\n\t\t\t\"__meta_ovhcloud_dedicated_server_support_level\":    \"pro\",\n\t\t\t\"instance\": \"abcde\",\n\t\t},\n\t} {\n\t\tt.Run(fmt.Sprintf(\"item %d\", i), func(t *testing.T) {\n\t\t\trequire.Equal(t, lbls, targetGroup.Targets[i])\n\t\t})\n\t}\n}\n\nfunc MockDedicatedAPI(w http.ResponseWriter, r *http.Request) {\n\tif r.Header.Get(\"X-Ovh-Application\") != ovhcloudApplicationKeyTest {\n\t\thttp.Error(w, \"bad application key\", http.StatusBadRequest)\n\t\treturn\n\t}\n\tw.Header().Set(\"Content-Type\", \"application/json\")\n\tif r.URL.Path == \"/dedicated/server\" {\n\t\tdedicatedServersList, err := os.ReadFile(\"testdata/dedicated_server/dedicated_servers.json\")\n\t\tif err != nil {\n\t\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t\t\treturn\n\t\t}\n\t\t_, err = w.Write(dedicatedServersList)\n\t\tif err != nil {\n\t\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t\t\treturn\n\t\t}\n\t}\n\tif r.URL.Path == \"/dedicated/server/abcde\" {\n\t\tdedicatedServer, err := os.ReadFile(\"testdata/dedicated_server/dedicated_servers_details.json\")\n\t\tif err != nil {\n\t\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t\t\treturn\n\t\t}\n\t\t_, err = w.Write(dedicatedServer)\n\t\tif err != nil {\n\t\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t\t\treturn\n\t\t}\n\t}\n\tif r.URL.Path == \"/dedicated/server/abcde/ips\" {\n\t\tdedicatedServerIPs, err := os.ReadFile(\"testdata/dedicated_server/dedicated_servers_abcde_ips.json\")\n\t\tif err != nil {\n\t\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t\t\treturn\n\t\t}\n\t\t_, err = w.Write(dedicatedServerIPs)\n\t\tif err != nil {\n\t\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n\t\t\treturn\n\t\t}\n\t}\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: prometheus\nSource File Path(s): ['prometheus\\\\discovery_test.go\\\\source_files\\\\discovery.go']\nTest File Path: prometheus\\discovery_test.go\\discovery_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: prometheus\\discovery_test.go\\source_files\\discovery.go\n// Copyright 2020 The Prometheus Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage discovery\n\nimport (\n\t\"context\"\n\t\"log/slog\"\n\t\"reflect\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/common/config\"\n\n\t\"github.com/prometheus/prometheus/discovery/targetgroup\"\n)\n\n// Discoverer provides information about target groups. It maintains a set\n// of sources from which TargetGroups can originate. Whenever a discovery provider\n// detects a potential change, it sends the TargetGroup through its channel.\n//\n// Discoverer does not know if an actual change happened.\n// It does guarantee that it sends the new TargetGroup whenever a change happens.\n//\n// Discoverers should initially send a full set of all discoverable TargetGroups.\ntype Discoverer interface {\n\t// Run hands a channel to the discovery provider (Consul, DNS, etc.) through which\n\t// it can send updated target groups. It must return when the context is canceled.\n\t// It should not close the update channel on returning.\n\tRun(ctx context.Context, up chan<- []*targetgroup.Group)\n}\n\n// DiscovererMetrics are internal metrics of service discovery mechanisms.\ntype DiscovererMetrics interface {\n\tRegister() error\n\tUnregister()\n}\n\n// DiscovererOptions provides options for a Discoverer.\ntype DiscovererOptions struct {\n\tLogger *slog.Logger\n\n\tMetrics DiscovererMetrics\n\n\t// Extra HTTP client options to expose to Discoverers. This field may be\n\t// ignored; Discoverer implementations must opt-in to reading it.\n\tHTTPClientOptions []config.HTTPClientOption\n}\n\n// RefreshMetrics are used by the \"refresh\" package.\n// We define them here in the \"discovery\" package in order to avoid a cyclic dependency between\n// \"discovery\" and \"refresh\".\ntype RefreshMetrics struct {\n\tFailures prometheus.Counter\n\tDuration prometheus.Observer\n}\n\n// RefreshMetricsInstantiator instantiates the metrics used by the \"refresh\" package.\ntype RefreshMetricsInstantiator interface {\n\tInstantiate(mech string) *RefreshMetrics\n}\n\n// RefreshMetricsManager is an interface for registering, unregistering, and\n// instantiating metrics for the \"refresh\" package. Refresh metrics are\n// registered and unregistered outside of the service discovery mechanism. This\n// is so that the same metrics can be reused across different service discovery\n// mechanisms. To manage refresh metrics inside the SD mechanism, we'd need to\n// use const labels which are specific to that SD. However, doing so would also\n// expose too many unused metrics on the Prometheus /metrics endpoint.\ntype RefreshMetricsManager interface {\n\tDiscovererMetrics\n\tRefreshMetricsInstantiator\n}\n\n// A Config provides the configuration and constructor for a Discoverer.\ntype Config interface {\n\t// Name returns the name of the discovery mechanism.\n\tName() string\n\n\t// NewDiscoverer returns a Discoverer for the Config\n\t// with the given DiscovererOptions.\n\tNewDiscoverer(DiscovererOptions) (Discoverer, error)\n\n\t// NewDiscovererMetrics returns the metrics used by the service discovery.\n\tNewDiscovererMetrics(prometheus.Registerer, RefreshMetricsInstantiator) DiscovererMetrics\n}\n\n// Configs is a slice of Config values that uses custom YAML marshaling and unmarshaling\n// to represent itself as a mapping of the Config values grouped by their types.\ntype Configs []Config\n\n// SetDirectory joins any relative file paths with dir.\nfunc (c *Configs) SetDirectory(dir string) {\n\tfor _, c := range *c {\n\t\tif v, ok := c.(config.DirectorySetter); ok {\n\t\t\tv.SetDirectory(dir)\n\t\t}\n\t}\n}\n\n// UnmarshalYAML implements yaml.Unmarshaler.\nfunc (c *Configs) UnmarshalYAML(unmarshal func(interface{}) error) error {\n\tcfgTyp := reflect.StructOf(configFields)\n\tcfgPtr := reflect.New(cfgTyp)\n\tcfgVal := cfgPtr.Elem()\n\n\tif err := unmarshal(cfgPtr.Interface()); err != nil {\n\t\treturn replaceYAMLTypeError(err, cfgTyp, configsType)\n\t}\n\n\tvar err error\n\t*c, err = readConfigs(cfgVal, 0)\n\treturn err\n}\n\n// MarshalYAML implements yaml.Marshaler.\nfunc (c Configs) MarshalYAML() (interface{}, error) {\n\tcfgTyp := reflect.StructOf(configFields)\n\tcfgPtr := reflect.New(cfgTyp)\n\tcfgVal := cfgPtr.Elem()\n\n\tif err := writeConfigs(cfgVal, c); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn cfgPtr.Interface(), nil\n}\n\n// A StaticConfig is a Config that provides a static list of targets.\ntype StaticConfig []*targetgroup.Group\n\n// Name returns the name of the service discovery mechanism.\nfunc (StaticConfig) Name() string { return \"static\" }\n\n// NewDiscoverer returns a Discoverer for the Config.\nfunc (c StaticConfig) NewDiscoverer(DiscovererOptions) (Discoverer, error) {\n\treturn staticDiscoverer(c), nil\n}\n\n// NewDiscovererMetrics returns NoopDiscovererMetrics because no metrics are\n// needed for this service discovery mechanism.\nfunc (c StaticConfig) NewDiscovererMetrics(prometheus.Registerer, RefreshMetricsInstantiator) DiscovererMetrics {\n\treturn &NoopDiscovererMetrics{}\n}\n\ntype staticDiscoverer []*targetgroup.Group\n\nfunc (c staticDiscoverer) Run(ctx context.Context, up chan<- []*targetgroup.Group) {\n\t// TODO: existing implementation closes up chan, but documentation explicitly forbids it...?\n\tdefer close(up)\n\tselect {\n\tcase <-ctx.Done():\n\tcase up <- c:\n\t}\n}\n\n### Source File Dependency Files Content\n### Dependency File: prometheus\\discovery_test.go\\dependent_files\\targetgroup.go\n// Copyright 2013 The Prometheus Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage targetgroup\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\n\t\"github.com/prometheus/common/model\"\n)\n\n// Group is a set of targets with a common label set(production , test, staging etc.).\ntype Group struct {\n\t// Targets is a list of targets identified by a label set. Each target is\n\t// uniquely identifiable in the group by its address label.\n\tTargets []model.LabelSet\n\t// Labels is a set of labels that is common across all targets in the group.\n\tLabels model.LabelSet\n\n\t// Source is an identifier that describes a group of targets.\n\tSource string\n}\n\nfunc (tg Group) String() string {\n\treturn tg.Source\n}\n\n// UnmarshalYAML implements the yaml.Unmarshaler interface.\nfunc (tg *Group) UnmarshalYAML(unmarshal func(interface{}) error) error {\n\tg := struct {\n\t\tTargets []string       `yaml:\"targets\"`\n\t\tLabels  model.LabelSet `yaml:\"labels\"`\n\t}{}\n\tif err := unmarshal(&g); err != nil {\n\t\treturn err\n\t}\n\ttg.Targets = make([]model.LabelSet, 0, len(g.Targets))\n\tfor _, t := range g.Targets {\n\t\ttg.Targets = append(tg.Targets, model.LabelSet{\n\t\t\tmodel.AddressLabel: model.LabelValue(t),\n\t\t})\n\t}\n\ttg.Labels = g.Labels\n\treturn nil\n}\n\n// MarshalYAML implements the yaml.Marshaler interface.\nfunc (tg Group) MarshalYAML() (interface{}, error) {\n\tg := &struct {\n\t\tTargets []string       `yaml:\"targets\"`\n\t\tLabels  model.LabelSet `yaml:\"labels,omitempty\"`\n\t}{\n\t\tTargets: make([]string, 0, len(tg.Targets)),\n\t\tLabels:  tg.Labels,\n\t}\n\tfor _, t := range tg.Targets {\n\t\tg.Targets = append(g.Targets, string(t[model.AddressLabel]))\n\t}\n\treturn g, nil\n}\n\n// UnmarshalJSON implements the json.Unmarshaler interface.\nfunc (tg *Group) UnmarshalJSON(b []byte) error {\n\tg := struct {\n\t\tTargets []string       `json:\"targets\"`\n\t\tLabels  model.LabelSet `json:\"labels\"`\n\t}{}\n\n\tdec := json.NewDecoder(bytes.NewReader(b))\n\tdec.DisallowUnknownFields()\n\tif err := dec.Decode(&g); err != nil {\n\t\treturn err\n\t}\n\ttg.Targets = make([]model.LabelSet, 0, len(g.Targets))\n\tfor _, t := range g.Targets {\n\t\ttg.Targets = append(tg.Targets, model.LabelSet{\n\t\t\tmodel.AddressLabel: model.LabelValue(t),\n\t\t})\n\t}\n\ttg.Labels = g.Labels\n\treturn nil\n}\n\n// MarshalJSON implements the json.Marshaler interface.\nfunc (tg Group) MarshalJSON() ([]byte, error) {\n\tg := &struct {\n\t\tTargets []string       `json:\"targets\"`\n\t\tLabels  model.LabelSet `json:\"labels,omitempty\"`\n\t}{\n\t\tTargets: make([]string, 0, len(tg.Targets)),\n\t\tLabels:  tg.Labels,\n\t}\n\tfor _, t := range tg.Targets {\n\t\tg.Targets = append(g.Targets, string(t[model.AddressLabel]))\n\t}\n\treturn json.Marshal(g)\n}\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:47:45"}, {"role": "assistant", "content": "```go\n// Copyright 2024 The Prometheus Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage discovery\n\nimport (\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/require\"\n\t\"gopkg.in/yaml.v2\"\n)\n\nfunc TestConfigsCustomUnMarshalMarshal(t *testing.T) {\n\tinput := `static_configs:\n- targets:\n  - foo:1234\n  - bar:4321\n`\n\tcfg := &Configs{}\n\terr := yaml.UnmarshalStrict([]byte(input), cfg)\n\trequire.NoError(t, err)\n\n\toutput, err := yaml.Marshal(cfg)\n\trequire.NoError(t, err)\n\trequire.Equal(t, input, string(output))\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: prometheus\nSource File Path(s): ['prometheus\\\\generic_test.go\\\\source_files\\\\generic.go']\nTest File Path: prometheus\\generic_test.go\\generic_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: prometheus\\generic_test.go\\source_files\\generic.go\n// Copyright 2022 The Prometheus Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage histogram\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"math\"\n\t\"strings\"\n)\n\nconst (\n\tExponentialSchemaMax int32 = 8\n\tExponentialSchemaMin int32 = -4\n\tCustomBucketsSchema  int32 = -53\n)\n\nvar (\n\tErrHistogramCountNotBigEnough     = errors.New(\"histogram's observation count should be at least the number of observations found in the buckets\")\n\tErrHistogramCountMismatch         = errors.New(\"histogram's observation count should equal the number of observations found in the buckets (in absence of NaN)\")\n\tErrHistogramNegativeBucketCount   = errors.New(\"histogram has a bucket whose observation count is negative\")\n\tErrHistogramSpanNegativeOffset    = errors.New(\"histogram has a span whose offset is negative\")\n\tErrHistogramSpansBucketsMismatch  = errors.New(\"histogram spans specify different number of buckets than provided\")\n\tErrHistogramCustomBucketsMismatch = errors.New(\"histogram custom bounds are too few\")\n\tErrHistogramCustomBucketsInvalid  = errors.New(\"histogram custom bounds must be in strictly increasing order\")\n\tErrHistogramCustomBucketsInfinite = errors.New(\"histogram custom bounds must be finite\")\n\tErrHistogramsIncompatibleSchema   = errors.New(\"cannot apply this operation on histograms with a mix of exponential and custom bucket schemas\")\n\tErrHistogramsIncompatibleBounds   = errors.New(\"cannot apply this operation on custom buckets histograms with different custom bounds\")\n)\n\nfunc IsCustomBucketsSchema(s int32) bool {\n\treturn s == CustomBucketsSchema\n}\n\nfunc IsExponentialSchema(s int32) bool {\n\treturn s >= ExponentialSchemaMin && s <= ExponentialSchemaMax\n}\n\n// BucketCount is a type constraint for the count in a bucket, which can be\n// float64 (for type FloatHistogram) or uint64 (for type Histogram).\ntype BucketCount interface {\n\tfloat64 | uint64\n}\n\n// InternalBucketCount is used internally by Histogram and FloatHistogram. The\n// difference to the BucketCount above is that Histogram internally uses deltas\n// between buckets rather than absolute counts (while FloatHistogram uses\n// absolute counts directly). Go type parameters don't allow type\n// specialization. Therefore, where special treatment of deltas between buckets\n// vs. absolute counts is important, this information has to be provided as a\n// separate boolean parameter \"deltaBuckets\".\ntype InternalBucketCount interface {\n\tfloat64 | int64\n}\n\n// Bucket represents a bucket with lower and upper limit and the absolute count\n// of samples in the bucket. It also specifies if each limit is inclusive or\n// not. (Mathematically, inclusive limits create a closed interval, and\n// non-inclusive limits an open interval.)\n//\n// To represent cumulative buckets, Lower is set to -Inf, and the Count is then\n// cumulative (including the counts of all buckets for smaller values).\ntype Bucket[BC BucketCount] struct {\n\tLower, Upper                   float64\n\tLowerInclusive, UpperInclusive bool\n\tCount                          BC\n\n\t// Index within schema. To easily compare buckets that share the same\n\t// schema and sign (positive or negative). Irrelevant for the zero bucket.\n\tIndex int32\n}\n\n// strippedBucket is Bucket without bound values (which are expensive to calculate\n// and not used in certain use cases).\ntype strippedBucket[BC BucketCount] struct {\n\tcount BC\n\tindex int32\n}\n\n// String returns a string representation of a Bucket, using the usual\n// mathematical notation of '['/']' for inclusive bounds and '('/')' for\n// non-inclusive bounds.\nfunc (b Bucket[BC]) String() string {\n\tvar sb strings.Builder\n\tif b.LowerInclusive {\n\t\tsb.WriteRune('[')\n\t} else {\n\t\tsb.WriteRune('(')\n\t}\n\tfmt.Fprintf(&sb, \"%g,%g\", b.Lower, b.Upper)\n\tif b.UpperInclusive {\n\t\tsb.WriteRune(']')\n\t} else {\n\t\tsb.WriteRune(')')\n\t}\n\tfmt.Fprintf(&sb, \":%v\", b.Count)\n\treturn sb.String()\n}\n\n// BucketIterator iterates over the buckets of a Histogram, returning decoded\n// buckets.\ntype BucketIterator[BC BucketCount] interface {\n\t// Next advances the iterator by one.\n\tNext() bool\n\t// At returns the current bucket.\n\tAt() Bucket[BC]\n}\n\n// baseBucketIterator provides a struct that is shared by most BucketIterator\n// implementations, together with an implementation of the At method. This\n// iterator can be embedded in full implementations of BucketIterator to save on\n// code replication.\ntype baseBucketIterator[BC BucketCount, IBC InternalBucketCount] struct {\n\tschema  int32\n\tspans   []Span\n\tbuckets []IBC\n\n\tpositive bool // Whether this is for positive buckets.\n\n\tspansIdx   int    // Current span within spans slice.\n\tidxInSpan  uint32 // Index in the current span. 0 <= idxInSpan < span.Length.\n\tbucketsIdx int    // Current bucket within buckets slice.\n\n\tcurrCount IBC   // Count in the current bucket.\n\tcurrIdx   int32 // The actual bucket index.\n\n\tcustomValues []float64 // Bounds (usually upper) for histograms with custom buckets.\n}\n\nfunc (b *baseBucketIterator[BC, IBC]) At() Bucket[BC] {\n\treturn b.at(b.schema)\n}\n\n// at is an internal version of the exported At to enable using a different schema.\nfunc (b *baseBucketIterator[BC, IBC]) at(schema int32) Bucket[BC] {\n\tbucket := Bucket[BC]{\n\t\tCount: BC(b.currCount),\n\t\tIndex: b.currIdx,\n\t}\n\tif b.positive {\n\t\tbucket.Upper = getBound(b.currIdx, schema, b.customValues)\n\t\tbucket.Lower = getBound(b.currIdx-1, schema, b.customValues)\n\t} else {\n\t\tbucket.Lower = -getBound(b.currIdx, schema, b.customValues)\n\t\tbucket.Upper = -getBound(b.currIdx-1, schema, b.customValues)\n\t}\n\tif IsCustomBucketsSchema(schema) {\n\t\tbucket.LowerInclusive = b.currIdx == 0\n\t\tbucket.UpperInclusive = true\n\t} else {\n\t\tbucket.LowerInclusive = bucket.Lower < 0\n\t\tbucket.UpperInclusive = bucket.Upper > 0\n\t}\n\treturn bucket\n}\n\n// strippedAt returns current strippedBucket (which lacks bucket bounds but is cheaper to compute).\nfunc (b *baseBucketIterator[BC, IBC]) strippedAt() strippedBucket[BC] {\n\treturn strippedBucket[BC]{\n\t\tcount: BC(b.currCount),\n\t\tindex: b.currIdx,\n\t}\n}\n\n// compactBuckets is a generic function used by both Histogram.Compact and\n// FloatHistogram.Compact. Set deltaBuckets to true if the provided buckets are\n// deltas. Set it to false if the buckets contain absolute counts.\nfunc compactBuckets[IBC InternalBucketCount](buckets []IBC, spans []Span, maxEmptyBuckets int, deltaBuckets bool) ([]IBC, []Span) {\n\t// Fast path: If there are no empty buckets AND no offset in any span is\n\t// <= maxEmptyBuckets AND no span has length 0, there is nothing to do and we can return\n\t// immediately. We check that first because it's cheap and presumably\n\t// common.\n\tnothingToDo := true\n\tvar currentBucketAbsolute IBC\n\tfor _, bucket := range buckets {\n\t\tif deltaBuckets {\n\t\t\tcurrentBucketAbsolute += bucket\n\t\t} else {\n\t\t\tcurrentBucketAbsolute = bucket\n\t\t}\n\t\tif currentBucketAbsolute == 0 {\n\t\t\tnothingToDo = false\n\t\t\tbreak\n\t\t}\n\t}\n\tif nothingToDo {\n\t\tfor _, span := range spans {\n\t\t\tif int(span.Offset) <= maxEmptyBuckets || span.Length == 0 {\n\t\t\t\tnothingToDo = false\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif nothingToDo {\n\t\t\treturn buckets, spans\n\t\t}\n\t}\n\n\tvar iBucket, iSpan int\n\tvar posInSpan uint32\n\tcurrentBucketAbsolute = 0\n\n\t// Helper function.\n\temptyBucketsHere := func() int {\n\t\ti := 0\n\t\tabs := currentBucketAbsolute\n\t\tfor uint32(i)+posInSpan < spans[iSpan].Length && abs == 0 {\n\t\t\ti++\n\t\t\tif i+iBucket >= len(buckets) {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tabs = buckets[i+iBucket]\n\t\t}\n\t\treturn i\n\t}\n\n\t// Merge spans with zero-offset to avoid special cases later.\n\tif len(spans) > 1 {\n\t\tfor i, span := range spans[1:] {\n\t\t\tif span.Offset == 0 {\n\t\t\t\tspans[iSpan].Length += span.Length\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tiSpan++\n\t\t\tif i+1 != iSpan {\n\t\t\t\tspans[iSpan] = span\n\t\t\t}\n\t\t}\n\t\tspans = spans[:iSpan+1]\n\t\tiSpan = 0\n\t}\n\n\t// Merge spans with zero-length to avoid special cases later.\n\tfor i, span := range spans {\n\t\tif span.Length == 0 {\n\t\t\tif i+1 < len(spans) {\n\t\t\t\tspans[i+1].Offset += span.Offset\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tif i != iSpan {\n\t\t\tspans[iSpan] = span\n\t\t}\n\t\tiSpan++\n\t}\n\tspans = spans[:iSpan]\n\tiSpan = 0\n\n\t// Cut out empty buckets from start and end of spans, no matter\n\t// what. Also cut out empty buckets from the middle of a span but only\n\t// if there are more than maxEmptyBuckets consecutive empty buckets.\n\tfor iBucket < len(buckets) {\n\t\tif deltaBuckets {\n\t\t\tcurrentBucketAbsolute += buckets[iBucket]\n\t\t} else {\n\t\t\tcurrentBucketAbsolute = buckets[iBucket]\n\t\t}\n\t\tif nEmpty := emptyBucketsHere(); nEmpty > 0 {\n\t\t\tif posInSpan > 0 &&\n\t\t\t\tnEmpty < int(spans[iSpan].Length-posInSpan) &&\n\t\t\t\tnEmpty <= maxEmptyBuckets {\n\t\t\t\t// The empty buckets are in the middle of a\n\t\t\t\t// span, and there are few enough to not bother.\n\t\t\t\t// Just fast-forward.\n\t\t\t\tiBucket += nEmpty\n\t\t\t\tif deltaBuckets {\n\t\t\t\t\tcurrentBucketAbsolute = 0\n\t\t\t\t}\n\t\t\t\tposInSpan += uint32(nEmpty)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// In all other cases, we cut out the empty buckets.\n\t\t\tif deltaBuckets && iBucket+nEmpty < len(buckets) {\n\t\t\t\tcurrentBucketAbsolute = -buckets[iBucket]\n\t\t\t\tbuckets[iBucket+nEmpty] += buckets[iBucket]\n\t\t\t}\n\t\t\tbuckets = append(buckets[:iBucket], buckets[iBucket+nEmpty:]...)\n\t\t\tif posInSpan == 0 {\n\t\t\t\t// Start of span.\n\t\t\t\tif nEmpty == int(spans[iSpan].Length) {\n\t\t\t\t\t// The whole span is empty.\n\t\t\t\t\toffset := spans[iSpan].Offset\n\t\t\t\t\tspans = append(spans[:iSpan], spans[iSpan+1:]...)\n\t\t\t\t\tif len(spans) > iSpan {\n\t\t\t\t\t\tspans[iSpan].Offset += offset + int32(nEmpty)\n\t\t\t\t\t}\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tspans[iSpan].Length -= uint32(nEmpty)\n\t\t\t\tspans[iSpan].Offset += int32(nEmpty)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// It's in the middle or in the end of the span.\n\t\t\t// Split the current span.\n\t\t\tnewSpan := Span{\n\t\t\t\tOffset: int32(nEmpty),\n\t\t\t\tLength: spans[iSpan].Length - posInSpan - uint32(nEmpty),\n\t\t\t}\n\t\t\tspans[iSpan].Length = posInSpan\n\t\t\t// In any case, we have to split to the next span.\n\t\t\tiSpan++\n\t\t\tposInSpan = 0\n\t\t\tif newSpan.Length == 0 {\n\t\t\t\t// The span is empty, so we were already at the end of a span.\n\t\t\t\t// We don't have to insert the new span, just adjust the next\n\t\t\t\t// span's offset, if there is one.\n\t\t\t\tif iSpan < len(spans) {\n\t\t\t\t\tspans[iSpan].Offset += int32(nEmpty)\n\t\t\t\t}\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// Insert the new span.\n\t\t\tspans = append(spans, Span{})\n\t\t\tif iSpan+1 < len(spans) {\n\t\t\t\tcopy(spans[iSpan+1:], spans[iSpan:])\n\t\t\t}\n\t\t\tspans[iSpan] = newSpan\n\t\t\tcontinue\n\t\t}\n\t\tiBucket++\n\t\tposInSpan++\n\t\tif posInSpan >= spans[iSpan].Length {\n\t\t\tposInSpan = 0\n\t\t\tiSpan++\n\t\t}\n\t}\n\tif maxEmptyBuckets == 0 || len(buckets) == 0 {\n\t\treturn buckets, spans\n\t}\n\n\t// Finally, check if any offsets between spans are small enough to merge\n\t// the spans.\n\tiBucket = int(spans[0].Length)\n\tif deltaBuckets {\n\t\tcurrentBucketAbsolute = 0\n\t\tfor _, bucket := range buckets[:iBucket] {\n\t\t\tcurrentBucketAbsolute += bucket\n\t\t}\n\t}\n\tiSpan = 1\n\tfor iSpan < len(spans) {\n\t\tif int(spans[iSpan].Offset) > maxEmptyBuckets {\n\t\t\tl := int(spans[iSpan].Length)\n\t\t\tif deltaBuckets {\n\t\t\t\tfor _, bucket := range buckets[iBucket : iBucket+l] {\n\t\t\t\t\tcurrentBucketAbsolute += bucket\n\t\t\t\t}\n\t\t\t}\n\t\t\tiBucket += l\n\t\t\tiSpan++\n\t\t\tcontinue\n\t\t}\n\t\t// Merge span with previous one and insert empty buckets.\n\t\toffset := int(spans[iSpan].Offset)\n\t\tspans[iSpan-1].Length += uint32(offset) + spans[iSpan].Length\n\t\tspans = append(spans[:iSpan], spans[iSpan+1:]...)\n\t\tnewBuckets := make([]IBC, len(buckets)+offset)\n\t\tcopy(newBuckets, buckets[:iBucket])\n\t\tcopy(newBuckets[iBucket+offset:], buckets[iBucket:])\n\t\tif deltaBuckets {\n\t\t\tnewBuckets[iBucket] = -currentBucketAbsolute\n\t\t\tnewBuckets[iBucket+offset] += currentBucketAbsolute\n\t\t}\n\t\tiBucket += offset\n\t\tbuckets = newBuckets\n\t\tcurrentBucketAbsolute = buckets[iBucket]\n\t\t// Note that with many merges, it would be more efficient to\n\t\t// first record all the chunks of empty buckets to insert and\n\t\t// then do it in one go through all the buckets.\n\t}\n\n\treturn buckets, spans\n}\n\nfunc checkHistogramSpans(spans []Span, numBuckets int) error {\n\tvar spanBuckets int\n\tfor n, span := range spans {\n\t\tif n > 0 && span.Offset < 0 {\n\t\t\treturn fmt.Errorf(\"span number %d with offset %d: %w\", n+1, span.Offset, ErrHistogramSpanNegativeOffset)\n\t\t}\n\t\tspanBuckets += int(span.Length)\n\t}\n\tif spanBuckets != numBuckets {\n\t\treturn fmt.Errorf(\"spans need %d buckets, have %d buckets: %w\", spanBuckets, numBuckets, ErrHistogramSpansBucketsMismatch)\n\t}\n\treturn nil\n}\n\nfunc checkHistogramBuckets[BC BucketCount, IBC InternalBucketCount](buckets []IBC, count *BC, deltas bool) error {\n\tif len(buckets) == 0 {\n\t\treturn nil\n\t}\n\n\tvar last IBC\n\tfor i := 0; i < len(buckets); i++ {\n\t\tvar c IBC\n\t\tif deltas {\n\t\t\tc = last + buckets[i]\n\t\t} else {\n\t\t\tc = buckets[i]\n\t\t}\n\t\tif c < 0 {\n\t\t\treturn fmt.Errorf(\"bucket number %d has observation count of %v: %w\", i+1, c, ErrHistogramNegativeBucketCount)\n\t\t}\n\t\tlast = c\n\t\t*count += BC(c)\n\t}\n\n\treturn nil\n}\n\nfunc checkHistogramCustomBounds(bounds []float64, spans []Span, numBuckets int) error {\n\tprev := math.Inf(-1)\n\tfor _, curr := range bounds {\n\t\tif curr <= prev {\n\t\t\treturn fmt.Errorf(\"previous bound is %f and current is %f: %w\", prev, curr, ErrHistogramCustomBucketsInvalid)\n\t\t}\n\t\tprev = curr\n\t}\n\tif prev == math.Inf(1) {\n\t\treturn fmt.Errorf(\"last +Inf bound must not be explicitly defined: %w\", ErrHistogramCustomBucketsInfinite)\n\t}\n\n\tvar spanBuckets int\n\tvar totalSpanLength int\n\tfor n, span := range spans {\n\t\tif span.Offset < 0 {\n\t\t\treturn fmt.Errorf(\"span number %d with offset %d: %w\", n+1, span.Offset, ErrHistogramSpanNegativeOffset)\n\t\t}\n\t\tspanBuckets += int(span.Length)\n\t\ttotalSpanLength += int(span.Length) + int(span.Offset)\n\t}\n\tif spanBuckets != numBuckets {\n\t\treturn fmt.Errorf(\"spans need %d buckets, have %d buckets: %w\", spanBuckets, numBuckets, ErrHistogramSpansBucketsMismatch)\n\t}\n\tif (len(bounds) + 1) < totalSpanLength {\n\t\treturn fmt.Errorf(\"only %d custom bounds defined which is insufficient to cover total span length of %d: %w\", len(bounds), totalSpanLength, ErrHistogramCustomBucketsMismatch)\n\t}\n\n\treturn nil\n}\n\nfunc getBound(idx, schema int32, customValues []float64) float64 {\n\tif IsCustomBucketsSchema(schema) {\n\t\tlength := int32(len(customValues))\n\t\tswitch {\n\t\tcase idx > length || idx < -1:\n\t\t\tpanic(fmt.Errorf(\"index %d out of bounds for custom bounds of length %d\", idx, length))\n\t\tcase idx == length:\n\t\t\treturn math.Inf(1)\n\t\tcase idx == -1:\n\t\t\treturn math.Inf(-1)\n\t\tdefault:\n\t\t\treturn customValues[idx]\n\t\t}\n\t}\n\treturn getBoundExponential(idx, schema)\n}\n\nfunc getBoundExponential(idx, schema int32) float64 {\n\t// Here a bit of context about the behavior for the last bucket counting\n\t// regular numbers (called simply \"last bucket\" below) and the bucket\n\t// counting observations of \u00b1Inf (called \"inf bucket\" below, with an idx\n\t// one higher than that of the \"last bucket\"):\n\t//\n\t// If we apply the usual formula to the last bucket, its upper bound\n\t// would be calculated as +Inf. The reason is that the max possible\n\t// regular float64 number (math.MaxFloat64) doesn't coincide with one of\n\t// the calculated bucket boundaries. So the calculated boundary has to\n\t// be larger than math.MaxFloat64, and the only float64 larger than\n\t// math.MaxFloat64 is +Inf. However, we want to count actual\n\t// observations of \u00b1Inf in the inf bucket. Therefore, we have to treat\n\t// the upper bound of the last bucket specially and set it to\n\t// math.MaxFloat64. (The upper bound of the inf bucket, with its idx\n\t// being one higher than that of the last bucket, naturally comes out as\n\t// +Inf by the usual formula. So that's fine.)\n\t//\n\t// math.MaxFloat64 has a frac of 0.9999999999999999 and an exp of\n\t// 1024. If there were a float64 number following math.MaxFloat64, it\n\t// would have a frac of 1.0 and an exp of 1024, or equivalently a frac\n\t// of 0.5 and an exp of 1025. However, since frac must be smaller than\n\t// 1, and exp must be smaller than 1025, either representation overflows\n\t// a float64. (Which, in turn, is the reason that math.MaxFloat64 is the\n\t// largest possible float64. Q.E.D.) However, the formula for\n\t// calculating the upper bound from the idx and schema of the last\n\t// bucket results in precisely that. It is either frac=1.0 & exp=1024\n\t// (for schema < 0) or frac=0.5 & exp=1025 (for schema >=0). (This is,\n\t// by the way, a power of two where the exponent itself is a power of\n\t// two, 2\u00b9\u2070 in fact, which coincides with a bucket boundary in all\n\t// schemas.) So these are the special cases we have to catch below.\n\tif schema < 0 {\n\t\texp := int(idx) << -schema\n\t\tif exp == 1024 {\n\t\t\t// This is the last bucket before the overflow bucket\n\t\t\t// (for \u00b1Inf observations). Return math.MaxFloat64 as\n\t\t\t// explained above.\n\t\t\treturn math.MaxFloat64\n\t\t}\n\t\treturn math.Ldexp(1, exp)\n\t}\n\n\tfracIdx := idx & ((1 << schema) - 1)\n\tfrac := exponentialBounds[schema][fracIdx]\n\texp := (int(idx) >> schema) + 1\n\tif frac == 0.5 && exp == 1025 {\n\t\t// This is the last bucket before the overflow bucket (for \u00b1Inf\n\t\t// observations). Return math.MaxFloat64 as explained above.\n\t\treturn math.MaxFloat64\n\t}\n\treturn math.Ldexp(frac, exp)\n}\n\n// exponentialBounds is a precalculated table of bucket bounds in the interval\n// [0.5,1) in schema 0 to 8.\nvar exponentialBounds = [][]float64{\n\t// Schema \"0\":\n\t{0.5},\n\t// Schema 1:\n\t{0.5, 0.7071067811865475},\n\t// Schema 2:\n\t{0.5, 0.5946035575013605, 0.7071067811865475, 0.8408964152537144},\n\t// Schema 3:\n\t{\n\t\t0.5, 0.5452538663326288, 0.5946035575013605, 0.6484197773255048,\n\t\t0.7071067811865475, 0.7711054127039704, 0.8408964152537144, 0.9170040432046711,\n\t},\n\t// Schema 4:\n\t{\n\t\t0.5, 0.5221368912137069, 0.5452538663326288, 0.5693943173783458,\n\t\t0.5946035575013605, 0.620928906036742, 0.6484197773255048, 0.6771277734684463,\n\t\t0.7071067811865475, 0.7384130729697496, 0.7711054127039704, 0.805245165974627,\n\t\t0.8408964152537144, 0.8781260801866495, 0.9170040432046711, 0.9576032806985735,\n\t},\n\t// Schema 5:\n\t{\n\t\t0.5, 0.5109485743270583, 0.5221368912137069, 0.5335702003384117,\n\t\t0.5452538663326288, 0.5571933712979462, 0.5693943173783458, 0.5818624293887887,\n\t\t0.5946035575013605, 0.6076236799902344, 0.620928906036742, 0.6345254785958666,\n\t\t0.6484197773255048, 0.6626183215798706, 0.6771277734684463, 0.6919549409819159,\n\t\t0.7071067811865475, 0.7225904034885232, 0.7384130729697496, 0.7545822137967112,\n\t\t0.7711054127039704, 0.7879904225539431, 0.805245165974627, 0.8228777390769823,\n\t\t0.8408964152537144, 0.8593096490612387, 0.8781260801866495, 0.8973545375015533,\n\t\t0.9170040432046711, 0.9370838170551498, 0.9576032806985735, 0.9785720620876999,\n\t},\n\t// Schema 6:\n\t{\n\t\t0.5, 0.5054446430258502, 0.5109485743270583, 0.5165124395106142,\n\t\t0.5221368912137069, 0.5278225891802786, 0.5335702003384117, 0.5393803988785598,\n\t\t0.5452538663326288, 0.5511912916539204, 0.5571933712979462, 0.5632608093041209,\n\t\t0.5693943173783458, 0.5755946149764913, 0.5818624293887887, 0.5881984958251406,\n\t\t0.5946035575013605, 0.6010783657263515, 0.6076236799902344, 0.6142402680534349,\n\t\t0.620928906036742, 0.6276903785123455, 0.6345254785958666, 0.6414350080393891,\n\t\t0.6484197773255048, 0.6554806057623822, 0.6626183215798706, 0.6698337620266515,\n\t\t0.6771277734684463, 0.6845012114872953, 0.6919549409819159, 0.6994898362691555,\n\t\t0.7071067811865475, 0.7148066691959849, 0.7225904034885232, 0.7304588970903234,\n\t\t0.7384130729697496, 0.7464538641456323, 0.7545822137967112, 0.762799075372269,\n\t\t0.7711054127039704, 0.7795022001189185, 0.7879904225539431, 0.7965710756711334,\n\t\t0.805245165974627, 0.8140137109286738, 0.8228777390769823, 0.8318382901633681,\n\t\t0.8408964152537144, 0.8500531768592616, 0.8593096490612387, 0.8686669176368529,\n\t\t0.8781260801866495, 0.8876882462632604, 0.8973545375015533, 0.9071260877501991,\n\t\t0.9170040432046711, 0.9269895625416926, 0.9370838170551498, 0.9472879907934827,\n\t\t0.9576032806985735, 0.9680308967461471, 0.9785720620876999, 0.9892280131939752,\n\t},\n\t// Schema 7:\n\t{\n\t\t0.5, 0.5027149505564014, 0.5054446430258502, 0.5081891574554764,\n\t\t0.5109485743270583, 0.5137229745593818, 0.5165124395106142, 0.5193170509806894,\n\t\t0.5221368912137069, 0.5249720429003435, 0.5278225891802786, 0.5306886136446309,\n\t\t0.5335702003384117, 0.5364674337629877, 0.5393803988785598, 0.5423091811066545,\n\t\t0.5452538663326288, 0.5482145409081883, 0.5511912916539204, 0.5541842058618393,\n\t\t0.5571933712979462, 0.5602188762048033, 0.5632608093041209, 0.5663192597993595,\n\t\t0.5693943173783458, 0.572486072215902, 0.5755946149764913, 0.5787200368168754,\n\t\t0.5818624293887887, 0.585021884841625, 0.5881984958251406, 0.5913923554921704,\n\t\t0.5946035575013605, 0.5978321960199137, 0.6010783657263515, 0.6043421618132907,\n\t\t0.6076236799902344, 0.6109230164863786, 0.6142402680534349, 0.6175755319684665,\n\t\t0.620928906036742, 0.6243004885946023, 0.6276903785123455, 0.6310986751971253,\n\t\t0.6345254785958666, 0.637970889198196, 0.6414350080393891, 0.6449179367033329,\n\t\t0.6484197773255048, 0.6519406325959679, 0.6554806057623822, 0.659039800633032,\n\t\t0.6626183215798706, 0.6662162735415805, 0.6698337620266515, 0.6734708931164728,\n\t\t0.6771277734684463, 0.6808045103191123, 0.6845012114872953, 0.688217985377265,\n\t\t0.6919549409819159, 0.6957121878859629, 0.6994898362691555, 0.7032879969095076,\n\t\t0.7071067811865475, 0.7109463010845827, 0.7148066691959849, 0.718687998724491,\n\t\t0.7225904034885232, 0.7265139979245261, 0.7304588970903234, 0.7344252166684908,\n\t\t0.7384130729697496, 0.7424225829363761, 0.7464538641456323, 0.7505070348132126,\n\t\t0.7545822137967112, 0.7586795205991071, 0.762799075372269, 0.7669409989204777,\n\t\t0.7711054127039704, 0.7752924388424999, 0.7795022001189185, 0.7837348199827764,\n\t\t0.7879904225539431, 0.7922691326262467, 0.7965710756711334, 0.8008963778413465,\n\t\t0.805245165974627, 0.8096175675974316, 0.8140137109286738, 0.8184337248834821,\n\t\t0.8228777390769823, 0.8273458838280969, 0.8318382901633681, 0.8363550898207981,\n\t\t0.8408964152537144, 0.8454623996346523, 0.8500531768592616, 0.8546688815502312,\n\t\t0.8593096490612387, 0.8639756154809185, 0.8686669176368529, 0.8733836930995842,\n\t\t0.8781260801866495, 0.8828942179666361, 0.8876882462632604, 0.8925083056594671,\n\t\t0.8973545375015533, 0.9022270839033115, 0.9071260877501991, 0.9120516927035263,\n\t\t0.9170040432046711, 0.9219832844793128, 0.9269895625416926, 0.9320230241988943,\n\t\t0.9370838170551498, 0.9421720895161669, 0.9472879907934827, 0.9524316709088368,\n\t\t0.9576032806985735, 0.9628029718180622, 0.9680308967461471, 0.9732872087896164,\n\t\t0.9785720620876999, 0.9838856116165875, 0.9892280131939752, 0.9945994234836328,\n\t},\n\t// Schema 8:\n\t{\n\t\t0.5, 0.5013556375251013, 0.5027149505564014, 0.5040779490592088,\n\t\t0.5054446430258502, 0.5068150424757447, 0.5081891574554764, 0.509566998038869,\n\t\t0.5109485743270583, 0.5123338964485679, 0.5137229745593818, 0.5151158188430205,\n\t\t0.5165124395106142, 0.5179128468009786, 0.5193170509806894, 0.520725062344158,\n\t\t0.5221368912137069, 0.5235525479396449, 0.5249720429003435, 0.526395386502313,\n\t\t0.5278225891802786, 0.5292536613972564, 0.5306886136446309, 0.5321274564422321,\n\t\t0.5335702003384117, 0.5350168559101208, 0.5364674337629877, 0.5379219445313954,\n\t\t0.5393803988785598, 0.5408428074966075, 0.5423091811066545, 0.5437795304588847,\n\t\t0.5452538663326288, 0.5467321995364429, 0.5482145409081883, 0.549700901315111,\n\t\t0.5511912916539204, 0.5526857228508706, 0.5541842058618393, 0.5556867516724088,\n\t\t0.5571933712979462, 0.5587040757836845, 0.5602188762048033, 0.5617377836665098,\n\t\t0.5632608093041209, 0.564787964283144, 0.5663192597993595, 0.5678547070789026,\n\t\t0.5693943173783458, 0.5709381019847808, 0.572486072215902, 0.5740382394200894,\n\t\t0.5755946149764913, 0.5771552102951081, 0.5787200368168754, 0.5802891060137493,\n\t\t0.5818624293887887, 0.5834400184762408, 0.585021884841625, 0.5866080400818185,\n\t\t0.5881984958251406, 0.5897932637314379, 0.5913923554921704, 0.5929957828304968,\n\t\t0.5946035575013605, 0.5962156912915756, 0.5978321960199137, 0.5994530835371903,\n\t\t0.6010783657263515, 0.6027080545025619, 0.6043421618132907, 0.6059806996384005,\n\t\t0.6076236799902344, 0.6092711149137041, 0.6109230164863786, 0.6125793968185725,\n\t\t0.6142402680534349, 0.6159056423670379, 0.6175755319684665, 0.6192499490999082,\n\t\t0.620928906036742, 0.622612415087629, 0.6243004885946023, 0.6259931389331581,\n\t\t0.6276903785123455, 0.6293922197748583, 0.6310986751971253, 0.6328097572894031,\n\t\t0.6345254785958666, 0.6362458516947014, 0.637970889198196, 0.6397006037528346,\n\t\t0.6414350080393891, 0.6431741147730128, 0.6449179367033329, 0.6466664866145447,\n\t\t0.6484197773255048, 0.6501778216898253, 0.6519406325959679, 0.6537082229673385,\n\t\t0.6554806057623822, 0.6572577939746774, 0.659039800633032, 0.6608266388015788,\n\t\t0.6626183215798706, 0.6644148621029772, 0.6662162735415805, 0.6680225691020727,\n\t\t0.6698337620266515, 0.6716498655934177, 0.6734708931164728, 0.6752968579460171,\n\t\t0.6771277734684463, 0.6789636531064505, 0.6808045103191123, 0.6826503586020058,\n\t\t0.6845012114872953, 0.6863570825438342, 0.688217985377265, 0.690083933630119,\n\t\t0.6919549409819159, 0.6938310211492645, 0.6957121878859629, 0.6975984549830999,\n\t\t0.6994898362691555, 0.7013863456101023, 0.7032879969095076, 0.7051948041086352,\n\t\t0.7071067811865475, 0.7090239421602076, 0.7109463010845827, 0.7128738720527471,\n\t\t0.7148066691959849, 0.7167447066838943, 0.718687998724491, 0.7206365595643126,\n\t\t0.7225904034885232, 0.7245495448210174, 0.7265139979245261, 0.7284837772007218,\n\t\t0.7304588970903234, 0.7324393720732029, 0.7344252166684908, 0.7364164454346837,\n\t\t0.7384130729697496, 0.7404151139112358, 0.7424225829363761, 0.7444354947621984,\n\t\t0.7464538641456323, 0.7484777058836176, 0.7505070348132126, 0.7525418658117031,\n\t\t0.7545822137967112, 0.7566280937263048, 0.7586795205991071, 0.7607365094544071,\n\t\t0.762799075372269, 0.7648672334736434, 0.7669409989204777, 0.7690203869158282,\n\t\t0.7711054127039704, 0.7731960915705107, 0.7752924388424999, 0.7773944698885442,\n\t\t0.7795022001189185, 0.7816156449856788, 0.7837348199827764, 0.7858597406461707,\n\t\t0.7879904225539431, 0.7901268813264122, 0.7922691326262467, 0.7944171921585818,\n\t\t0.7965710756711334, 0.7987307989543135, 0.8008963778413465, 0.8030678282083853,\n\t\t0.805245165974627, 0.8074284071024302, 0.8096175675974316, 0.8118126635086642,\n\t\t0.8140137109286738, 0.8162207259936375, 0.8184337248834821, 0.820652723822003,\n\t\t0.8228777390769823, 0.8251087869603088, 0.8273458838280969, 0.8295890460808079,\n\t\t0.8318382901633681, 0.8340936325652911, 0.8363550898207981, 0.8386226785089391,\n\t\t0.8408964152537144, 0.8431763167241966, 0.8454623996346523, 0.8477546807446661,\n\t\t0.8500531768592616, 0.8523579048290255, 0.8546688815502312, 0.8569861239649629,\n\t\t0.8593096490612387, 0.8616394738731368, 0.8639756154809185, 0.8663180910111553,\n\t\t0.8686669176368529, 0.871022112577578, 0.8733836930995842, 0.8757516765159389,\n\t\t0.8781260801866495, 0.8805069215187917, 0.8828942179666361, 0.8852879870317771,\n\t\t0.8876882462632604, 0.890095013257712, 0.8925083056594671, 0.8949281411607002,\n\t\t0.8973545375015533, 0.8997875124702672, 0.9022270839033115, 0.9046732696855155,\n\t\t0.9071260877501991, 0.909585556079304, 0.9120516927035263, 0.9145245157024483,\n\t\t0.9170040432046711, 0.9194902933879467, 0.9219832844793128, 0.9244830347552253,\n\t\t0.9269895625416926, 0.92950288621441, 0.9320230241988943, 0.9345499949706191,\n\t\t0.9370838170551498, 0.93962450902828, 0.9421720895161669, 0.9447265771954693,\n\t\t0.9472879907934827, 0.9498563490882775, 0.9524316709088368, 0.9550139751351947,\n\t\t0.9576032806985735, 0.9601996065815236, 0.9628029718180622, 0.9654133954938133,\n\t\t0.9680308967461471, 0.9706554947643201, 0.9732872087896164, 0.9759260581154889,\n\t\t0.9785720620876999, 0.9812252401044634, 0.9838856116165875, 0.9865531961276168,\n\t\t0.9892280131939752, 0.9919100824251095, 0.9945994234836328, 0.9972960560854698,\n\t},\n}\n\n// reduceResolution reduces the input spans, buckets in origin schema to the spans, buckets in target schema.\n// The target schema must be smaller than the original schema.\n// Set deltaBuckets to true if the provided buckets are\n// deltas. Set it to false if the buckets contain absolute counts.\n// Set inplace to true to reuse input slices and avoid allocations (otherwise\n// new slices will be allocated for result).\nfunc reduceResolution[IBC InternalBucketCount](\n\toriginSpans []Span,\n\toriginBuckets []IBC,\n\toriginSchema,\n\ttargetSchema int32,\n\tdeltaBuckets bool,\n\tinplace bool,\n) ([]Span, []IBC) {\n\tvar (\n\t\ttargetSpans           []Span // The spans in the target schema.\n\t\ttargetBuckets         []IBC  // The bucket counts in the target schema.\n\t\tbucketIdx             int32  // The index of bucket in the origin schema.\n\t\tbucketCountIdx        int    // The position of a bucket in origin bucket count slice `originBuckets`.\n\t\ttargetBucketIdx       int32  // The index of bucket in the target schema.\n\t\tlastBucketCount       IBC    // The last visited bucket's count in the origin schema.\n\t\tlastTargetBucketIdx   int32  // The index of the last added target bucket.\n\t\tlastTargetBucketCount IBC\n\t)\n\n\tif inplace {\n\t\t// Slice reuse is safe because when reducing the resolution,\n\t\t// target slices don't grow faster than origin slices are being read.\n\t\ttargetSpans = originSpans[:0]\n\t\ttargetBuckets = originBuckets[:0]\n\t}\n\n\tfor _, span := range originSpans {\n\t\t// Determine the index of the first bucket in this span.\n\t\tbucketIdx += span.Offset\n\t\tfor j := 0; j < int(span.Length); j++ {\n\t\t\t// Determine the index of the bucket in the target schema from the index in the original schema.\n\t\t\ttargetBucketIdx = targetIdx(bucketIdx, originSchema, targetSchema)\n\n\t\t\tswitch {\n\t\t\tcase len(targetSpans) == 0:\n\t\t\t\t// This is the first span in the targetSpans.\n\t\t\t\tspan := Span{\n\t\t\t\t\tOffset: targetBucketIdx,\n\t\t\t\t\tLength: 1,\n\t\t\t\t}\n\t\t\t\ttargetSpans = append(targetSpans, span)\n\t\t\t\ttargetBuckets = append(targetBuckets, originBuckets[bucketCountIdx])\n\t\t\t\tlastTargetBucketIdx = targetBucketIdx\n\t\t\t\tlastBucketCount = originBuckets[bucketCountIdx]\n\t\t\t\tlastTargetBucketCount = originBuckets[bucketCountIdx]\n\n\t\t\tcase lastTargetBucketIdx == targetBucketIdx:\n\t\t\t\t// The current bucket has to be merged into the same target bucket as the previous bucket.\n\t\t\t\tif deltaBuckets {\n\t\t\t\t\tlastBucketCount += originBuckets[bucketCountIdx]\n\t\t\t\t\ttargetBuckets[len(targetBuckets)-1] += lastBucketCount\n\t\t\t\t\tlastTargetBucketCount += lastBucketCount\n\t\t\t\t} else {\n\t\t\t\t\ttargetBuckets[len(targetBuckets)-1] += originBuckets[bucketCountIdx]\n\t\t\t\t}\n\n\t\t\tcase (lastTargetBucketIdx + 1) == targetBucketIdx:\n\t\t\t\t// The current bucket has to go into a new target bucket,\n\t\t\t\t// and that bucket is next to the previous target bucket,\n\t\t\t\t// so we add it to the current target span.\n\t\t\t\ttargetSpans[len(targetSpans)-1].Length++\n\t\t\t\tlastTargetBucketIdx++\n\t\t\t\tif deltaBuckets {\n\t\t\t\t\tlastBucketCount += originBuckets[bucketCountIdx]\n\t\t\t\t\ttargetBuckets = append(targetBuckets, lastBucketCount-lastTargetBucketCount)\n\t\t\t\t\tlastTargetBucketCount = lastBucketCount\n\t\t\t\t} else {\n\t\t\t\t\ttargetBuckets = append(targetBuckets, originBuckets[bucketCountIdx])\n\t\t\t\t}\n\n\t\t\tcase (lastTargetBucketIdx + 1) < targetBucketIdx:\n\t\t\t\t// The current bucket has to go into a new target bucket,\n\t\t\t\t// and that bucket is separated by a gap from the previous target bucket,\n\t\t\t\t// so we need to add a new target span.\n\t\t\t\tspan := Span{\n\t\t\t\t\tOffset: targetBucketIdx - lastTargetBucketIdx - 1,\n\t\t\t\t\tLength: 1,\n\t\t\t\t}\n\t\t\t\ttargetSpans = append(targetSpans, span)\n\t\t\t\tlastTargetBucketIdx = targetBucketIdx\n\t\t\t\tif deltaBuckets {\n\t\t\t\t\tlastBucketCount += originBuckets[bucketCountIdx]\n\t\t\t\t\ttargetBuckets = append(targetBuckets, lastBucketCount-lastTargetBucketCount)\n\t\t\t\t\tlastTargetBucketCount = lastBucketCount\n\t\t\t\t} else {\n\t\t\t\t\ttargetBuckets = append(targetBuckets, originBuckets[bucketCountIdx])\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tbucketIdx++\n\t\t\tbucketCountIdx++\n\t\t}\n\t}\n\n\treturn targetSpans, targetBuckets\n}\n\nfunc clearIfNotNil[T any](items []T) []T {\n\tif items == nil {\n\t\treturn nil\n\t}\n\treturn items[:0]\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:47:45"}, {"role": "assistant", "content": "```go\n// Copyright 2022 The Prometheus Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage histogram\n\nimport (\n\t\"math\"\n\t\"slices\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestGetBoundExponential(t *testing.T) {\n\tscenarios := []struct {\n\t\tidx    int32\n\t\tschema int32\n\t\twant   float64\n\t}{\n\t\t{\n\t\t\tidx:    -1,\n\t\t\tschema: -1,\n\t\t\twant:   0.25,\n\t\t},\n\t\t{\n\t\t\tidx:    0,\n\t\t\tschema: -1,\n\t\t\twant:   1,\n\t\t},\n\t\t{\n\t\t\tidx:    1,\n\t\t\tschema: -1,\n\t\t\twant:   4,\n\t\t},\n\t\t{\n\t\t\tidx:    512,\n\t\t\tschema: -1,\n\t\t\twant:   math.MaxFloat64,\n\t\t},\n\t\t{\n\t\t\tidx:    513,\n\t\t\tschema: -1,\n\t\t\twant:   math.Inf(+1),\n\t\t},\n\t\t{\n\t\t\tidx:    -1,\n\t\t\tschema: 0,\n\t\t\twant:   0.5,\n\t\t},\n\t\t{\n\t\t\tidx:    0,\n\t\t\tschema: 0,\n\t\t\twant:   1,\n\t\t},\n\t\t{\n\t\t\tidx:    1,\n\t\t\tschema: 0,\n\t\t\twant:   2,\n\t\t},\n\t\t{\n\t\t\tidx:    1024,\n\t\t\tschema: 0,\n\t\t\twant:   math.MaxFloat64,\n\t\t},\n\t\t{\n\t\t\tidx:    1025,\n\t\t\tschema: 0,\n\t\t\twant:   math.Inf(+1),\n\t\t},\n\t\t{\n\t\t\tidx:    -1,\n\t\t\tschema: 2,\n\t\t\twant:   0.8408964152537144,\n\t\t},\n\t\t{\n\t\t\tidx:    0,\n\t\t\tschema: 2,\n\t\t\twant:   1,\n\t\t},\n\t\t{\n\t\t\tidx:    1,\n\t\t\tschema: 2,\n\t\t\twant:   1.189207115002721,\n\t\t},\n\t\t{\n\t\t\tidx:    4096,\n\t\t\tschema: 2,\n\t\t\twant:   math.MaxFloat64,\n\t\t},\n\t\t{\n\t\t\tidx:    4097,\n\t\t\tschema: 2,\n\t\t\twant:   math.Inf(+1),\n\t\t},\n\t}\n\n\tfor _, s := range scenarios {\n\t\tgot := getBoundExponential(s.idx, s.schema)\n\t\tif s.want != got {\n\t\t\trequire.Equal(t, s.want, got, \"idx %d, schema %d\", s.idx, s.schema)\n\t\t}\n\t}\n}\n\nfunc TestReduceResolutionHistogram(t *testing.T) {\n\tcases := []struct {\n\t\tspans           []Span\n\t\tbuckets         []int64\n\t\tschema          int32\n\t\ttargetSchema    int32\n\t\texpectedSpans   []Span\n\t\texpectedBuckets []int64\n\t}{\n\t\t{\n\t\t\tspans: []Span{\n\t\t\t\t{Offset: 0, Length: 4},\n\t\t\t\t{Offset: 0, Length: 0},\n\t\t\t\t{Offset: 3, Length: 2},\n\t\t\t},\n\t\t\tbuckets:      []int64{1, 2, -2, 1, -1, 0},\n\t\t\tschema:       0,\n\t\t\ttargetSchema: -1,\n\t\t\texpectedSpans: []Span{\n\t\t\t\t{Offset: 0, Length: 3},\n\t\t\t\t{Offset: 1, Length: 1},\n\t\t\t},\n\t\t\texpectedBuckets: []int64{1, 3, -2, 0},\n\t\t\t// schema 0, base 2 { (0.5, 1]:1  (1,2]:3, (2,4]:1, (4,8]:2, (8,16]:0, (16,32]:0, (32,64]:0, (64,128]:1, (128,256]:1}\",\n\t\t\t// schema 1, base 4 { (0.25, 1):1 (1,4]:4,          (4,16]:2,          (16,64]:0,            (64,256]:2}\n\t\t},\n\t}\n\n\tfor _, tc := range cases {\n\t\tspansCopy, bucketsCopy := slices.Clone(tc.spans), slices.Clone(tc.buckets)\n\t\tspans, buckets := reduceResolution(tc.spans, tc.buckets, tc.schema, tc.targetSchema, true, false)\n\t\trequire.Equal(t, tc.expectedSpans, spans)\n\t\trequire.Equal(t, tc.expectedBuckets, buckets)\n\t\t// Verify inputs were not mutated:\n\t\trequire.Equal(t, spansCopy, tc.spans)\n\t\trequire.Equal(t, bucketsCopy, tc.buckets)\n\n\t\t// Output slices reuse input slices:\n\t\tconst inplace = true\n\t\tspans, buckets = reduceResolution(tc.spans, tc.buckets, tc.schema, tc.targetSchema, true, inplace)\n\t\trequire.Equal(t, tc.expectedSpans, spans)\n\t\trequire.Equal(t, tc.expectedBuckets, buckets)\n\t\t// Verify inputs were mutated which is now expected:\n\t\trequire.Equal(t, spans, tc.spans[:len(spans)])\n\t\trequire.Equal(t, buckets, tc.buckets[:len(buckets)])\n\t}\n}\n\nfunc TestReduceResolutionFloatHistogram(t *testing.T) {\n\tcases := []struct {\n\t\tspans           []Span\n\t\tbuckets         []float64\n\t\tschema          int32\n\t\ttargetSchema    int32\n\t\texpectedSpans   []Span\n\t\texpectedBuckets []float64\n\t}{\n\t\t{\n\t\t\tspans: []Span{\n\t\t\t\t{Offset: 0, Length: 4},\n\t\t\t\t{Offset: 0, Length: 0},\n\t\t\t\t{Offset: 3, Length: 2},\n\t\t\t},\n\t\t\tbuckets:      []float64{1, 3, 1, 2, 1, 1},\n\t\t\tschema:       0,\n\t\t\ttargetSchema: -1,\n\t\t\texpectedSpans: []Span{\n\t\t\t\t{Offset: 0, Length: 3},\n\t\t\t\t{Offset: 1, Length: 1},\n\t\t\t},\n\t\t\texpectedBuckets: []float64{1, 4, 2, 2},\n\t\t\t// schema 0, base 2 { (0.5, 1]:1  (1,2]:3, (2,4]:1, (4,8]:2, (8,16]:0, (16,32]:0, (32,64]:0, (64,128]:1, (128,256]:1}\",\n\t\t\t// schema 1, base 4 { (0.25, 1):1 (1,4]:4,          (4,16]:2,          (16,64]:0,            (64,256]:2}\n\t\t},\n\t}\n\n\tfor _, tc := range cases {\n\t\tspansCopy, bucketsCopy := slices.Clone(tc.spans), slices.Clone(tc.buckets)\n\t\tspans, buckets := reduceResolution(tc.spans, tc.buckets, tc.schema, tc.targetSchema, false, false)\n\t\trequire.Equal(t, tc.expectedSpans, spans)\n\t\trequire.Equal(t, tc.expectedBuckets, buckets)\n\t\t// Verify inputs were not mutated:\n\t\trequire.Equal(t, spansCopy, tc.spans)\n\t\trequire.Equal(t, bucketsCopy, tc.buckets)\n\n\t\t// Output slices reuse input slices:\n\t\tconst inplace = true\n\t\tspans, buckets = reduceResolution(tc.spans, tc.buckets, tc.schema, tc.targetSchema, false, inplace)\n\t\trequire.Equal(t, tc.expectedSpans, spans)\n\t\trequire.Equal(t, tc.expectedBuckets, buckets)\n\t\t// Verify inputs were mutated which is now expected:\n\t\trequire.Equal(t, spans, tc.spans[:len(spans)])\n\t\trequire.Equal(t, buckets, tc.buckets[:len(buckets)])\n\t}\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: prometheus\nSource File Path(s): ['prometheus\\\\hypervisor_test.go\\\\source_files\\\\hypervisor.go']\nTest File Path: prometheus\\hypervisor_test.go\\hypervisor_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: prometheus\\hypervisor_test.go\\source_files\\hypervisor.go\n// Copyright 2017 The Prometheus Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage openstack\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"net\"\n\t\"strconv\"\n\n\t\"github.com/gophercloud/gophercloud/v2\"\n\t\"github.com/gophercloud/gophercloud/v2/openstack\"\n\t\"github.com/gophercloud/gophercloud/v2/openstack/compute/v2/hypervisors\"\n\t\"github.com/gophercloud/gophercloud/v2/pagination\"\n\t\"github.com/prometheus/common/model\"\n\n\t\"github.com/prometheus/prometheus/discovery/targetgroup\"\n)\n\nconst (\n\topenstackLabelHypervisorID       = openstackLabelPrefix + \"hypervisor_id\"\n\topenstackLabelHypervisorHostIP   = openstackLabelPrefix + \"hypervisor_host_ip\"\n\topenstackLabelHypervisorHostName = openstackLabelPrefix + \"hypervisor_hostname\"\n\topenstackLabelHypervisorStatus   = openstackLabelPrefix + \"hypervisor_status\"\n\topenstackLabelHypervisorState    = openstackLabelPrefix + \"hypervisor_state\"\n\topenstackLabelHypervisorType     = openstackLabelPrefix + \"hypervisor_type\"\n)\n\n// HypervisorDiscovery discovers OpenStack hypervisors.\ntype HypervisorDiscovery struct {\n\tprovider     *gophercloud.ProviderClient\n\tauthOpts     *gophercloud.AuthOptions\n\tregion       string\n\tlogger       *slog.Logger\n\tport         int\n\tavailability gophercloud.Availability\n}\n\n// newHypervisorDiscovery returns a new hypervisor discovery.\nfunc newHypervisorDiscovery(provider *gophercloud.ProviderClient, opts *gophercloud.AuthOptions,\n\tport int, region string, availability gophercloud.Availability, l *slog.Logger,\n) *HypervisorDiscovery {\n\treturn &HypervisorDiscovery{\n\t\tprovider: provider, authOpts: opts,\n\t\tregion: region, port: port, availability: availability, logger: l,\n\t}\n}\n\nfunc (h *HypervisorDiscovery) refresh(ctx context.Context) ([]*targetgroup.Group, error) {\n\terr := openstack.Authenticate(ctx, h.provider, *h.authOpts)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"could not authenticate to OpenStack: %w\", err)\n\t}\n\n\tclient, err := openstack.NewComputeV2(h.provider, gophercloud.EndpointOpts{\n\t\tRegion: h.region, Availability: h.availability,\n\t})\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"could not create OpenStack compute session: %w\", err)\n\t}\n\n\ttg := &targetgroup.Group{\n\t\tSource: \"OS_\" + h.region,\n\t}\n\t// OpenStack API reference\n\t// https://developer.openstack.org/api-ref/compute/#list-hypervisors-details\n\tpagerHypervisors := hypervisors.List(client, nil)\n\terr = pagerHypervisors.EachPage(ctx, func(ctx context.Context, page pagination.Page) (bool, error) {\n\t\thypervisorList, err := hypervisors.ExtractHypervisors(page)\n\t\tif err != nil {\n\t\t\treturn false, fmt.Errorf(\"could not extract hypervisors: %w\", err)\n\t\t}\n\t\tfor _, hypervisor := range hypervisorList {\n\t\t\tlabels := model.LabelSet{}\n\t\t\taddr := net.JoinHostPort(hypervisor.HostIP, strconv.Itoa(h.port))\n\t\t\tlabels[model.AddressLabel] = model.LabelValue(addr)\n\t\t\tlabels[openstackLabelHypervisorID] = model.LabelValue(hypervisor.ID)\n\t\t\tlabels[openstackLabelHypervisorHostName] = model.LabelValue(hypervisor.HypervisorHostname)\n\t\t\tlabels[openstackLabelHypervisorHostIP] = model.LabelValue(hypervisor.HostIP)\n\t\t\tlabels[openstackLabelHypervisorStatus] = model.LabelValue(hypervisor.Status)\n\t\t\tlabels[openstackLabelHypervisorState] = model.LabelValue(hypervisor.State)\n\t\t\tlabels[openstackLabelHypervisorType] = model.LabelValue(hypervisor.HypervisorType)\n\t\t\ttg.Targets = append(tg.Targets, labels)\n\t\t}\n\t\treturn true, nil\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn []*targetgroup.Group{tg}, nil\n}\n\n### Source File Dependency Files Content\n### Dependency File: prometheus\\hypervisor_test.go\\dependent_files\\targetgroup.go\n// Copyright 2013 The Prometheus Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage targetgroup\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\n\t\"github.com/prometheus/common/model\"\n)\n\n// Group is a set of targets with a common label set(production , test, staging etc.).\ntype Group struct {\n\t// Targets is a list of targets identified by a label set. Each target is\n\t// uniquely identifiable in the group by its address label.\n\tTargets []model.LabelSet\n\t// Labels is a set of labels that is common across all targets in the group.\n\tLabels model.LabelSet\n\n\t// Source is an identifier that describes a group of targets.\n\tSource string\n}\n\nfunc (tg Group) String() string {\n\treturn tg.Source\n}\n\n// UnmarshalYAML implements the yaml.Unmarshaler interface.\nfunc (tg *Group) UnmarshalYAML(unmarshal func(interface{}) error) error {\n\tg := struct {\n\t\tTargets []string       `yaml:\"targets\"`\n\t\tLabels  model.LabelSet `yaml:\"labels\"`\n\t}{}\n\tif err := unmarshal(&g); err != nil {\n\t\treturn err\n\t}\n\ttg.Targets = make([]model.LabelSet, 0, len(g.Targets))\n\tfor _, t := range g.Targets {\n\t\ttg.Targets = append(tg.Targets, model.LabelSet{\n\t\t\tmodel.AddressLabel: model.LabelValue(t),\n\t\t})\n\t}\n\ttg.Labels = g.Labels\n\treturn nil\n}\n\n// MarshalYAML implements the yaml.Marshaler interface.\nfunc (tg Group) MarshalYAML() (interface{}, error) {\n\tg := &struct {\n\t\tTargets []string       `yaml:\"targets\"`\n\t\tLabels  model.LabelSet `yaml:\"labels,omitempty\"`\n\t}{\n\t\tTargets: make([]string, 0, len(tg.Targets)),\n\t\tLabels:  tg.Labels,\n\t}\n\tfor _, t := range tg.Targets {\n\t\tg.Targets = append(g.Targets, string(t[model.AddressLabel]))\n\t}\n\treturn g, nil\n}\n\n// UnmarshalJSON implements the json.Unmarshaler interface.\nfunc (tg *Group) UnmarshalJSON(b []byte) error {\n\tg := struct {\n\t\tTargets []string       `json:\"targets\"`\n\t\tLabels  model.LabelSet `json:\"labels\"`\n\t}{}\n\n\tdec := json.NewDecoder(bytes.NewReader(b))\n\tdec.DisallowUnknownFields()\n\tif err := dec.Decode(&g); err != nil {\n\t\treturn err\n\t}\n\ttg.Targets = make([]model.LabelSet, 0, len(g.Targets))\n\tfor _, t := range g.Targets {\n\t\ttg.Targets = append(tg.Targets, model.LabelSet{\n\t\t\tmodel.AddressLabel: model.LabelValue(t),\n\t\t})\n\t}\n\ttg.Labels = g.Labels\n\treturn nil\n}\n\n// MarshalJSON implements the json.Marshaler interface.\nfunc (tg Group) MarshalJSON() ([]byte, error) {\n\tg := &struct {\n\t\tTargets []string       `json:\"targets\"`\n\t\tLabels  model.LabelSet `json:\"labels,omitempty\"`\n\t}{\n\t\tTargets: make([]string, 0, len(tg.Targets)),\n\t\tLabels:  tg.Labels,\n\t}\n\tfor _, t := range tg.Targets {\n\t\tg.Targets = append(g.Targets, string(t[model.AddressLabel]))\n\t}\n\treturn json.Marshal(g)\n}\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:47:45"}, {"role": "assistant", "content": "```go\n// Copyright 2017 The Prometheus Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage openstack\n\nimport (\n\t\"context\"\n\t\"testing\"\n\n\t\"github.com/prometheus/common/model\"\n\t\"github.com/stretchr/testify/require\"\n)\n\ntype OpenstackSDHypervisorTestSuite struct {\n\tMock *SDMock\n}\n\nfunc (s *OpenstackSDHypervisorTestSuite) SetupTest(t *testing.T) {\n\ts.Mock = NewSDMock(t)\n\ts.Mock.Setup()\n\n\ts.Mock.HandleHypervisorListSuccessfully()\n\ts.Mock.HandleVersionsSuccessfully()\n\ts.Mock.HandleAuthSuccessfully()\n}\n\nfunc (s *OpenstackSDHypervisorTestSuite) openstackAuthSuccess() (refresher, error) {\n\tconf := SDConfig{\n\t\tIdentityEndpoint: s.Mock.Endpoint(),\n\t\tPassword:         \"test\",\n\t\tUsername:         \"test\",\n\t\tDomainName:       \"12345\",\n\t\tRegion:           \"RegionOne\",\n\t\tRole:             \"hypervisor\",\n\t}\n\treturn newRefresher(&conf, nil)\n}\n\nfunc TestOpenstackSDHypervisorRefresh(t *testing.T) {\n\tmock := &OpenstackSDHypervisorTestSuite{}\n\tmock.SetupTest(t)\n\n\thypervisor, _ := mock.openstackAuthSuccess()\n\tctx := context.Background()\n\ttgs, err := hypervisor.refresh(ctx)\n\trequire.Len(t, tgs, 1)\n\ttg := tgs[0]\n\trequire.NoError(t, err)\n\trequire.NotNil(t, tg)\n\trequire.NotNil(t, tg.Targets)\n\trequire.Len(t, tg.Targets, 2)\n\n\tfor l, v := range map[string]string{\n\t\t\"__address__\":                          \"172.16.70.14:0\",\n\t\t\"__meta_openstack_hypervisor_hostname\": \"nc14.cloud.com\",\n\t\t\"__meta_openstack_hypervisor_type\":     \"QEMU\",\n\t\t\"__meta_openstack_hypervisor_host_ip\":  \"172.16.70.14\",\n\t\t\"__meta_openstack_hypervisor_state\":    \"up\",\n\t\t\"__meta_openstack_hypervisor_status\":   \"enabled\",\n\t\t\"__meta_openstack_hypervisor_id\":       \"1\",\n\t} {\n\t\trequire.Equal(t, model.LabelValue(v), tg.Targets[0][model.LabelName(l)])\n\t}\n\n\tfor l, v := range map[string]string{\n\t\t\"__address__\":                          \"172.16.70.13:0\",\n\t\t\"__meta_openstack_hypervisor_hostname\": \"cc13.cloud.com\",\n\t\t\"__meta_openstack_hypervisor_type\":     \"QEMU\",\n\t\t\"__meta_openstack_hypervisor_host_ip\":  \"172.16.70.13\",\n\t\t\"__meta_openstack_hypervisor_state\":    \"up\",\n\t\t\"__meta_openstack_hypervisor_status\":   \"enabled\",\n\t\t\"__meta_openstack_hypervisor_id\":       \"721\",\n\t} {\n\t\trequire.Equal(t, model.LabelValue(v), tg.Targets[1][model.LabelName(l)])\n\t}\n}\n\nfunc TestOpenstackSDHypervisorRefreshWithDoneContext(t *testing.T) {\n\tmock := &OpenstackSDHypervisorTestSuite{}\n\tmock.SetupTest(t)\n\n\thypervisor, _ := mock.openstackAuthSuccess()\n\tctx, cancel := context.WithCancel(context.Background())\n\tcancel()\n\t_, err := hypervisor.refresh(ctx)\n\trequire.ErrorContains(t, err, context.Canceled.Error(), \"%q doesn't contain %q\", err, context.Canceled)\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: prometheus\nSource File Path(s): ['prometheus\\\\loadbalancer_test.go\\\\source_files\\\\loadbalancer.go']\nTest File Path: prometheus\\loadbalancer_test.go\\loadbalancer_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: prometheus\\loadbalancer_test.go\\source_files\\loadbalancer.go\n// Copyright 2017 The Prometheus Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage openstack\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"net\"\n\t\"strconv\"\n\t\"strings\"\n\n\t\"github.com/gophercloud/gophercloud/v2\"\n\t\"github.com/gophercloud/gophercloud/v2/openstack\"\n\t\"github.com/gophercloud/gophercloud/v2/openstack/loadbalancer/v2/listeners\"\n\t\"github.com/gophercloud/gophercloud/v2/openstack/loadbalancer/v2/loadbalancers\"\n\t\"github.com/gophercloud/gophercloud/v2/openstack/networking/v2/extensions/layer3/floatingips\"\n\t\"github.com/prometheus/common/model\"\n\t\"github.com/prometheus/common/promslog\"\n\n\t\"github.com/prometheus/prometheus/discovery/targetgroup\"\n)\n\nconst (\n\topenstackLabelLoadBalancerID                 = openstackLabelPrefix + \"loadbalancer_id\"\n\topenstackLabelLoadBalancerName               = openstackLabelPrefix + \"loadbalancer_name\"\n\topenstackLabelLoadBalancerOperatingStatus    = openstackLabelPrefix + \"loadbalancer_operating_status\"\n\topenstackLabelLoadBalancerProvisioningStatus = openstackLabelPrefix + \"loadbalancer_provisioning_status\"\n\topenstackLabelLoadBalancerAvailabilityZone   = openstackLabelPrefix + \"loadbalancer_availability_zone\"\n\topenstackLabelLoadBalancerFloatingIP         = openstackLabelPrefix + \"loadbalancer_floating_ip\"\n\topenstackLabelLoadBalancerVIP                = openstackLabelPrefix + \"loadbalancer_vip\"\n\topenstackLabelLoadBalancerProvider           = openstackLabelPrefix + \"loadbalancer_provider\"\n\topenstackLabelLoadBalancerTags               = openstackLabelPrefix + \"loadbalancer_tags\"\n)\n\n// LoadBalancerDiscovery discovers OpenStack load balancers.\ntype LoadBalancerDiscovery struct {\n\tprovider     *gophercloud.ProviderClient\n\tauthOpts     *gophercloud.AuthOptions\n\tregion       string\n\tlogger       *slog.Logger\n\tavailability gophercloud.Availability\n}\n\n// NewLoadBalancerDiscovery returns a new loadbalancer discovery.\nfunc newLoadBalancerDiscovery(provider *gophercloud.ProviderClient, opts *gophercloud.AuthOptions,\n\tregion string, availability gophercloud.Availability, l *slog.Logger,\n) *LoadBalancerDiscovery {\n\tif l == nil {\n\t\tl = promslog.NewNopLogger()\n\t}\n\treturn &LoadBalancerDiscovery{\n\t\tprovider: provider, authOpts: opts,\n\t\tregion: region, availability: availability, logger: l,\n\t}\n}\n\nfunc (i *LoadBalancerDiscovery) refresh(ctx context.Context) ([]*targetgroup.Group, error) {\n\terr := openstack.Authenticate(ctx, i.provider, *i.authOpts)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"could not authenticate to OpenStack: %w\", err)\n\t}\n\n\tclient, err := openstack.NewLoadBalancerV2(i.provider, gophercloud.EndpointOpts{\n\t\tRegion: i.region, Availability: i.availability,\n\t})\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"could not create OpenStack load balancer session: %w\", err)\n\t}\n\n\tnetworkClient, err := openstack.NewNetworkV2(i.provider, gophercloud.EndpointOpts{\n\t\tRegion: i.region, Availability: i.availability,\n\t})\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"could not create OpenStack network session: %w\", err)\n\t}\n\n\tallPages, err := loadbalancers.List(client, loadbalancers.ListOpts{}).AllPages(ctx)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to list load balancers: %w\", err)\n\t}\n\n\tallLBs, err := loadbalancers.ExtractLoadBalancers(allPages)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to extract load balancers: %w\", err)\n\t}\n\n\t// Fetch all listeners in one API call\n\tlistenerPages, err := listeners.List(client, listeners.ListOpts{}).AllPages(ctx)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to list all listeners: %w\", err)\n\t}\n\n\tallListeners, err := listeners.ExtractListeners(listenerPages)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to extract all listeners: %w\", err)\n\t}\n\n\t// Create a map to group listeners by Load Balancer ID\n\tlistenerMap := make(map[string][]listeners.Listener)\n\tfor _, listener := range allListeners {\n\t\t// Iterate through each associated Load Balancer ID in the Loadbalancers array\n\t\tfor _, lb := range listener.Loadbalancers {\n\t\t\tlistenerMap[lb.ID] = append(listenerMap[lb.ID], listener)\n\t\t}\n\t}\n\n\t// Fetch all floating IPs\n\tfipPages, err := floatingips.List(networkClient, floatingips.ListOpts{}).AllPages(ctx)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to list all fips: %w\", err)\n\t}\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to list floating IPs: %w\", err)\n\t}\n\n\tallFIPs, err := floatingips.ExtractFloatingIPs(fipPages)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to extract floating IPs: %w\", err)\n\t}\n\n\t// Create a map to associate floating IPs with their resource IDs\n\tfipMap := make(map[string]string) // Key: LoadBalancerID/PortID, Value: Floating IP\n\tfor _, fip := range allFIPs {\n\t\tif fip.PortID != \"\" {\n\t\t\tfipMap[fip.PortID] = fip.FloatingIP\n\t\t}\n\t}\n\n\ttg := &targetgroup.Group{\n\t\tSource: \"OS_\" + i.region,\n\t}\n\n\tfor _, lb := range allLBs {\n\t\t// Retrieve listeners for this load balancer from the map\n\t\tlbListeners, exists := listenerMap[lb.ID]\n\t\tif !exists || len(lbListeners) == 0 {\n\t\t\ti.logger.Debug(\"Got no listener\", \"loadbalancer\", lb.ID)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Variable to store the port of the first PROMETHEUS listener\n\t\tvar listenerPort int\n\t\thasPrometheusListener := false\n\n\t\t// Check if any listener has the PROMETHEUS protocol\n\t\tfor _, listener := range lbListeners {\n\t\t\tif listener.Protocol == \"PROMETHEUS\" {\n\t\t\t\thasPrometheusListener = true\n\t\t\t\tlistenerPort = listener.ProtocolPort\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\n\t\t// Skip LBs without PROMETHEUS listener protocol\n\t\tif !hasPrometheusListener {\n\t\t\ti.logger.Debug(\"Got no PROMETHEUS listener\", \"loadbalancer\", lb.ID)\n\t\t\tcontinue\n\t\t}\n\n\t\tlabels := model.LabelSet{}\n\t\taddr := net.JoinHostPort(lb.VipAddress, strconv.Itoa(listenerPort))\n\t\tlabels[model.AddressLabel] = model.LabelValue(addr)\n\t\tlabels[openstackLabelLoadBalancerID] = model.LabelValue(lb.ID)\n\t\tlabels[openstackLabelLoadBalancerName] = model.LabelValue(lb.Name)\n\t\tlabels[openstackLabelLoadBalancerOperatingStatus] = model.LabelValue(lb.OperatingStatus)\n\t\tlabels[openstackLabelLoadBalancerProvisioningStatus] = model.LabelValue(lb.ProvisioningStatus)\n\t\tlabels[openstackLabelLoadBalancerAvailabilityZone] = model.LabelValue(lb.AvailabilityZone)\n\t\tlabels[openstackLabelLoadBalancerVIP] = model.LabelValue(lb.VipAddress)\n\t\tlabels[openstackLabelLoadBalancerProvider] = model.LabelValue(lb.Provider)\n\t\tlabels[openstackLabelProjectID] = model.LabelValue(lb.ProjectID)\n\n\t\tif len(lb.Tags) > 0 {\n\t\t\tlabels[openstackLabelLoadBalancerTags] = model.LabelValue(strings.Join(lb.Tags, \",\"))\n\t\t}\n\n\t\tif floatingIP, exists := fipMap[lb.VipPortID]; exists {\n\t\t\tlabels[openstackLabelLoadBalancerFloatingIP] = model.LabelValue(floatingIP)\n\t\t}\n\n\t\ttg.Targets = append(tg.Targets, labels)\n\t}\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn []*targetgroup.Group{tg}, nil\n}\n\n### Source File Dependency Files Content\n### Dependency File: prometheus\\loadbalancer_test.go\\dependent_files\\targetgroup.go\n// Copyright 2013 The Prometheus Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage targetgroup\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\n\t\"github.com/prometheus/common/model\"\n)\n\n// Group is a set of targets with a common label set(production , test, staging etc.).\ntype Group struct {\n\t// Targets is a list of targets identified by a label set. Each target is\n\t// uniquely identifiable in the group by its address label.\n\tTargets []model.LabelSet\n\t// Labels is a set of labels that is common across all targets in the group.\n\tLabels model.LabelSet\n\n\t// Source is an identifier that describes a group of targets.\n\tSource string\n}\n\nfunc (tg Group) String() string {\n\treturn tg.Source\n}\n\n// UnmarshalYAML implements the yaml.Unmarshaler interface.\nfunc (tg *Group) UnmarshalYAML(unmarshal func(interface{}) error) error {\n\tg := struct {\n\t\tTargets []string       `yaml:\"targets\"`\n\t\tLabels  model.LabelSet `yaml:\"labels\"`\n\t}{}\n\tif err := unmarshal(&g); err != nil {\n\t\treturn err\n\t}\n\ttg.Targets = make([]model.LabelSet, 0, len(g.Targets))\n\tfor _, t := range g.Targets {\n\t\ttg.Targets = append(tg.Targets, model.LabelSet{\n\t\t\tmodel.AddressLabel: model.LabelValue(t),\n\t\t})\n\t}\n\ttg.Labels = g.Labels\n\treturn nil\n}\n\n// MarshalYAML implements the yaml.Marshaler interface.\nfunc (tg Group) MarshalYAML() (interface{}, error) {\n\tg := &struct {\n\t\tTargets []string       `yaml:\"targets\"`\n\t\tLabels  model.LabelSet `yaml:\"labels,omitempty\"`\n\t}{\n\t\tTargets: make([]string, 0, len(tg.Targets)),\n\t\tLabels:  tg.Labels,\n\t}\n\tfor _, t := range tg.Targets {\n\t\tg.Targets = append(g.Targets, string(t[model.AddressLabel]))\n\t}\n\treturn g, nil\n}\n\n// UnmarshalJSON implements the json.Unmarshaler interface.\nfunc (tg *Group) UnmarshalJSON(b []byte) error {\n\tg := struct {\n\t\tTargets []string       `json:\"targets\"`\n\t\tLabels  model.LabelSet `json:\"labels\"`\n\t}{}\n\n\tdec := json.NewDecoder(bytes.NewReader(b))\n\tdec.DisallowUnknownFields()\n\tif err := dec.Decode(&g); err != nil {\n\t\treturn err\n\t}\n\ttg.Targets = make([]model.LabelSet, 0, len(g.Targets))\n\tfor _, t := range g.Targets {\n\t\ttg.Targets = append(tg.Targets, model.LabelSet{\n\t\t\tmodel.AddressLabel: model.LabelValue(t),\n\t\t})\n\t}\n\ttg.Labels = g.Labels\n\treturn nil\n}\n\n// MarshalJSON implements the json.Marshaler interface.\nfunc (tg Group) MarshalJSON() ([]byte, error) {\n\tg := &struct {\n\t\tTargets []string       `json:\"targets\"`\n\t\tLabels  model.LabelSet `json:\"labels,omitempty\"`\n\t}{\n\t\tTargets: make([]string, 0, len(tg.Targets)),\n\t\tLabels:  tg.Labels,\n\t}\n\tfor _, t := range tg.Targets {\n\t\tg.Targets = append(g.Targets, string(t[model.AddressLabel]))\n\t}\n\treturn json.Marshal(g)\n}\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:47:45"}, {"role": "assistant", "content": "```go\n// Copyright 2017 The Prometheus Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage openstack\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"testing\"\n\n\t\"github.com/prometheus/common/model\"\n\t\"github.com/stretchr/testify/require\"\n)\n\ntype OpenstackSDLoadBalancerTestSuite struct {\n\tMock *SDMock\n}\n\nfunc (s *OpenstackSDLoadBalancerTestSuite) SetupTest(t *testing.T) {\n\ts.Mock = NewSDMock(t)\n\ts.Mock.Setup()\n\n\ts.Mock.HandleLoadBalancerListSuccessfully()\n\ts.Mock.HandleListenersListSuccessfully()\n\ts.Mock.HandleFloatingIPListSuccessfully()\n\n\ts.Mock.HandleVersionsSuccessfully()\n\ts.Mock.HandleAuthSuccessfully()\n}\n\nfunc (s *OpenstackSDLoadBalancerTestSuite) openstackAuthSuccess() (refresher, error) {\n\tconf := SDConfig{\n\t\tIdentityEndpoint: s.Mock.Endpoint(),\n\t\tPassword:         \"test\",\n\t\tUsername:         \"test\",\n\t\tDomainName:       \"12345\",\n\t\tRegion:           \"RegionOne\",\n\t\tRole:             \"loadbalancer\",\n\t}\n\treturn newRefresher(&conf, nil)\n}\n\nfunc TestOpenstackSDLoadBalancerRefresh(t *testing.T) {\n\tmock := &OpenstackSDLoadBalancerTestSuite{}\n\tmock.SetupTest(t)\n\n\tinstance, err := mock.openstackAuthSuccess()\n\trequire.NoError(t, err)\n\n\tctx := context.Background()\n\ttgs, err := instance.refresh(ctx)\n\n\trequire.NoError(t, err)\n\trequire.Len(t, tgs, 1)\n\n\ttg := tgs[0]\n\trequire.NotNil(t, tg)\n\trequire.NotNil(t, tg.Targets)\n\trequire.Len(t, tg.Targets, 4)\n\n\tfor i, lbls := range []model.LabelSet{\n\t\t{\n\t\t\t\"__address__\":                                       model.LabelValue(\"10.0.0.32:9273\"),\n\t\t\t\"__meta_openstack_loadbalancer_id\":                  model.LabelValue(\"ef079b0c-e610-4dfb-b1aa-b49f07ac48e5\"),\n\t\t\t\"__meta_openstack_loadbalancer_name\":                model.LabelValue(\"lb1\"),\n\t\t\t\"__meta_openstack_loadbalancer_operating_status\":    model.LabelValue(\"ONLINE\"),\n\t\t\t\"__meta_openstack_loadbalancer_provisioning_status\": model.LabelValue(\"ACTIVE\"),\n\t\t\t\"__meta_openstack_loadbalancer_availability_zone\":   model.LabelValue(\"az1\"),\n\t\t\t\"__meta_openstack_loadbalancer_floating_ip\":         model.LabelValue(\"192.168.1.2\"),\n\t\t\t\"__meta_openstack_loadbalancer_vip\":                 model.LabelValue(\"10.0.0.32\"),\n\t\t\t\"__meta_openstack_loadbalancer_provider\":            model.LabelValue(\"amphora\"),\n\t\t\t\"__meta_openstack_loadbalancer_tags\":                model.LabelValue(\"tag1,tag2\"),\n\t\t\t\"__meta_openstack_project_id\":                       model.LabelValue(\"fcad67a6189847c4aecfa3c81a05783b\"),\n\t\t},\n\t\t{\n\t\t\t\"__address__\":                                       model.LabelValue(\"10.0.2.78:8080\"),\n\t\t\t\"__meta_openstack_loadbalancer_id\":                  model.LabelValue(\"d92c471e-8d3e-4b9f-b2b5-9c72a9e3ef54\"),\n\t\t\t\"__meta_openstack_loadbalancer_name\":                model.LabelValue(\"lb3\"),\n\t\t\t\"__meta_openstack_loadbalancer_operating_status\":    model.LabelValue(\"ONLINE\"),\n\t\t\t\"__meta_openstack_loadbalancer_provisioning_status\": model.LabelValue(\"ACTIVE\"),\n\t\t\t\"__meta_openstack_loadbalancer_availability_zone\":   model.LabelValue(\"az3\"),\n\t\t\t\"__meta_openstack_loadbalancer_floating_ip\":         model.LabelValue(\"192.168.3.4\"),\n\t\t\t\"__meta_openstack_loadbalancer_vip\":                 model.LabelValue(\"10.0.2.78\"),\n\t\t\t\"__meta_openstack_loadbalancer_provider\":            model.LabelValue(\"amphora\"),\n\t\t\t\"__meta_openstack_loadbalancer_tags\":                model.LabelValue(\"tag5,tag6\"),\n\t\t\t\"__meta_openstack_project_id\":                       model.LabelValue(\"ac57f03dba1a4fdebff3e67201bc7a85\"),\n\t\t},\n\t\t{\n\t\t\t\"__address__\":                                       model.LabelValue(\"10.0.3.99:9090\"),\n\t\t\t\"__meta_openstack_loadbalancer_id\":                  model.LabelValue(\"f5c7e918-df38-4a5a-a7d4-d9c27ab2cf67\"),\n\t\t\t\"__meta_openstack_loadbalancer_name\":                model.LabelValue(\"lb4\"),\n\t\t\t\"__meta_openstack_loadbalancer_operating_status\":    model.LabelValue(\"ONLINE\"),\n\t\t\t\"__meta_openstack_loadbalancer_provisioning_status\": model.LabelValue(\"ACTIVE\"),\n\t\t\t\"__meta_openstack_loadbalancer_availability_zone\":   model.LabelValue(\"az1\"),\n\t\t\t\"__meta_openstack_loadbalancer_floating_ip\":         model.LabelValue(\"192.168.4.5\"),\n\t\t\t\"__meta_openstack_loadbalancer_vip\":                 model.LabelValue(\"10.0.3.99\"),\n\t\t\t\"__meta_openstack_loadbalancer_provider\":            model.LabelValue(\"amphora\"),\n\t\t\t\"__meta_openstack_project_id\":                       model.LabelValue(\"fa8c372dfe4d4c92b0c4e3a2d9b3c9fa\"),\n\t\t},\n\t\t{\n\t\t\t\"__address__\":                                       model.LabelValue(\"10.0.4.88:9876\"),\n\t\t\t\"__meta_openstack_loadbalancer_id\":                  model.LabelValue(\"e83a6d92-7a3e-4567-94b3-20c83b32a75e\"),\n\t\t\t\"__meta_openstack_loadbalancer_name\":                model.LabelValue(\"lb5\"),\n\t\t\t\"__meta_openstack_loadbalancer_operating_status\":    model.LabelValue(\"ONLINE\"),\n\t\t\t\"__meta_openstack_loadbalancer_provisioning_status\": model.LabelValue(\"ACTIVE\"),\n\t\t\t\"__meta_openstack_loadbalancer_availability_zone\":   model.LabelValue(\"az4\"),\n\t\t\t\"__meta_openstack_loadbalancer_vip\":                 model.LabelValue(\"10.0.4.88\"),\n\t\t\t\"__meta_openstack_loadbalancer_provider\":            model.LabelValue(\"amphora\"),\n\t\t\t\"__meta_openstack_project_id\":                       model.LabelValue(\"a5d3b2e1e6f34cd9a5f7c2f01a6b8e29\"),\n\t\t},\n\t} {\n\t\tt.Run(fmt.Sprintf(\"item %d\", i), func(t *testing.T) {\n\t\t\trequire.Equal(t, lbls, tg.Targets[i])\n\t\t})\n\t}\n}\n\nfunc TestOpenstackSDLoadBalancerRefreshWithDoneContext(t *testing.T) {\n\tmock := &OpenstackSDLoadBalancerTestSuite{}\n\tmock.SetupTest(t)\n\n\tloadbalancer, _ := mock.openstackAuthSuccess()\n\tctx, cancel := context.WithCancel(context.Background())\n\tcancel()\n\t_, err := loadbalancer.refresh(ctx)\n\trequire.ErrorContains(t, err, context.Canceled.Error(), \"%q doesn't contain %q\", err, context.Canceled)\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: prometheus\nSource File Path(s): ['prometheus\\\\matcher_test.go\\\\source_files\\\\matcher.go']\nTest File Path: prometheus\\matcher_test.go\\matcher_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: prometheus\\matcher_test.go\\source_files\\matcher.go\n// Copyright 2017 The Prometheus Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage labels\n\nimport (\n\t\"bytes\"\n\t\"strconv\"\n)\n\n// MatchType is an enum for label matching types.\ntype MatchType int\n\n// Possible MatchTypes.\nconst (\n\tMatchEqual MatchType = iota\n\tMatchNotEqual\n\tMatchRegexp\n\tMatchNotRegexp\n)\n\nvar matchTypeToStr = [...]string{\n\tMatchEqual:     \"=\",\n\tMatchNotEqual:  \"!=\",\n\tMatchRegexp:    \"=~\",\n\tMatchNotRegexp: \"!~\",\n}\n\nfunc (m MatchType) String() string {\n\tif m < MatchEqual || m > MatchNotRegexp {\n\t\tpanic(\"unknown match type\")\n\t}\n\treturn matchTypeToStr[m]\n}\n\n// Matcher models the matching of a label.\ntype Matcher struct {\n\tType  MatchType\n\tName  string\n\tValue string\n\n\tre *FastRegexMatcher\n}\n\n// NewMatcher returns a matcher object.\nfunc NewMatcher(t MatchType, n, v string) (*Matcher, error) {\n\tm := &Matcher{\n\t\tType:  t,\n\t\tName:  n,\n\t\tValue: v,\n\t}\n\tif t == MatchRegexp || t == MatchNotRegexp {\n\t\tre, err := NewFastRegexMatcher(v)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tm.re = re\n\t}\n\treturn m, nil\n}\n\n// MustNewMatcher panics on error - only for use in tests!\nfunc MustNewMatcher(mt MatchType, name, val string) *Matcher {\n\tm, err := NewMatcher(mt, name, val)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn m\n}\n\nfunc (m *Matcher) String() string {\n\t// Start a buffer with a pre-allocated size on stack to cover most needs.\n\tvar bytea [1024]byte\n\tb := bytes.NewBuffer(bytea[:0])\n\n\tif m.shouldQuoteName() {\n\t\tb.Write(strconv.AppendQuote(b.AvailableBuffer(), m.Name))\n\t} else {\n\t\tb.WriteString(m.Name)\n\t}\n\tb.WriteString(m.Type.String())\n\tb.Write(strconv.AppendQuote(b.AvailableBuffer(), m.Value))\n\n\treturn b.String()\n}\n\nfunc (m *Matcher) shouldQuoteName() bool {\n\tfor i, c := range m.Name {\n\t\tif c == '_' || (c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z') || (i > 0 && c >= '0' && c <= '9') {\n\t\t\tcontinue\n\t\t}\n\t\treturn true\n\t}\n\treturn len(m.Name) == 0\n}\n\n// Matches returns whether the matcher matches the given string value.\nfunc (m *Matcher) Matches(s string) bool {\n\tswitch m.Type {\n\tcase MatchEqual:\n\t\treturn s == m.Value\n\tcase MatchNotEqual:\n\t\treturn s != m.Value\n\tcase MatchRegexp:\n\t\treturn m.re.MatchString(s)\n\tcase MatchNotRegexp:\n\t\treturn !m.re.MatchString(s)\n\t}\n\tpanic(\"labels.Matcher.Matches: invalid match type\")\n}\n\n// Inverse returns a matcher that matches the opposite.\nfunc (m *Matcher) Inverse() (*Matcher, error) {\n\tswitch m.Type {\n\tcase MatchEqual:\n\t\treturn NewMatcher(MatchNotEqual, m.Name, m.Value)\n\tcase MatchNotEqual:\n\t\treturn NewMatcher(MatchEqual, m.Name, m.Value)\n\tcase MatchRegexp:\n\t\treturn NewMatcher(MatchNotRegexp, m.Name, m.Value)\n\tcase MatchNotRegexp:\n\t\treturn NewMatcher(MatchRegexp, m.Name, m.Value)\n\t}\n\tpanic(\"labels.Matcher.Matches: invalid match type\")\n}\n\n// GetRegexString returns the regex string.\nfunc (m *Matcher) GetRegexString() string {\n\tif m.re == nil {\n\t\treturn \"\"\n\t}\n\treturn m.re.GetRegexString()\n}\n\n// SetMatches returns a set of equality matchers for the current regex matchers if possible.\n// For examples the regexp `a(b|f)` will returns \"ab\" and \"af\".\n// Returns nil if we can't replace the regexp by only equality matchers.\nfunc (m *Matcher) SetMatches() []string {\n\tif m.re == nil {\n\t\treturn nil\n\t}\n\treturn m.re.SetMatches()\n}\n\n// Prefix returns the required prefix of the value to match, if possible.\n// It will be empty if it's an equality matcher or if the prefix can't be determined.\nfunc (m *Matcher) Prefix() string {\n\tif m.re == nil {\n\t\treturn \"\"\n\t}\n\treturn m.re.prefix\n}\n\n// IsRegexOptimized returns whether regex is optimized.\nfunc (m *Matcher) IsRegexOptimized() bool {\n\tif m.re == nil {\n\t\treturn false\n\t}\n\treturn m.re.IsOptimized()\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:47:45"}, {"role": "assistant", "content": "```go\n// Copyright 2017 The Prometheus Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage labels\n\nimport (\n\t\"fmt\"\n\t\"math/rand\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc mustNewMatcher(t *testing.T, mType MatchType, value string) *Matcher {\n\tm, err := NewMatcher(mType, \"test_label_name\", value)\n\trequire.NoError(t, err)\n\treturn m\n}\n\nfunc TestMatcher(t *testing.T) {\n\ttests := []struct {\n\t\tmatcher *Matcher\n\t\tvalue   string\n\t\tmatch   bool\n\t}{\n\t\t{\n\t\t\tmatcher: mustNewMatcher(t, MatchEqual, \"bar\"),\n\t\t\tvalue:   \"bar\",\n\t\t\tmatch:   true,\n\t\t},\n\t\t{\n\t\t\tmatcher: mustNewMatcher(t, MatchEqual, \"bar\"),\n\t\t\tvalue:   \"foo-bar\",\n\t\t\tmatch:   false,\n\t\t},\n\t\t{\n\t\t\tmatcher: mustNewMatcher(t, MatchNotEqual, \"bar\"),\n\t\t\tvalue:   \"bar\",\n\t\t\tmatch:   false,\n\t\t},\n\t\t{\n\t\t\tmatcher: mustNewMatcher(t, MatchNotEqual, \"bar\"),\n\t\t\tvalue:   \"foo-bar\",\n\t\t\tmatch:   true,\n\t\t},\n\t\t{\n\t\t\tmatcher: mustNewMatcher(t, MatchRegexp, \"bar\"),\n\t\t\tvalue:   \"bar\",\n\t\t\tmatch:   true,\n\t\t},\n\t\t{\n\t\t\tmatcher: mustNewMatcher(t, MatchRegexp, \"bar\"),\n\t\t\tvalue:   \"foo-bar\",\n\t\t\tmatch:   false,\n\t\t},\n\t\t{\n\t\t\tmatcher: mustNewMatcher(t, MatchRegexp, \".*bar\"),\n\t\t\tvalue:   \"foo-bar\",\n\t\t\tmatch:   true,\n\t\t},\n\t\t{\n\t\t\tmatcher: mustNewMatcher(t, MatchNotRegexp, \"bar\"),\n\t\t\tvalue:   \"bar\",\n\t\t\tmatch:   false,\n\t\t},\n\t\t{\n\t\t\tmatcher: mustNewMatcher(t, MatchNotRegexp, \"bar\"),\n\t\t\tvalue:   \"foo-bar\",\n\t\t\tmatch:   true,\n\t\t},\n\t\t{\n\t\t\tmatcher: mustNewMatcher(t, MatchNotRegexp, \".*bar\"),\n\t\t\tvalue:   \"foo-bar\",\n\t\t\tmatch:   false,\n\t\t},\n\t\t{\n\t\t\tmatcher: mustNewMatcher(t, MatchRegexp, \"$*bar\"),\n\t\t\tvalue:   \"foo-bar\",\n\t\t\tmatch:   false,\n\t\t},\n\t\t{\n\t\t\tmatcher: mustNewMatcher(t, MatchRegexp, \"bar^+\"),\n\t\t\tvalue:   \"foo-bar\",\n\t\t\tmatch:   false,\n\t\t},\n\t\t{\n\t\t\tmatcher: mustNewMatcher(t, MatchRegexp, \"$+bar\"),\n\t\t\tvalue:   \"foo-bar\",\n\t\t\tmatch:   false,\n\t\t},\n\t}\n\n\tfor _, test := range tests {\n\t\trequire.Equal(t, test.matcher.Matches(test.value), test.match)\n\t}\n}\n\nfunc TestInverse(t *testing.T) {\n\ttests := []struct {\n\t\tmatcher  *Matcher\n\t\texpected *Matcher\n\t}{\n\t\t{\n\t\t\tmatcher:  &Matcher{Type: MatchEqual, Name: \"name1\", Value: \"value1\"},\n\t\t\texpected: &Matcher{Type: MatchNotEqual, Name: \"name1\", Value: \"value1\"},\n\t\t},\n\t\t{\n\t\t\tmatcher:  &Matcher{Type: MatchNotEqual, Name: \"name2\", Value: \"value2\"},\n\t\t\texpected: &Matcher{Type: MatchEqual, Name: \"name2\", Value: \"value2\"},\n\t\t},\n\t\t{\n\t\t\tmatcher:  &Matcher{Type: MatchRegexp, Name: \"name3\", Value: \"value3.*\"},\n\t\t\texpected: &Matcher{Type: MatchNotRegexp, Name: \"name3\", Value: \"value3.*\"},\n\t\t},\n\t\t{\n\t\t\tmatcher:  &Matcher{Type: MatchNotRegexp, Name: \"name4\", Value: \"value4.*\"},\n\t\t\texpected: &Matcher{Type: MatchRegexp, Name: \"name4\", Value: \"value4.*\"},\n\t\t},\n\t}\n\n\tfor _, test := range tests {\n\t\tresult, err := test.matcher.Inverse()\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, test.expected.Type, result.Type)\n\t}\n}\n\nfunc TestPrefix(t *testing.T) {\n\tfor i, tc := range []struct {\n\t\tmatcher *Matcher\n\t\tprefix  string\n\t}{\n\t\t{\n\t\t\tmatcher: mustNewMatcher(t, MatchEqual, \"abc\"),\n\t\t\tprefix:  \"\",\n\t\t},\n\t\t{\n\t\t\tmatcher: mustNewMatcher(t, MatchNotEqual, \"abc\"),\n\t\t\tprefix:  \"\",\n\t\t},\n\t\t{\n\t\t\tmatcher: mustNewMatcher(t, MatchRegexp, \"abc.+\"),\n\t\t\tprefix:  \"abc\",\n\t\t},\n\t\t{\n\t\t\tmatcher: mustNewMatcher(t, MatchRegexp, \"abcd|abc.+\"),\n\t\t\tprefix:  \"abc\",\n\t\t},\n\t\t{\n\t\t\tmatcher: mustNewMatcher(t, MatchNotRegexp, \"abcd|abc.+\"),\n\t\t\tprefix:  \"abc\",\n\t\t},\n\t\t{\n\t\t\tmatcher: mustNewMatcher(t, MatchRegexp, \"abc(def|ghj)|ab|a.\"),\n\t\t\tprefix:  \"a\",\n\t\t},\n\t\t{\n\t\t\tmatcher: mustNewMatcher(t, MatchRegexp, \"foo.+bar|foo.*baz\"),\n\t\t\tprefix:  \"foo\",\n\t\t},\n\t\t{\n\t\t\tmatcher: mustNewMatcher(t, MatchRegexp, \"abc|.*\"),\n\t\t\tprefix:  \"\",\n\t\t},\n\t\t{\n\t\t\tmatcher: mustNewMatcher(t, MatchRegexp, \"abc|def\"),\n\t\t\tprefix:  \"\",\n\t\t},\n\t\t{\n\t\t\tmatcher: mustNewMatcher(t, MatchRegexp, \".+def\"),\n\t\t\tprefix:  \"\",\n\t\t},\n\t} {\n\t\tt.Run(fmt.Sprintf(\"%d: %s\", i, tc.matcher), func(t *testing.T) {\n\t\t\trequire.Equal(t, tc.prefix, tc.matcher.Prefix())\n\t\t})\n\t}\n}\n\nfunc TestIsRegexOptimized(t *testing.T) {\n\tfor i, tc := range []struct {\n\t\tmatcher          *Matcher\n\t\tisRegexOptimized bool\n\t}{\n\t\t{\n\t\t\tmatcher:          mustNewMatcher(t, MatchEqual, \"abc\"),\n\t\t\tisRegexOptimized: false,\n\t\t},\n\t\t{\n\t\t\tmatcher:          mustNewMatcher(t, MatchRegexp, \".\"),\n\t\t\tisRegexOptimized: false,\n\t\t},\n\t\t{\n\t\t\tmatcher:          mustNewMatcher(t, MatchRegexp, \"abc.+\"),\n\t\t\tisRegexOptimized: true,\n\t\t},\n\t} {\n\t\tt.Run(fmt.Sprintf(\"%d: %s\", i, tc.matcher), func(t *testing.T) {\n\t\t\trequire.Equal(t, tc.isRegexOptimized, tc.matcher.IsRegexOptimized())\n\t\t})\n\t}\n}\n\nfunc BenchmarkMatchType_String(b *testing.B) {\n\tfor i := 0; i <= b.N; i++ {\n\t\t_ = MatchType(i % int(MatchNotRegexp+1)).String()\n\t}\n}\n\nfunc BenchmarkNewMatcher(b *testing.B) {\n\tb.Run(\"regex matcher with literal\", func(b *testing.B) {\n\t\tb.ReportAllocs()\n\t\tb.ResetTimer()\n\t\tfor i := 0; i <= b.N; i++ {\n\t\t\tNewMatcher(MatchRegexp, \"foo\", \"bar\")\n\t\t}\n\t})\n}\n\nfunc BenchmarkMatcher_String(b *testing.B) {\n\ttype benchCase struct {\n\t\tname     string\n\t\tmatchers []*Matcher\n\t}\n\tcases := []benchCase{\n\t\t{\n\t\t\tname: \"short name equal\",\n\t\t\tmatchers: []*Matcher{\n\t\t\t\tMustNewMatcher(MatchEqual, \"foo\", \"bar\"),\n\t\t\t\tMustNewMatcher(MatchEqual, \"bar\", \"baz\"),\n\t\t\t\tMustNewMatcher(MatchEqual, \"abc\", \"def\"),\n\t\t\t\tMustNewMatcher(MatchEqual, \"ghi\", \"klm\"),\n\t\t\t\tMustNewMatcher(MatchEqual, \"nop\", \"qrs\"),\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"short quoted name not equal\",\n\t\t\tmatchers: []*Matcher{\n\t\t\t\tMustNewMatcher(MatchEqual, \"f.o\", \"bar\"),\n\t\t\t\tMustNewMatcher(MatchEqual, \"b.r\", \"baz\"),\n\t\t\t\tMustNewMatcher(MatchEqual, \"a.c\", \"def\"),\n\t\t\t\tMustNewMatcher(MatchEqual, \"g.i\", \"klm\"),\n\t\t\t\tMustNewMatcher(MatchEqual, \"n.p\", \"qrs\"),\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"short quoted name with quotes not equal\",\n\t\t\tmatchers: []*Matcher{\n\t\t\t\tMustNewMatcher(MatchEqual, `\"foo\"`, \"bar\"),\n\t\t\t\tMustNewMatcher(MatchEqual, `\"foo\"`, \"baz\"),\n\t\t\t\tMustNewMatcher(MatchEqual, `\"foo\"`, \"def\"),\n\t\t\t\tMustNewMatcher(MatchEqual, `\"foo\"`, \"klm\"),\n\t\t\t\tMustNewMatcher(MatchEqual, `\"foo\"`, \"qrs\"),\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"short name value with quotes equal\",\n\t\t\tmatchers: []*Matcher{\n\t\t\t\tMustNewMatcher(MatchEqual, \"foo\", `\"bar\"`),\n\t\t\t\tMustNewMatcher(MatchEqual, \"bar\", `\"baz\"`),\n\t\t\t\tMustNewMatcher(MatchEqual, \"abc\", `\"def\"`),\n\t\t\t\tMustNewMatcher(MatchEqual, \"ghi\", `\"klm\"`),\n\t\t\t\tMustNewMatcher(MatchEqual, \"nop\", `\"qrs\"`),\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"short name and long value regexp\",\n\t\t\tmatchers: []*Matcher{\n\t\t\t\tMustNewMatcher(MatchRegexp, \"foo\", \"five_six_seven_eight_nine_ten_one_two_three_four\"),\n\t\t\t\tMustNewMatcher(MatchRegexp, \"bar\", \"one_two_three_four_five_six_seven_eight_nine_ten\"),\n\t\t\t\tMustNewMatcher(MatchRegexp, \"abc\", \"two_three_four_five_six_seven_eight_nine_ten_one\"),\n\t\t\t\tMustNewMatcher(MatchRegexp, \"ghi\", \"three_four_five_six_seven_eight_nine_ten_one_two\"),\n\t\t\t\tMustNewMatcher(MatchRegexp, \"nop\", \"four_five_six_seven_eight_nine_ten_one_two_three\"),\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"short name and long value with quotes equal\",\n\t\t\tmatchers: []*Matcher{\n\t\t\t\tMustNewMatcher(MatchEqual, \"foo\", `five_six_seven_eight_nine_ten_\"one\"_two_three_four`),\n\t\t\t\tMustNewMatcher(MatchEqual, \"bar\", `one_two_three_four_five_six_\"seven\"_eight_nine_ten`),\n\t\t\t\tMustNewMatcher(MatchEqual, \"abc\", `two_three_four_five_six_seven_\"eight\"_nine_ten_one`),\n\t\t\t\tMustNewMatcher(MatchEqual, \"ghi\", `three_four_five_six_seven_eight_\"nine\"_ten_one_two`),\n\t\t\t\tMustNewMatcher(MatchEqual, \"nop\", `four_five_six_seven_eight_nine_\"ten\"_one_two_three`),\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"long name regexp\",\n\t\t\tmatchers: []*Matcher{\n\t\t\t\tMustNewMatcher(MatchRegexp, \"one_two_three_four_five_six_seven_eight_nine_ten\", \"val\"),\n\t\t\t\tMustNewMatcher(MatchRegexp, \"two_three_four_five_six_seven_eight_nine_ten_one\", \"val\"),\n\t\t\t\tMustNewMatcher(MatchRegexp, \"three_four_five_six_seven_eight_nine_ten_one_two\", \"val\"),\n\t\t\t\tMustNewMatcher(MatchRegexp, \"four_five_six_seven_eight_nine_ten_one_two_three\", \"val\"),\n\t\t\t\tMustNewMatcher(MatchRegexp, \"five_six_seven_eight_nine_ten_one_two_three_four\", \"val\"),\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"long quoted name regexp\",\n\t\t\tmatchers: []*Matcher{\n\t\t\t\tMustNewMatcher(MatchRegexp, \"one.two.three.four.five.six.seven.eight.nine.ten\", \"val\"),\n\t\t\t\tMustNewMatcher(MatchRegexp, \"two.three.four.five.six.seven.eight.nine.ten.one\", \"val\"),\n\t\t\t\tMustNewMatcher(MatchRegexp, \"three.four.five.six.seven.eight.nine.ten.one.two\", \"val\"),\n\t\t\t\tMustNewMatcher(MatchRegexp, \"four.five.six.seven.eight.nine.ten.one.two.three\", \"val\"),\n\t\t\t\tMustNewMatcher(MatchRegexp, \"five.six.seven.eight.nine.ten.one.two.three.four\", \"val\"),\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"long name and long value regexp\",\n\t\t\tmatchers: []*Matcher{\n\t\t\t\tMustNewMatcher(MatchRegexp, \"one_two_three_four_five_six_seven_eight_nine_ten\", \"five_six_seven_eight_nine_ten_one_two_three_four\"),\n\t\t\t\tMustNewMatcher(MatchRegexp, \"two_three_four_five_six_seven_eight_nine_ten_one\", \"one_two_three_four_five_six_seven_eight_nine_ten\"),\n\t\t\t\tMustNewMatcher(MatchRegexp, \"three_four_five_six_seven_eight_nine_ten_one_two\", \"two_three_four_five_six_seven_eight_nine_ten_one\"),\n\t\t\t\tMustNewMatcher(MatchRegexp, \"four_five_six_seven_eight_nine_ten_one_two_three\", \"three_four_five_six_seven_eight_nine_ten_one_two\"),\n\t\t\t\tMustNewMatcher(MatchRegexp, \"five_six_seven_eight_nine_ten_one_two_three_four\", \"four_five_six_seven_eight_nine_ten_one_two_three\"),\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"long quoted name and long value regexp\",\n\t\t\tmatchers: []*Matcher{\n\t\t\t\tMustNewMatcher(MatchRegexp, \"one.two.three.four.five.six.seven.eight.nine.ten\", \"five.six.seven.eight.nine.ten.one.two.three.four\"),\n\t\t\t\tMustNewMatcher(MatchRegexp, \"two.three.four.five.six.seven.eight.nine.ten.one\", \"one.two.three.four.five.six.seven.eight.nine.ten\"),\n\t\t\t\tMustNewMatcher(MatchRegexp, \"three.four.five.six.seven.eight.nine.ten.one.two\", \"two.three.four.five.six.seven.eight.nine.ten.one\"),\n\t\t\t\tMustNewMatcher(MatchRegexp, \"four.five.six.seven.eight.nine.ten.one.two.three\", \"three.four.five.six.seven.eight.nine.ten.one.two\"),\n\t\t\t\tMustNewMatcher(MatchRegexp, \"five.six.seven.eight.nine.ten.one.two.three.four\", \"four.five.six.seven.eight.nine.ten.one.two.three\"),\n\t\t\t},\n\t\t},\n\t}\n\n\tvar mixed []*Matcher\n\tfor _, bc := range cases {\n\t\tmixed = append(mixed, bc.matchers...)\n\t}\n\trand.Shuffle(len(mixed), func(i, j int) { mixed[i], mixed[j] = mixed[j], mixed[i] })\n\tcases = append(cases, benchCase{name: \"mixed\", matchers: mixed})\n\n\tfor _, bc := range cases {\n\t\tb.Run(bc.name, func(b *testing.B) {\n\t\t\tfor i := 0; i <= b.N; i++ {\n\t\t\t\tm := bc.matchers[i%len(bc.matchers)]\n\t\t\t\t_ = m.String()\n\t\t\t}\n\t\t})\n\t}\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: prometheus\nSource File Path(s): ['prometheus\\\\sharding_test.go\\\\source_files\\\\sharding.go']\nTest File Path: prometheus\\sharding_test.go\\sharding_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: prometheus\\sharding_test.go\\source_files\\sharding.go\n// Copyright 2020 The Prometheus Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n//go:build !stringlabels && !dedupelabels\n\npackage labels\n\nimport (\n\t\"github.com/cespare/xxhash/v2\"\n)\n\n// StableHash is a labels hashing implementation which is guaranteed to not change over time.\n// This function should be used whenever labels hashing backward compatibility must be guaranteed.\nfunc StableHash(ls Labels) uint64 {\n\t// Use xxhash.Sum64(b) for fast path as it's faster.\n\tb := make([]byte, 0, 1024)\n\tfor i, v := range ls {\n\t\tif len(b)+len(v.Name)+len(v.Value)+2 >= cap(b) {\n\t\t\t// If labels entry is 1KB+ do not allocate whole entry.\n\t\t\th := xxhash.New()\n\t\t\t_, _ = h.Write(b)\n\t\t\tfor _, v := range ls[i:] {\n\t\t\t\t_, _ = h.WriteString(v.Name)\n\t\t\t\t_, _ = h.Write(seps)\n\t\t\t\t_, _ = h.WriteString(v.Value)\n\t\t\t\t_, _ = h.Write(seps)\n\t\t\t}\n\t\t\treturn h.Sum64()\n\t\t}\n\n\t\tb = append(b, v.Name...)\n\t\tb = append(b, sep)\n\t\tb = append(b, v.Value...)\n\t\tb = append(b, sep)\n\t}\n\treturn xxhash.Sum64(b)\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:47:45"}, {"role": "assistant", "content": "```go\n// Copyright 2020 The Prometheus Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage labels\n\nimport (\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/require\"\n)\n\n// TestStableHash tests that StableHash is stable.\n// The hashes this test asserts should not be changed.\nfunc TestStableHash(t *testing.T) {\n\tfor expectedHash, lbls := range map[uint64]Labels{\n\t\t0xef46db3751d8e999: EmptyLabels(),\n\t\t0x347c8ee7a9e29708: FromStrings(\"hello\", \"world\"),\n\t\t0xcbab40540f26097d: FromStrings(MetricName, \"metric\", \"label\", \"value\"),\n\t} {\n\t\trequire.Equal(t, expectedHash, StableHash(lbls))\n\t}\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: prometheus\nSource File Path(s): ['prometheus\\\\targetgroup_test.go\\\\source_files\\\\targetgroup.go']\nTest File Path: prometheus\\targetgroup_test.go\\targetgroup_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: prometheus\\targetgroup_test.go\\source_files\\targetgroup.go\n// Copyright 2013 The Prometheus Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage targetgroup\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\n\t\"github.com/prometheus/common/model\"\n)\n\n// Group is a set of targets with a common label set(production , test, staging etc.).\ntype Group struct {\n\t// Targets is a list of targets identified by a label set. Each target is\n\t// uniquely identifiable in the group by its address label.\n\tTargets []model.LabelSet\n\t// Labels is a set of labels that is common across all targets in the group.\n\tLabels model.LabelSet\n\n\t// Source is an identifier that describes a group of targets.\n\tSource string\n}\n\nfunc (tg Group) String() string {\n\treturn tg.Source\n}\n\n// UnmarshalYAML implements the yaml.Unmarshaler interface.\nfunc (tg *Group) UnmarshalYAML(unmarshal func(interface{}) error) error {\n\tg := struct {\n\t\tTargets []string       `yaml:\"targets\"`\n\t\tLabels  model.LabelSet `yaml:\"labels\"`\n\t}{}\n\tif err := unmarshal(&g); err != nil {\n\t\treturn err\n\t}\n\ttg.Targets = make([]model.LabelSet, 0, len(g.Targets))\n\tfor _, t := range g.Targets {\n\t\ttg.Targets = append(tg.Targets, model.LabelSet{\n\t\t\tmodel.AddressLabel: model.LabelValue(t),\n\t\t})\n\t}\n\ttg.Labels = g.Labels\n\treturn nil\n}\n\n// MarshalYAML implements the yaml.Marshaler interface.\nfunc (tg Group) MarshalYAML() (interface{}, error) {\n\tg := &struct {\n\t\tTargets []string       `yaml:\"targets\"`\n\t\tLabels  model.LabelSet `yaml:\"labels,omitempty\"`\n\t}{\n\t\tTargets: make([]string, 0, len(tg.Targets)),\n\t\tLabels:  tg.Labels,\n\t}\n\tfor _, t := range tg.Targets {\n\t\tg.Targets = append(g.Targets, string(t[model.AddressLabel]))\n\t}\n\treturn g, nil\n}\n\n// UnmarshalJSON implements the json.Unmarshaler interface.\nfunc (tg *Group) UnmarshalJSON(b []byte) error {\n\tg := struct {\n\t\tTargets []string       `json:\"targets\"`\n\t\tLabels  model.LabelSet `json:\"labels\"`\n\t}{}\n\n\tdec := json.NewDecoder(bytes.NewReader(b))\n\tdec.DisallowUnknownFields()\n\tif err := dec.Decode(&g); err != nil {\n\t\treturn err\n\t}\n\ttg.Targets = make([]model.LabelSet, 0, len(g.Targets))\n\tfor _, t := range g.Targets {\n\t\ttg.Targets = append(tg.Targets, model.LabelSet{\n\t\t\tmodel.AddressLabel: model.LabelValue(t),\n\t\t})\n\t}\n\ttg.Labels = g.Labels\n\treturn nil\n}\n\n// MarshalJSON implements the json.Marshaler interface.\nfunc (tg Group) MarshalJSON() ([]byte, error) {\n\tg := &struct {\n\t\tTargets []string       `json:\"targets\"`\n\t\tLabels  model.LabelSet `json:\"labels,omitempty\"`\n\t}{\n\t\tTargets: make([]string, 0, len(tg.Targets)),\n\t\tLabels:  tg.Labels,\n\t}\n\tfor _, t := range tg.Targets {\n\t\tg.Targets = append(g.Targets, string(t[model.AddressLabel]))\n\t}\n\treturn json.Marshal(g)\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:47:45"}, {"role": "assistant", "content": "```go\n// Copyright 2018 The Prometheus Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage targetgroup\n\nimport (\n\t\"errors\"\n\t\"testing\"\n\n\t\"github.com/prometheus/common/model\"\n\t\"github.com/stretchr/testify/require\"\n\t\"gopkg.in/yaml.v2\"\n)\n\nfunc TestTargetGroupStrictJSONUnmarshal(t *testing.T) {\n\ttests := []struct {\n\t\tjson          string\n\t\texpectedReply error\n\t\texpectedGroup Group\n\t}{\n\t\t{\n\t\t\tjson:          `{\"labels\": {},\"targets\": []}`,\n\t\t\texpectedReply: nil,\n\t\t\texpectedGroup: Group{Targets: []model.LabelSet{}, Labels: model.LabelSet{}},\n\t\t},\n\t\t{\n\t\t\tjson:          `{\"labels\": {\"my\":\"label\"},\"targets\": [\"localhost:9090\",\"localhost:9091\"]}`,\n\t\t\texpectedReply: nil,\n\t\t\texpectedGroup: Group{Targets: []model.LabelSet{\n\t\t\t\t{\"__address__\": \"localhost:9090\"},\n\t\t\t\t{\"__address__\": \"localhost:9091\"},\n\t\t\t}, Labels: model.LabelSet{\"my\": \"label\"}},\n\t\t},\n\t\t{\n\t\t\tjson:          `{\"label\": {},\"targets\": []}`,\n\t\t\texpectedReply: errors.New(\"json: unknown field \\\"label\\\"\"),\n\t\t},\n\t\t{\n\t\t\tjson:          `{\"labels\": {},\"target\": []}`,\n\t\t\texpectedReply: errors.New(\"json: unknown field \\\"target\\\"\"),\n\t\t},\n\t}\n\n\tfor _, test := range tests {\n\t\ttg := Group{}\n\t\tactual := tg.UnmarshalJSON([]byte(test.json))\n\t\trequire.Equal(t, test.expectedReply, actual)\n\t\trequire.Equal(t, test.expectedGroup, tg)\n\t}\n}\n\nfunc TestTargetGroupJSONMarshal(t *testing.T) {\n\ttests := []struct {\n\t\texpectedJSON string\n\t\texpectedErr  error\n\t\tgroup        Group\n\t}{\n\t\t{\n\t\t\t// labels should be omitted if empty.\n\t\t\tgroup:        Group{},\n\t\t\texpectedJSON: `{\"targets\": []}`,\n\t\t\texpectedErr:  nil,\n\t\t},\n\t\t{\n\t\t\t// targets only exposes addresses.\n\t\t\tgroup: Group{\n\t\t\t\tTargets: []model.LabelSet{\n\t\t\t\t\t{\"__address__\": \"localhost:9090\"},\n\t\t\t\t\t{\"__address__\": \"localhost:9091\"},\n\t\t\t\t},\n\t\t\t\tLabels: model.LabelSet{\"foo\": \"bar\", \"bar\": \"baz\"},\n\t\t\t},\n\t\t\texpectedJSON: `{\"targets\": [\"localhost:9090\", \"localhost:9091\"], \"labels\": {\"bar\": \"baz\", \"foo\": \"bar\"}}`,\n\t\t\texpectedErr:  nil,\n\t\t},\n\t}\n\n\tfor _, test := range tests {\n\t\tactual, err := test.group.MarshalJSON()\n\t\trequire.Equal(t, test.expectedErr, err)\n\t\trequire.JSONEq(t, test.expectedJSON, string(actual))\n\t}\n}\n\nfunc TestTargetGroupYamlMarshal(t *testing.T) {\n\tmarshal := func(g interface{}) []byte {\n\t\td, err := yaml.Marshal(g)\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\treturn d\n\t}\n\n\ttests := []struct {\n\t\texpectedYaml string\n\t\texpectedErr  error\n\t\tgroup        Group\n\t}{\n\t\t{\n\t\t\t// labels should be omitted if empty.\n\t\t\tgroup:        Group{},\n\t\t\texpectedYaml: \"targets: []\\n\",\n\t\t\texpectedErr:  nil,\n\t\t},\n\t\t{\n\t\t\t// targets only exposes addresses.\n\t\t\tgroup: Group{\n\t\t\t\tTargets: []model.LabelSet{\n\t\t\t\t\t{\"__address__\": \"localhost:9090\"},\n\t\t\t\t\t{\"__address__\": \"localhost:9091\"},\n\t\t\t\t},\n\t\t\t\tLabels: model.LabelSet{\"foo\": \"bar\", \"bar\": \"baz\"},\n\t\t\t},\n\t\t\texpectedYaml: \"targets:\\n- localhost:9090\\n- localhost:9091\\nlabels:\\n  bar: baz\\n  foo: bar\\n\",\n\t\t\texpectedErr:  nil,\n\t\t},\n\t}\n\n\tfor _, test := range tests {\n\t\tactual, err := test.group.MarshalYAML()\n\t\trequire.Equal(t, test.expectedErr, err)\n\t\trequire.Equal(t, test.expectedYaml, string(marshal(actual)))\n\t}\n}\n\nfunc TestTargetGroupYamlUnmarshal(t *testing.T) {\n\tunmarshal := func(d []byte) func(interface{}) error {\n\t\treturn func(o interface{}) error {\n\t\t\treturn yaml.Unmarshal(d, o)\n\t\t}\n\t}\n\ttests := []struct {\n\t\tyaml          string\n\t\texpectedGroup Group\n\t\texpectedReply error\n\t}{\n\t\t{\n\t\t\t// empty target group.\n\t\t\tyaml:          \"labels:\\ntargets:\\n\",\n\t\t\texpectedGroup: Group{Targets: []model.LabelSet{}},\n\t\t\texpectedReply: nil,\n\t\t},\n\t\t{\n\t\t\t// brackets syntax.\n\t\t\tyaml:          \"labels:\\n  my:  label\\ntargets:\\n  ['localhost:9090', 'localhost:9191']\",\n\t\t\texpectedReply: nil,\n\t\t\texpectedGroup: Group{Targets: []model.LabelSet{\n\t\t\t\t{\"__address__\": \"localhost:9090\"},\n\t\t\t\t{\"__address__\": \"localhost:9191\"},\n\t\t\t}, Labels: model.LabelSet{\"my\": \"label\"}},\n\t\t},\n\t\t{\n\t\t\t// incorrect syntax.\n\t\t\tyaml:          \"labels:\\ntargets:\\n  'localhost:9090'\",\n\t\t\texpectedReply: &yaml.TypeError{Errors: []string{\"line 3: cannot unmarshal !!str `localho...` into []string\"}},\n\t\t},\n\t}\n\n\tfor _, test := range tests {\n\t\ttg := Group{}\n\t\tactual := tg.UnmarshalYAML(unmarshal([]byte(test.yaml)))\n\t\trequire.Equal(t, test.expectedReply, actual)\n\t\trequire.Equal(t, test.expectedGroup, tg)\n\t}\n}\n\nfunc TestString(t *testing.T) {\n\t// String() should return only the source, regardless of other attributes.\n\tgroup1 := Group{\n\t\tTargets: []model.LabelSet{\n\t\t\t{\"__address__\": \"localhost:9090\"},\n\t\t\t{\"__address__\": \"localhost:9091\"},\n\t\t},\n\t\tSource: \"<source>\",\n\t\tLabels: model.LabelSet{\"foo\": \"bar\", \"bar\": \"baz\"},\n\t}\n\tgroup2 := Group{\n\t\tTargets: []model.LabelSet{},\n\t\tSource:  \"<source>\",\n\t\tLabels:  model.LabelSet{},\n\t}\n\trequire.Equal(t, \"<source>\", group1.String())\n\trequire.Equal(t, \"<source>\", group2.String())\n\trequire.Equal(t, group1.String(), group2.String())\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: prometheus\nSource File Path(s): ['prometheus\\\\varbit_test.go\\\\source_files\\\\varbit.go']\nTest File Path: prometheus\\varbit_test.go\\varbit_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: prometheus\\varbit_test.go\\source_files\\varbit.go\n// Copyright 2021 The Prometheus Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage chunkenc\n\nimport (\n\t\"fmt\"\n\t\"math/bits\"\n)\n\n// putVarbitInt writes an int64 using varbit encoding with a bit bucketing\n// optimized for the dod's observed in histogram buckets, plus a few additional\n// buckets for large numbers.\n//\n// For optimal space utilization, each branch didn't need to support any values\n// of any of the prior branches. So we could expand the range of each branch. Do\n// more with fewer bits. It would come at the price of more expensive encoding\n// and decoding (cutting out and later adding back that center-piece we\n// skip). With the distributions of values we see in practice, we would reduce\n// the size by around 1%. A more detailed study would be needed for precise\n// values, but it's appears quite certain that we would end up far below 10%,\n// which would maybe convince us to invest the increased coding/decoding cost.\nfunc putVarbitInt(b *bstream, val int64) {\n\tswitch {\n\tcase val == 0: // Precisely 0, needs 1 bit.\n\t\tb.writeBit(zero)\n\tcase bitRange(val, 3): // -3 <= val <= 4, needs 5 bits.\n\t\tb.writeBits(0b10, 2)\n\t\tb.writeBits(uint64(val), 3)\n\tcase bitRange(val, 6): // -31 <= val <= 32, 9 bits.\n\t\tb.writeBits(0b110, 3)\n\t\tb.writeBits(uint64(val), 6)\n\tcase bitRange(val, 9): // -255 <= val <= 256, 13 bits.\n\t\tb.writeBits(0b1110, 4)\n\t\tb.writeBits(uint64(val), 9)\n\tcase bitRange(val, 12): // -2047 <= val <= 2048, 17 bits.\n\t\tb.writeBits(0b11110, 5)\n\t\tb.writeBits(uint64(val), 12)\n\tcase bitRange(val, 18): // -131071 <= val <= 131072, 3 bytes.\n\t\tb.writeBits(0b111110, 6)\n\t\tb.writeBits(uint64(val), 18)\n\tcase bitRange(val, 25): // -16777215 <= val <= 16777216, 4 bytes.\n\t\tb.writeBits(0b1111110, 7)\n\t\tb.writeBits(uint64(val), 25)\n\tcase bitRange(val, 56): // -36028797018963967 <= val <= 36028797018963968, 8 bytes.\n\t\tb.writeBits(0b11111110, 8)\n\t\tb.writeBits(uint64(val), 56)\n\tdefault:\n\t\tb.writeBits(0b11111111, 8) // Worst case, needs 9 bytes.\n\t\tb.writeBits(uint64(val), 64)\n\t}\n}\n\n// readVarbitInt reads an int64 encoded with putVarbitInt.\nfunc readVarbitInt(b *bstreamReader) (int64, error) {\n\tvar d byte\n\tfor i := 0; i < 8; i++ {\n\t\td <<= 1\n\t\tbit, err := b.readBitFast()\n\t\tif err != nil {\n\t\t\tbit, err = b.readBit()\n\t\t}\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\t\tif bit == zero {\n\t\t\tbreak\n\t\t}\n\t\td |= 1\n\t}\n\n\tvar val int64\n\tvar sz uint8\n\n\tswitch d {\n\tcase 0b0:\n\t\t// val == 0\n\tcase 0b10:\n\t\tsz = 3\n\tcase 0b110:\n\t\tsz = 6\n\tcase 0b1110:\n\t\tsz = 9\n\tcase 0b11110:\n\t\tsz = 12\n\tcase 0b111110:\n\t\tsz = 18\n\tcase 0b1111110:\n\t\tsz = 25\n\tcase 0b11111110:\n\t\tsz = 56\n\tcase 0b11111111:\n\t\t// Do not use fast because it's very unlikely it will succeed.\n\t\tbits, err := b.readBits(64)\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\n\t\tval = int64(bits)\n\tdefault:\n\t\treturn 0, fmt.Errorf(\"invalid bit pattern %b\", d)\n\t}\n\n\tif sz != 0 {\n\t\tbits, err := b.readBitsFast(sz)\n\t\tif err != nil {\n\t\t\tbits, err = b.readBits(sz)\n\t\t}\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\t\tif bits > (1 << (sz - 1)) {\n\t\t\t// Or something.\n\t\t\tbits -= (1 << sz)\n\t\t}\n\t\tval = int64(bits)\n\t}\n\n\treturn val, nil\n}\n\nfunc bitRangeUint(x uint64, nbits int) bool {\n\treturn bits.LeadingZeros64(x) >= 64-nbits\n}\n\n// putVarbitUint writes a uint64 using varbit encoding. It uses the same bit\n// buckets as putVarbitInt.\nfunc putVarbitUint(b *bstream, val uint64) {\n\tswitch {\n\tcase val == 0: // Precisely 0, needs 1 bit.\n\t\tb.writeBit(zero)\n\tcase bitRangeUint(val, 3): // val <= 7, needs 5 bits.\n\t\tb.writeBits(0b10, 2)\n\t\tb.writeBits(val, 3)\n\tcase bitRangeUint(val, 6): // val <= 63, 9 bits.\n\t\tb.writeBits(0b110, 3)\n\t\tb.writeBits(val, 6)\n\tcase bitRangeUint(val, 9): // val <= 511, 13 bits.\n\t\tb.writeBits(0b1110, 4)\n\t\tb.writeBits(val, 9)\n\tcase bitRangeUint(val, 12): // val <= 4095, 17 bits.\n\t\tb.writeBits(0b11110, 5)\n\t\tb.writeBits(val, 12)\n\tcase bitRangeUint(val, 18): // val <= 262143, 3 bytes.\n\t\tb.writeBits(0b111110, 6)\n\t\tb.writeBits(val, 18)\n\tcase bitRangeUint(val, 25): // val <= 33554431, 4 bytes.\n\t\tb.writeBits(0b1111110, 7)\n\t\tb.writeBits(val, 25)\n\tcase bitRangeUint(val, 56): // val <= 72057594037927935, 8 bytes.\n\t\tb.writeBits(0b11111110, 8)\n\t\tb.writeBits(val, 56)\n\tdefault:\n\t\tb.writeBits(0b11111111, 8) // Worst case, needs 9 bytes.\n\t\tb.writeBits(val, 64)\n\t}\n}\n\n// readVarbitUint reads a uint64 encoded with putVarbitUint.\nfunc readVarbitUint(b *bstreamReader) (uint64, error) {\n\tvar d byte\n\tfor i := 0; i < 8; i++ {\n\t\td <<= 1\n\t\tbit, err := b.readBitFast()\n\t\tif err != nil {\n\t\t\tbit, err = b.readBit()\n\t\t}\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\t\tif bit == zero {\n\t\t\tbreak\n\t\t}\n\t\td |= 1\n\t}\n\n\tvar (\n\t\tbits uint64\n\t\tsz   uint8\n\t\terr  error\n\t)\n\n\tswitch d {\n\tcase 0b0:\n\t\t// val == 0\n\tcase 0b10:\n\t\tsz = 3\n\tcase 0b110:\n\t\tsz = 6\n\tcase 0b1110:\n\t\tsz = 9\n\tcase 0b11110:\n\t\tsz = 12\n\tcase 0b111110:\n\t\tsz = 18\n\tcase 0b1111110:\n\t\tsz = 25\n\tcase 0b11111110:\n\t\tsz = 56\n\tcase 0b11111111:\n\t\t// Do not use fast because it's very unlikely it will succeed.\n\t\tbits, err = b.readBits(64)\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\tdefault:\n\t\treturn 0, fmt.Errorf(\"invalid bit pattern %b\", d)\n\t}\n\n\tif sz != 0 {\n\t\tbits, err = b.readBitsFast(sz)\n\t\tif err != nil {\n\t\t\tbits, err = b.readBits(sz)\n\t\t}\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\t}\n\n\treturn bits, nil\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:47:45"}, {"role": "assistant", "content": "```go\n// Copyright 2021 The Prometheus Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage chunkenc\n\nimport (\n\t\"math\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestVarbitInt(t *testing.T) {\n\tnumbers := []int64{\n\t\tmath.MinInt64,\n\t\t-36028797018963968, -36028797018963967,\n\t\t-16777216, -16777215,\n\t\t-131072, -131071,\n\t\t-2048, -2047,\n\t\t-256, -255,\n\t\t-32, -31,\n\t\t-4, -3,\n\t\t-1, 0, 1,\n\t\t4, 5,\n\t\t32, 33,\n\t\t256, 257,\n\t\t2048, 2049,\n\t\t131072, 131073,\n\t\t16777216, 16777217,\n\t\t36028797018963968, 36028797018963969,\n\t\tmath.MaxInt64,\n\t}\n\n\tbs := bstream{}\n\n\tfor _, n := range numbers {\n\t\tputVarbitInt(&bs, n)\n\t}\n\n\tbsr := newBReader(bs.bytes())\n\n\tfor _, want := range numbers {\n\t\tgot, err := readVarbitInt(&bsr)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, want, got)\n\t}\n}\n\nfunc TestVarbitUint(t *testing.T) {\n\tnumbers := []uint64{\n\t\t0, 1,\n\t\t7, 8,\n\t\t63, 64,\n\t\t511, 512,\n\t\t4095, 4096,\n\t\t262143, 262144,\n\t\t33554431, 33554432,\n\t\t72057594037927935, 72057594037927936,\n\t\tmath.MaxUint64,\n\t}\n\n\tbs := bstream{}\n\n\tfor _, n := range numbers {\n\t\tputVarbitUint(&bs, n)\n\t}\n\n\tbsr := newBReader(bs.bytes())\n\n\tfor _, want := range numbers {\n\t\tgot, err := readVarbitUint(&bsr)\n\t\trequire.NoError(t, err)\n\t\trequire.Equal(t, want, got)\n\t}\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: caddy\nSource File Path(s): ['caddy\\\\dispenser_test.go\\\\source_files\\\\dispenser.go']\nTest File Path: caddy\\dispenser_test.go\\dispenser_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: caddy\\dispenser_test.go\\source_files\\dispenser.go\n// Copyright 2015 Matthew Holt and The Caddy Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage caddyfile\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log\"\n\t\"strconv\"\n\t\"strings\"\n)\n\n// Dispenser is a type that dispenses tokens, similarly to a lexer,\n// except that it can do so with some notion of structure. An empty\n// Dispenser is invalid; call NewDispenser to make a proper instance.\ntype Dispenser struct {\n\ttokens  []Token\n\tcursor  int\n\tnesting int\n\n\t// A map of arbitrary context data that can be used\n\t// to pass through some information to unmarshalers.\n\tcontext map[string]any\n}\n\n// NewDispenser returns a Dispenser filled with the given tokens.\nfunc NewDispenser(tokens []Token) *Dispenser {\n\treturn &Dispenser{\n\t\ttokens: tokens,\n\t\tcursor: -1,\n\t}\n}\n\n// NewTestDispenser parses input into tokens and creates a new\n// Dispenser for test purposes only; any errors are fatal.\nfunc NewTestDispenser(input string) *Dispenser {\n\ttokens, err := allTokens(\"Testfile\", []byte(input))\n\tif err != nil && err != io.EOF {\n\t\tlog.Fatalf(\"getting all tokens from input: %v\", err)\n\t}\n\treturn NewDispenser(tokens)\n}\n\n// Next loads the next token. Returns true if a token\n// was loaded; false otherwise. If false, all tokens\n// have been consumed.\nfunc (d *Dispenser) Next() bool {\n\tif d.cursor < len(d.tokens)-1 {\n\t\td.cursor++\n\t\treturn true\n\t}\n\treturn false\n}\n\n// Prev moves to the previous token. It does the inverse\n// of Next(), except this function may decrement the cursor\n// to -1 so that the next call to Next() points to the\n// first token; this allows dispensing to \"start over\". This\n// method returns true if the cursor ends up pointing to a\n// valid token.\nfunc (d *Dispenser) Prev() bool {\n\tif d.cursor > -1 {\n\t\td.cursor--\n\t\treturn d.cursor > -1\n\t}\n\treturn false\n}\n\n// NextArg loads the next token if it is on the same\n// line and if it is not a block opening (open curly\n// brace). Returns true if an argument token was\n// loaded; false otherwise. If false, all tokens on\n// the line have been consumed except for potentially\n// a block opening. It handles imported tokens\n// correctly.\nfunc (d *Dispenser) NextArg() bool {\n\tif !d.nextOnSameLine() {\n\t\treturn false\n\t}\n\tif d.Val() == \"{\" {\n\t\t// roll back; a block opening is not an argument\n\t\td.cursor--\n\t\treturn false\n\t}\n\treturn true\n}\n\n// nextOnSameLine advances the cursor if the next\n// token is on the same line of the same file.\nfunc (d *Dispenser) nextOnSameLine() bool {\n\tif d.cursor < 0 {\n\t\td.cursor++\n\t\treturn true\n\t}\n\tif d.cursor >= len(d.tokens)-1 {\n\t\treturn false\n\t}\n\tcurr := d.tokens[d.cursor]\n\tnext := d.tokens[d.cursor+1]\n\tif !isNextOnNewLine(curr, next) {\n\t\td.cursor++\n\t\treturn true\n\t}\n\treturn false\n}\n\n// NextLine loads the next token only if it is not on the same\n// line as the current token, and returns true if a token was\n// loaded; false otherwise. If false, there is not another token\n// or it is on the same line. It handles imported tokens correctly.\nfunc (d *Dispenser) NextLine() bool {\n\tif d.cursor < 0 {\n\t\td.cursor++\n\t\treturn true\n\t}\n\tif d.cursor >= len(d.tokens)-1 {\n\t\treturn false\n\t}\n\tcurr := d.tokens[d.cursor]\n\tnext := d.tokens[d.cursor+1]\n\tif isNextOnNewLine(curr, next) {\n\t\td.cursor++\n\t\treturn true\n\t}\n\treturn false\n}\n\n// NextBlock can be used as the condition of a for loop\n// to load the next token as long as it opens a block or\n// is already in a block nested more than initialNestingLevel.\n// In other words, a loop over NextBlock() will iterate\n// all tokens in the block assuming the next token is an\n// open curly brace, until the matching closing brace.\n// The open and closing brace tokens for the outer-most\n// block will be consumed internally and omitted from\n// the iteration.\n//\n// Proper use of this method looks like this:\n//\n//\tfor nesting := d.Nesting(); d.NextBlock(nesting); {\n//\t}\n//\n// However, in simple cases where it is known that the\n// Dispenser is new and has not already traversed state\n// by a loop over NextBlock(), this will do:\n//\n//\tfor d.NextBlock(0) {\n//\t}\n//\n// As with other token parsing logic, a loop over\n// NextBlock() should be contained within a loop over\n// Next(), as it is usually prudent to skip the initial\n// token.\nfunc (d *Dispenser) NextBlock(initialNestingLevel int) bool {\n\tif d.nesting > initialNestingLevel {\n\t\tif !d.Next() {\n\t\t\treturn false // should be EOF error\n\t\t}\n\t\tif d.Val() == \"}\" && !d.nextOnSameLine() {\n\t\t\td.nesting--\n\t\t} else if d.Val() == \"{\" && !d.nextOnSameLine() {\n\t\t\td.nesting++\n\t\t}\n\t\treturn d.nesting > initialNestingLevel\n\t}\n\tif !d.nextOnSameLine() { // block must open on same line\n\t\treturn false\n\t}\n\tif d.Val() != \"{\" {\n\t\td.cursor-- // roll back if not opening brace\n\t\treturn false\n\t}\n\td.Next() // consume open curly brace\n\tif d.Val() == \"}\" {\n\t\treturn false // open and then closed right away\n\t}\n\td.nesting++\n\treturn true\n}\n\n// Nesting returns the current nesting level. Necessary\n// if using NextBlock()\nfunc (d *Dispenser) Nesting() int {\n\treturn d.nesting\n}\n\n// Val gets the text of the current token. If there is no token\n// loaded, it returns empty string.\nfunc (d *Dispenser) Val() string {\n\tif d.cursor < 0 || d.cursor >= len(d.tokens) {\n\t\treturn \"\"\n\t}\n\treturn d.tokens[d.cursor].Text\n}\n\n// ValRaw gets the raw text of the current token (including quotes).\n// If the token was a heredoc, then the delimiter is not included,\n// because that is not relevant to any unmarshaling logic at this time.\n// If there is no token loaded, it returns empty string.\nfunc (d *Dispenser) ValRaw() string {\n\tif d.cursor < 0 || d.cursor >= len(d.tokens) {\n\t\treturn \"\"\n\t}\n\tquote := d.tokens[d.cursor].wasQuoted\n\tif quote > 0 && quote != '<' {\n\t\t// string literal\n\t\treturn string(quote) + d.tokens[d.cursor].Text + string(quote)\n\t}\n\treturn d.tokens[d.cursor].Text\n}\n\n// ScalarVal gets value of the current token, converted to the closest\n// scalar type. If there is no token loaded, it returns nil.\nfunc (d *Dispenser) ScalarVal() any {\n\tif d.cursor < 0 || d.cursor >= len(d.tokens) {\n\t\treturn nil\n\t}\n\tquote := d.tokens[d.cursor].wasQuoted\n\ttext := d.tokens[d.cursor].Text\n\n\tif quote > 0 {\n\t\treturn text // string literal\n\t}\n\tif num, err := strconv.Atoi(text); err == nil {\n\t\treturn num\n\t}\n\tif num, err := strconv.ParseFloat(text, 64); err == nil {\n\t\treturn num\n\t}\n\tif bool, err := strconv.ParseBool(text); err == nil {\n\t\treturn bool\n\t}\n\treturn text\n}\n\n// Line gets the line number of the current token.\n// If there is no token loaded, it returns 0.\nfunc (d *Dispenser) Line() int {\n\tif d.cursor < 0 || d.cursor >= len(d.tokens) {\n\t\treturn 0\n\t}\n\treturn d.tokens[d.cursor].Line\n}\n\n// File gets the filename where the current token originated.\nfunc (d *Dispenser) File() string {\n\tif d.cursor < 0 || d.cursor >= len(d.tokens) {\n\t\treturn \"\"\n\t}\n\treturn d.tokens[d.cursor].File\n}\n\n// Args is a convenience function that loads the next arguments\n// (tokens on the same line) into an arbitrary number of strings\n// pointed to in targets. If there are not enough argument tokens\n// available to fill targets, false is returned and the remaining\n// targets are left unchanged. If all the targets are filled,\n// then true is returned.\nfunc (d *Dispenser) Args(targets ...*string) bool {\n\tfor i := 0; i < len(targets); i++ {\n\t\tif !d.NextArg() {\n\t\t\treturn false\n\t\t}\n\t\t*targets[i] = d.Val()\n\t}\n\treturn true\n}\n\n// AllArgs is like Args, but if there are more argument tokens\n// available than there are targets, false is returned. The\n// number of available argument tokens must match the number of\n// targets exactly to return true.\nfunc (d *Dispenser) AllArgs(targets ...*string) bool {\n\tif !d.Args(targets...) {\n\t\treturn false\n\t}\n\tif d.NextArg() {\n\t\td.Prev()\n\t\treturn false\n\t}\n\treturn true\n}\n\n// CountRemainingArgs counts the amount of remaining arguments\n// (tokens on the same line) without consuming the tokens.\nfunc (d *Dispenser) CountRemainingArgs() int {\n\tcount := 0\n\tfor d.NextArg() {\n\t\tcount++\n\t}\n\tfor i := 0; i < count; i++ {\n\t\td.Prev()\n\t}\n\treturn count\n}\n\n// RemainingArgs loads any more arguments (tokens on the same line)\n// into a slice and returns them. Open curly brace tokens also indicate\n// the end of arguments, and the curly brace is not included in\n// the return value nor is it loaded.\nfunc (d *Dispenser) RemainingArgs() []string {\n\tvar args []string\n\tfor d.NextArg() {\n\t\targs = append(args, d.Val())\n\t}\n\treturn args\n}\n\n// RemainingArgsRaw loads any more arguments (tokens on the same line,\n// retaining quotes) into a slice and returns them. Open curly brace\n// tokens also indicate the end of arguments, and the curly brace is\n// not included in the return value nor is it loaded.\nfunc (d *Dispenser) RemainingArgsRaw() []string {\n\tvar args []string\n\tfor d.NextArg() {\n\t\targs = append(args, d.ValRaw())\n\t}\n\treturn args\n}\n\n// NewFromNextSegment returns a new dispenser with a copy of\n// the tokens from the current token until the end of the\n// \"directive\" whether that be to the end of the line or\n// the end of a block that starts at the end of the line;\n// in other words, until the end of the segment.\nfunc (d *Dispenser) NewFromNextSegment() *Dispenser {\n\treturn NewDispenser(d.NextSegment())\n}\n\n// NextSegment returns a copy of the tokens from the current\n// token until the end of the line or block that starts at\n// the end of the line.\nfunc (d *Dispenser) NextSegment() Segment {\n\ttkns := Segment{d.Token()}\n\tfor d.NextArg() {\n\t\ttkns = append(tkns, d.Token())\n\t}\n\tvar openedBlock bool\n\tfor nesting := d.Nesting(); d.NextBlock(nesting); {\n\t\tif !openedBlock {\n\t\t\t// because NextBlock() consumes the initial open\n\t\t\t// curly brace, we rewind here to append it, since\n\t\t\t// our case is special in that we want the new\n\t\t\t// dispenser to have all the tokens including\n\t\t\t// surrounding curly braces\n\t\t\td.Prev()\n\t\t\ttkns = append(tkns, d.Token())\n\t\t\td.Next()\n\t\t\topenedBlock = true\n\t\t}\n\t\ttkns = append(tkns, d.Token())\n\t}\n\tif openedBlock {\n\t\t// include closing brace\n\t\ttkns = append(tkns, d.Token())\n\n\t\t// do not consume the closing curly brace; the\n\t\t// next iteration of the enclosing loop will\n\t\t// call Next() and consume it\n\t}\n\treturn tkns\n}\n\n// Token returns the current token.\nfunc (d *Dispenser) Token() Token {\n\tif d.cursor < 0 || d.cursor >= len(d.tokens) {\n\t\treturn Token{}\n\t}\n\treturn d.tokens[d.cursor]\n}\n\n// Reset sets d's cursor to the beginning, as\n// if this was a new and unused dispenser.\nfunc (d *Dispenser) Reset() {\n\td.cursor = -1\n\td.nesting = 0\n}\n\n// ArgErr returns an argument error, meaning that another\n// argument was expected but not found. In other words,\n// a line break or open curly brace was encountered instead of\n// an argument.\nfunc (d *Dispenser) ArgErr() error {\n\tif d.Val() == \"{\" {\n\t\treturn d.Err(\"unexpected token '{', expecting argument\")\n\t}\n\treturn d.Errf(\"wrong argument count or unexpected line ending after '%s'\", d.Val())\n}\n\n// SyntaxErr creates a generic syntax error which explains what was\n// found and what was expected.\nfunc (d *Dispenser) SyntaxErr(expected string) error {\n\tmsg := fmt.Sprintf(\"syntax error: unexpected token '%s', expecting '%s', at %s:%d import chain: ['%s']\", d.Val(), expected, d.File(), d.Line(), strings.Join(d.Token().imports, \"','\"))\n\treturn errors.New(msg)\n}\n\n// EOFErr returns an error indicating that the dispenser reached\n// the end of the input when searching for the next token.\nfunc (d *Dispenser) EOFErr() error {\n\treturn d.Errf(\"unexpected EOF\")\n}\n\n// Err generates a custom parse-time error with a message of msg.\nfunc (d *Dispenser) Err(msg string) error {\n\treturn d.WrapErr(errors.New(msg))\n}\n\n// Errf is like Err, but for formatted error messages\nfunc (d *Dispenser) Errf(format string, args ...any) error {\n\treturn d.WrapErr(fmt.Errorf(format, args...))\n}\n\n// WrapErr takes an existing error and adds the Caddyfile file and line number.\nfunc (d *Dispenser) WrapErr(err error) error {\n\tif len(d.Token().imports) > 0 {\n\t\treturn fmt.Errorf(\"%w, at %s:%d import chain ['%s']\", err, d.File(), d.Line(), strings.Join(d.Token().imports, \"','\"))\n\t}\n\treturn fmt.Errorf(\"%w, at %s:%d\", err, d.File(), d.Line())\n}\n\n// Delete deletes the current token and returns the updated slice\n// of tokens. The cursor is not advanced to the next token.\n// Because deletion modifies the underlying slice, this method\n// should only be called if you have access to the original slice\n// of tokens and/or are using the slice of tokens outside this\n// Dispenser instance. If you do not re-assign the slice with the\n// return value of this method, inconsistencies in the token\n// array will become apparent (or worse, hide from you like they\n// did me for 3 and a half freaking hours late one night).\nfunc (d *Dispenser) Delete() []Token {\n\tif d.cursor >= 0 && d.cursor <= len(d.tokens)-1 {\n\t\td.tokens = append(d.tokens[:d.cursor], d.tokens[d.cursor+1:]...)\n\t\td.cursor--\n\t}\n\treturn d.tokens\n}\n\n// DeleteN is the same as Delete, but can delete many tokens at once.\n// If there aren't N tokens available to delete, none are deleted.\nfunc (d *Dispenser) DeleteN(amount int) []Token {\n\tif amount > 0 && d.cursor >= (amount-1) && d.cursor <= len(d.tokens)-1 {\n\t\td.tokens = append(d.tokens[:d.cursor-(amount-1)], d.tokens[d.cursor+1:]...)\n\t\td.cursor -= amount\n\t}\n\treturn d.tokens\n}\n\n// SetContext sets a key-value pair in the context map.\nfunc (d *Dispenser) SetContext(key string, value any) {\n\tif d.context == nil {\n\t\td.context = make(map[string]any)\n\t}\n\td.context[key] = value\n}\n\n// GetContext gets the value of a key in the context map.\nfunc (d *Dispenser) GetContext(key string) any {\n\tif d.context == nil {\n\t\treturn nil\n\t}\n\treturn d.context[key]\n}\n\n// GetContextString gets the value of a key in the context map\n// as a string, or an empty string if the key does not exist.\nfunc (d *Dispenser) GetContextString(key string) string {\n\tif d.context == nil {\n\t\treturn \"\"\n\t}\n\tif val, ok := d.context[key].(string); ok {\n\t\treturn val\n\t}\n\treturn \"\"\n}\n\n// isNewLine determines whether the current token is on a different\n// line (higher line number) than the previous token. It handles imported\n// tokens correctly. If there isn't a previous token, it returns true.\nfunc (d *Dispenser) isNewLine() bool {\n\tif d.cursor < 1 {\n\t\treturn true\n\t}\n\tif d.cursor > len(d.tokens)-1 {\n\t\treturn false\n\t}\n\n\tprev := d.tokens[d.cursor-1]\n\tcurr := d.tokens[d.cursor]\n\treturn isNextOnNewLine(prev, curr)\n}\n\n// isNextOnNewLine determines whether the current token is on a different\n// line (higher line number) than the next token. It handles imported\n// tokens correctly. If there isn't a next token, it returns true.\nfunc (d *Dispenser) isNextOnNewLine() bool {\n\tif d.cursor < 0 {\n\t\treturn false\n\t}\n\tif d.cursor >= len(d.tokens)-1 {\n\t\treturn true\n\t}\n\n\tcurr := d.tokens[d.cursor]\n\tnext := d.tokens[d.cursor+1]\n\treturn isNextOnNewLine(curr, next)\n}\n\nconst MatcherNameCtxKey = \"matcher_name\"\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:48:09"}, {"role": "assistant", "content": "```go\n// Copyright 2015 Matthew Holt and The Caddy Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage caddyfile\n\nimport (\n\t\"errors\"\n\t\"reflect\"\n\t\"strings\"\n\t\"testing\"\n)\n\nfunc TestDispenser_Val_Next(t *testing.T) {\n\tinput := `host:port\n\t\t\t  dir1 arg1\n\t\t\t  dir2 arg2 arg3\n\t\t\t  dir3`\n\td := NewTestDispenser(input)\n\n\tif val := d.Val(); val != \"\" {\n\t\tt.Fatalf(\"Val(): Should return empty string when no token loaded; got '%s'\", val)\n\t}\n\n\tassertNext := func(shouldLoad bool, expectedCursor int, expectedVal string) {\n\t\tif loaded := d.Next(); loaded != shouldLoad {\n\t\t\tt.Errorf(\"Next(): Expected %v but got %v instead (val '%s')\", shouldLoad, loaded, d.Val())\n\t\t}\n\t\tif d.cursor != expectedCursor {\n\t\t\tt.Errorf(\"Expected cursor to be %d, but was %d\", expectedCursor, d.cursor)\n\t\t}\n\t\tif d.nesting != 0 {\n\t\t\tt.Errorf(\"Nesting should be 0, was %d instead\", d.nesting)\n\t\t}\n\t\tif val := d.Val(); val != expectedVal {\n\t\t\tt.Errorf(\"Val(): Expected '%s' but got '%s'\", expectedVal, val)\n\t\t}\n\t}\n\n\tassertNext(true, 0, \"host:port\")\n\tassertNext(true, 1, \"dir1\")\n\tassertNext(true, 2, \"arg1\")\n\tassertNext(true, 3, \"dir2\")\n\tassertNext(true, 4, \"arg2\")\n\tassertNext(true, 5, \"arg3\")\n\tassertNext(true, 6, \"dir3\")\n\t// Note: This next test simply asserts existing behavior.\n\t// If desired, we may wish to empty the token value after\n\t// reading past the EOF. Open an issue if you want this change.\n\tassertNext(false, 6, \"dir3\")\n}\n\nfunc TestDispenser_NextArg(t *testing.T) {\n\tinput := `dir1 arg1\n\t\t\t  dir2 arg2 arg3\n\t\t\t  dir3`\n\td := NewTestDispenser(input)\n\n\tassertNext := func(shouldLoad bool, expectedVal string, expectedCursor int) {\n\t\tif d.Next() != shouldLoad {\n\t\t\tt.Errorf(\"Next(): Should load token but got false instead (val: '%s')\", d.Val())\n\t\t}\n\t\tif d.cursor != expectedCursor {\n\t\t\tt.Errorf(\"Next(): Expected cursor to be at %d, but it was %d\", expectedCursor, d.cursor)\n\t\t}\n\t\tif val := d.Val(); val != expectedVal {\n\t\t\tt.Errorf(\"Val(): Expected '%s' but got '%s'\", expectedVal, val)\n\t\t}\n\t}\n\n\tassertNextArg := func(expectedVal string, loadAnother bool, expectedCursor int) {\n\t\tif !d.NextArg() {\n\t\t\tt.Error(\"NextArg(): Should load next argument but got false instead\")\n\t\t}\n\t\tif d.cursor != expectedCursor {\n\t\t\tt.Errorf(\"NextArg(): Expected cursor to be at %d, but it was %d\", expectedCursor, d.cursor)\n\t\t}\n\t\tif val := d.Val(); val != expectedVal {\n\t\t\tt.Errorf(\"Val(): Expected '%s' but got '%s'\", expectedVal, val)\n\t\t}\n\t\tif !loadAnother {\n\t\t\tif d.NextArg() {\n\t\t\t\tt.Fatalf(\"NextArg(): Should NOT load another argument, but got true instead (val: '%s')\", d.Val())\n\t\t\t}\n\t\t\tif d.cursor != expectedCursor {\n\t\t\t\tt.Errorf(\"NextArg(): Expected cursor to remain at %d, but it was %d\", expectedCursor, d.cursor)\n\t\t\t}\n\t\t}\n\t}\n\n\tassertNext(true, \"dir1\", 0)\n\tassertNextArg(\"arg1\", false, 1)\n\tassertNext(true, \"dir2\", 2)\n\tassertNextArg(\"arg2\", true, 3)\n\tassertNextArg(\"arg3\", false, 4)\n\tassertNext(true, \"dir3\", 5)\n\tassertNext(false, \"dir3\", 5)\n}\n\nfunc TestDispenser_NextLine(t *testing.T) {\n\tinput := `host:port\n\t\t\t  dir1 arg1\n\t\t\t  dir2 arg2 arg3`\n\td := NewTestDispenser(input)\n\n\tassertNextLine := func(shouldLoad bool, expectedVal string, expectedCursor int) {\n\t\tif d.NextLine() != shouldLoad {\n\t\t\tt.Errorf(\"NextLine(): Should load token but got false instead (val: '%s')\", d.Val())\n\t\t}\n\t\tif d.cursor != expectedCursor {\n\t\t\tt.Errorf(\"NextLine(): Expected cursor to be %d, instead was %d\", expectedCursor, d.cursor)\n\t\t}\n\t\tif val := d.Val(); val != expectedVal {\n\t\t\tt.Errorf(\"Val(): Expected '%s' but got '%s'\", expectedVal, val)\n\t\t}\n\t}\n\n\tassertNextLine(true, \"host:port\", 0)\n\tassertNextLine(true, \"dir1\", 1)\n\tassertNextLine(false, \"dir1\", 1)\n\td.Next() // arg1\n\tassertNextLine(true, \"dir2\", 3)\n\tassertNextLine(false, \"dir2\", 3)\n\td.Next() // arg2\n\tassertNextLine(false, \"arg2\", 4)\n\td.Next() // arg3\n\tassertNextLine(false, \"arg3\", 5)\n}\n\nfunc TestDispenser_NextBlock(t *testing.T) {\n\tinput := `foobar1 {\n\t\t\t  \tsub1 arg1\n\t\t\t  \tsub2\n\t\t\t  }\n\t\t\t  foobar2 {\n\t\t\t  }`\n\td := NewTestDispenser(input)\n\n\tassertNextBlock := func(shouldLoad bool, expectedCursor, expectedNesting int) {\n\t\tif loaded := d.NextBlock(0); loaded != shouldLoad {\n\t\t\tt.Errorf(\"NextBlock(): Should return %v but got %v\", shouldLoad, loaded)\n\t\t}\n\t\tif d.cursor != expectedCursor {\n\t\t\tt.Errorf(\"NextBlock(): Expected cursor to be %d, was %d\", expectedCursor, d.cursor)\n\t\t}\n\t\tif d.nesting != expectedNesting {\n\t\t\tt.Errorf(\"NextBlock(): Nesting should be %d, not %d\", expectedNesting, d.nesting)\n\t\t}\n\t}\n\n\tassertNextBlock(false, -1, 0)\n\td.Next() // foobar1\n\tassertNextBlock(true, 2, 1)\n\tassertNextBlock(true, 3, 1)\n\tassertNextBlock(true, 4, 1)\n\tassertNextBlock(false, 5, 0)\n\td.Next()                     // foobar2\n\tassertNextBlock(false, 8, 0) // empty block is as if it didn't exist\n}\n\nfunc TestDispenser_Args(t *testing.T) {\n\tvar s1, s2, s3 string\n\tinput := `dir1 arg1 arg2 arg3\n\t\t\t  dir2 arg4 arg5\n\t\t\t  dir3 arg6 arg7\n\t\t\t  dir4`\n\td := NewTestDispenser(input)\n\n\td.Next() // dir1\n\n\t// As many strings as arguments\n\tif all := d.Args(&s1, &s2, &s3); !all {\n\t\tt.Error(\"Args(): Expected true, got false\")\n\t}\n\tif s1 != \"arg1\" {\n\t\tt.Errorf(\"Args(): Expected s1 to be 'arg1', got '%s'\", s1)\n\t}\n\tif s2 != \"arg2\" {\n\t\tt.Errorf(\"Args(): Expected s2 to be 'arg2', got '%s'\", s2)\n\t}\n\tif s3 != \"arg3\" {\n\t\tt.Errorf(\"Args(): Expected s3 to be 'arg3', got '%s'\", s3)\n\t}\n\n\td.Next() // dir2\n\n\t// More strings than arguments\n\tif all := d.Args(&s1, &s2, &s3); all {\n\t\tt.Error(\"Args(): Expected false, got true\")\n\t}\n\tif s1 != \"arg4\" {\n\t\tt.Errorf(\"Args(): Expected s1 to be 'arg4', got '%s'\", s1)\n\t}\n\tif s2 != \"arg5\" {\n\t\tt.Errorf(\"Args(): Expected s2 to be 'arg5', got '%s'\", s2)\n\t}\n\tif s3 != \"arg3\" {\n\t\tt.Errorf(\"Args(): Expected s3 to be unchanged ('arg3'), instead got '%s'\", s3)\n\t}\n\n\t// (quick cursor check just for kicks and giggles)\n\tif d.cursor != 6 {\n\t\tt.Errorf(\"Cursor should be 6, but is %d\", d.cursor)\n\t}\n\n\td.Next() // dir3\n\n\t// More arguments than strings\n\tif all := d.Args(&s1); !all {\n\t\tt.Error(\"Args(): Expected true, got false\")\n\t}\n\tif s1 != \"arg6\" {\n\t\tt.Errorf(\"Args(): Expected s1 to be 'arg6', got '%s'\", s1)\n\t}\n\n\td.Next() // dir4\n\n\t// No arguments or strings\n\tif all := d.Args(); !all {\n\t\tt.Error(\"Args(): Expected true, got false\")\n\t}\n\n\t// No arguments but at least one string\n\tif all := d.Args(&s1); all {\n\t\tt.Error(\"Args(): Expected false, got true\")\n\t}\n}\n\nfunc TestDispenser_RemainingArgs(t *testing.T) {\n\tinput := `dir1 arg1 arg2 arg3\n\t\t\t  dir2 arg4 arg5\n\t\t\t  dir3 arg6 { arg7\n\t\t\t  dir4`\n\td := NewTestDispenser(input)\n\n\td.Next() // dir1\n\n\targs := d.RemainingArgs()\n\tif expected := []string{\"arg1\", \"arg2\", \"arg3\"}; !reflect.DeepEqual(args, expected) {\n\t\tt.Errorf(\"RemainingArgs(): Expected %v, got %v\", expected, args)\n\t}\n\n\td.Next() // dir2\n\n\targs = d.RemainingArgs()\n\tif expected := []string{\"arg4\", \"arg5\"}; !reflect.DeepEqual(args, expected) {\n\t\tt.Errorf(\"RemainingArgs(): Expected %v, got %v\", expected, args)\n\t}\n\n\td.Next() // dir3\n\n\targs = d.RemainingArgs()\n\tif expected := []string{\"arg6\"}; !reflect.DeepEqual(args, expected) {\n\t\tt.Errorf(\"RemainingArgs(): Expected %v, got %v\", expected, args)\n\t}\n\n\td.Next() // {\n\td.Next() // arg7\n\td.Next() // dir4\n\n\targs = d.RemainingArgs()\n\tif len(args) != 0 {\n\t\tt.Errorf(\"RemainingArgs(): Expected %v, got %v\", []string{}, args)\n\t}\n}\n\nfunc TestDispenser_ArgErr_Err(t *testing.T) {\n\tinput := `dir1 {\n\t\t\t  }\n\t\t\t  dir2 arg1 arg2`\n\td := NewTestDispenser(input)\n\n\td.cursor = 1 // {\n\n\tif err := d.ArgErr(); err == nil || !strings.Contains(err.Error(), \"{\") {\n\t\tt.Errorf(\"ArgErr(): Expected an error message with { in it, but got '%v'\", err)\n\t}\n\n\td.cursor = 5 // arg2\n\n\tif err := d.ArgErr(); err == nil || !strings.Contains(err.Error(), \"arg2\") {\n\t\tt.Errorf(\"ArgErr(): Expected an error message with 'arg2' in it; got '%v'\", err)\n\t}\n\n\terr := d.Err(\"foobar\")\n\tif err == nil {\n\t\tt.Fatalf(\"Err(): Expected an error, got nil\")\n\t}\n\n\tif !strings.Contains(err.Error(), \"Testfile:3\") {\n\t\tt.Errorf(\"Expected error message with filename:line in it; got '%v'\", err)\n\t}\n\n\tif !strings.Contains(err.Error(), \"foobar\") {\n\t\tt.Errorf(\"Expected error message with custom message in it ('foobar'); got '%v'\", err)\n\t}\n\n\tErrBarIsFull := errors.New(\"bar is full\")\n\tbookingError := d.Errf(\"unable to reserve: %w\", ErrBarIsFull)\n\tif !errors.Is(bookingError, ErrBarIsFull) {\n\t\tt.Errorf(\"Errf(): should be able to unwrap the error chain\")\n\t}\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: caddy\nSource File Path(s): ['caddy\\\\formatter_test.go\\\\source_files\\\\formatter.go']\nTest File Path: caddy\\formatter_test.go\\formatter_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: caddy\\formatter_test.go\\source_files\\formatter.go\n// Copyright 2015 Matthew Holt and The Caddy Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage caddyfile\n\nimport (\n\t\"bytes\"\n\t\"io\"\n\t\"slices\"\n\t\"unicode\"\n)\n\n// Format formats the input Caddyfile to a standard, nice-looking\n// appearance. It works by reading each rune of the input and taking\n// control over all the bracing and whitespace that is written; otherwise,\n// words, comments, placeholders, and escaped characters are all treated\n// literally and written as they appear in the input.\nfunc Format(input []byte) []byte {\n\tinput = bytes.TrimSpace(input)\n\n\tout := new(bytes.Buffer)\n\trdr := bytes.NewReader(input)\n\n\ttype heredocState int\n\n\tconst (\n\t\theredocClosed  heredocState = 0\n\t\theredocOpening heredocState = 1\n\t\theredocOpened  heredocState = 2\n\t)\n\n\tvar (\n\t\tlast rune // the last character that was written to the result\n\n\t\tspace           = true // whether current/previous character was whitespace (beginning of input counts as space)\n\t\tbeginningOfLine = true // whether we are at beginning of line\n\n\t\topenBrace        bool // whether current word/token is or started with open curly brace\n\t\topenBraceWritten bool // if openBrace, whether that brace was written or not\n\t\topenBraceSpace   bool // whether there was a non-newline space before open brace\n\n\t\tnewLines int // count of newlines consumed\n\n\t\tcomment bool // whether we're in a comment\n\t\tquoted  bool // whether we're in a quoted segment\n\t\tescaped bool // whether current char is escaped\n\n\t\theredoc              heredocState // whether we're in a heredoc\n\t\theredocEscaped       bool         // whether heredoc is escaped\n\t\theredocMarker        []rune\n\t\theredocClosingMarker []rune\n\n\t\tnesting int // indentation level\n\t)\n\n\twrite := func(ch rune) {\n\t\tout.WriteRune(ch)\n\t\tlast = ch\n\t}\n\n\tindent := func() {\n\t\tfor tabs := nesting; tabs > 0; tabs-- {\n\t\t\twrite('\\t')\n\t\t}\n\t}\n\n\tnextLine := func() {\n\t\twrite('\\n')\n\t\tbeginningOfLine = true\n\t}\n\n\tfor {\n\t\tch, _, err := rdr.ReadRune()\n\t\tif err != nil {\n\t\t\tif err == io.EOF {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tpanic(err)\n\t\t}\n\n\t\t// detect whether we have the start of a heredoc\n\t\tif !quoted && !(heredoc != heredocClosed || heredocEscaped) &&\n\t\t\tspace && last == '<' && ch == '<' {\n\t\t\twrite(ch)\n\t\t\theredoc = heredocOpening\n\t\t\tspace = false\n\t\t\tcontinue\n\t\t}\n\n\t\tif heredoc == heredocOpening {\n\t\t\tif ch == '\\n' {\n\t\t\t\tif len(heredocMarker) > 0 && heredocMarkerRegexp.MatchString(string(heredocMarker)) {\n\t\t\t\t\theredoc = heredocOpened\n\t\t\t\t} else {\n\t\t\t\t\theredocMarker = nil\n\t\t\t\t\theredoc = heredocClosed\n\t\t\t\t\tnextLine()\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\twrite(ch)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif unicode.IsSpace(ch) {\n\t\t\t\t// a space means it's just a regular token and not a heredoc\n\t\t\t\theredocMarker = nil\n\t\t\t\theredoc = heredocClosed\n\t\t\t} else {\n\t\t\t\theredocMarker = append(heredocMarker, ch)\n\t\t\t\twrite(ch)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\t\t// if we're in a heredoc, all characters are read&write as-is\n\t\tif heredoc == heredocOpened {\n\t\t\theredocClosingMarker = append(heredocClosingMarker, ch)\n\t\t\tif len(heredocClosingMarker) > len(heredocMarker)+1 { // We assert that the heredocClosingMarker is followed by a unicode.Space\n\t\t\t\theredocClosingMarker = heredocClosingMarker[1:]\n\t\t\t}\n\t\t\t// check if we're done\n\t\t\tif unicode.IsSpace(ch) && slices.Equal(heredocClosingMarker[:len(heredocClosingMarker)-1], heredocMarker) {\n\t\t\t\theredocMarker = nil\n\t\t\t\theredocClosingMarker = nil\n\t\t\t\theredoc = heredocClosed\n\t\t\t} else {\n\t\t\t\twrite(ch)\n\t\t\t\tif ch == '\\n' {\n\t\t\t\t\theredocClosingMarker = heredocClosingMarker[:0]\n\t\t\t\t}\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\tif last == '<' && space {\n\t\t\tspace = false\n\t\t}\n\n\t\tif comment {\n\t\t\tif ch == '\\n' {\n\t\t\t\tcomment = false\n\t\t\t\tspace = true\n\t\t\t\tnextLine()\n\t\t\t\tcontinue\n\t\t\t} else {\n\t\t\t\twrite(ch)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\tif !escaped && ch == '\\\\' {\n\t\t\tif space {\n\t\t\t\twrite(' ')\n\t\t\t\tspace = false\n\t\t\t}\n\t\t\twrite(ch)\n\t\t\tescaped = true\n\t\t\tcontinue\n\t\t}\n\n\t\tif escaped {\n\t\t\tif ch == '<' {\n\t\t\t\theredocEscaped = true\n\t\t\t}\n\t\t\twrite(ch)\n\t\t\tescaped = false\n\t\t\tcontinue\n\t\t}\n\n\t\tif quoted {\n\t\t\tif ch == '\"' {\n\t\t\t\tquoted = false\n\t\t\t}\n\t\t\twrite(ch)\n\t\t\tcontinue\n\t\t}\n\n\t\tif space && ch == '\"' {\n\t\t\tquoted = true\n\t\t}\n\n\t\tif unicode.IsSpace(ch) {\n\t\t\tspace = true\n\t\t\theredocEscaped = false\n\t\t\tif ch == '\\n' {\n\t\t\t\tnewLines++\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tspacePrior := space\n\t\tspace = false\n\n\t\t//////////////////////////////////////////////////////////\n\t\t// I find it helpful to think of the formatting loop in two\n\t\t// main sections; by the time we reach this point, we\n\t\t// know we are in a \"regular\" part of the file: we know\n\t\t// the character is not a space, not in a literal segment\n\t\t// like a comment or quoted, it's not escaped, etc.\n\t\t//////////////////////////////////////////////////////////\n\n\t\tif ch == '#' {\n\t\t\tcomment = true\n\t\t}\n\n\t\tif openBrace && spacePrior && !openBraceWritten {\n\t\t\tif nesting == 0 && last == '}' {\n\t\t\t\tnextLine()\n\t\t\t\tnextLine()\n\t\t\t}\n\n\t\t\topenBrace = false\n\t\t\tif beginningOfLine {\n\t\t\t\tindent()\n\t\t\t} else if !openBraceSpace {\n\t\t\t\twrite(' ')\n\t\t\t}\n\t\t\twrite('{')\n\t\t\topenBraceWritten = true\n\t\t\tnextLine()\n\t\t\tnewLines = 0\n\t\t\t// prevent infinite nesting from ridiculous inputs (issue #4169)\n\t\t\tif nesting < 10 {\n\t\t\t\tnesting++\n\t\t\t}\n\t\t}\n\n\t\tswitch {\n\t\tcase ch == '{':\n\t\t\topenBrace = true\n\t\t\topenBraceWritten = false\n\t\t\topenBraceSpace = spacePrior && !beginningOfLine\n\t\t\tif openBraceSpace {\n\t\t\t\twrite(' ')\n\t\t\t}\n\t\t\tcontinue\n\n\t\tcase ch == '}' && (spacePrior || !openBrace):\n\t\t\tif last != '\\n' {\n\t\t\t\tnextLine()\n\t\t\t}\n\t\t\tif nesting > 0 {\n\t\t\t\tnesting--\n\t\t\t}\n\t\t\tindent()\n\t\t\twrite('}')\n\t\t\tnewLines = 0\n\t\t\tcontinue\n\t\t}\n\n\t\tif newLines > 2 {\n\t\t\tnewLines = 2\n\t\t}\n\t\tfor i := 0; i < newLines; i++ {\n\t\t\tnextLine()\n\t\t}\n\t\tnewLines = 0\n\t\tif beginningOfLine {\n\t\t\tindent()\n\t\t}\n\t\tif nesting == 0 && last == '}' && beginningOfLine {\n\t\t\tnextLine()\n\t\t\tnextLine()\n\t\t}\n\n\t\tif !beginningOfLine && spacePrior {\n\t\t\twrite(' ')\n\t\t}\n\n\t\tif openBrace && !openBraceWritten {\n\t\t\twrite('{')\n\t\t\topenBraceWritten = true\n\t\t}\n\n\t\tif spacePrior && ch == '<' {\n\t\t\tspace = true\n\t\t}\n\n\t\twrite(ch)\n\n\t\tbeginningOfLine = false\n\t}\n\n\t// the Caddyfile does not need any leading or trailing spaces, but...\n\ttrimmedResult := bytes.TrimSpace(out.Bytes())\n\n\t// ...Caddyfiles should, however, end with a newline because\n\t// newlines are significant to the syntax of the file\n\treturn append(trimmedResult, '\\n')\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:48:09"}, {"role": "assistant", "content": "```go\n// Copyright 2015 Matthew Holt and The Caddy Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage caddyfile\n\nimport (\n\t\"strings\"\n\t\"testing\"\n)\n\nfunc TestFormatter(t *testing.T) {\n\tfor i, tc := range []struct {\n\t\tdescription string\n\t\tinput       string\n\t\texpect      string\n\t}{\n\t\t{\n\t\t\tdescription: \"very simple\",\n\t\t\tinput: `abc   def\n\tg hi jkl\nmn`,\n\t\t\texpect: `abc def\ng hi jkl\nmn`,\n\t\t},\n\t\t{\n\t\t\tdescription: \"basic indentation, line breaks, and nesting\",\n\t\t\tinput: `  a\nb\n\n\tc {\n\t\td\n}\n\ne { f\n}\n\n\n\ng {\nh {\ni\n}\n}\n\nj { k {\nl\n}\n}\n\nm {\n\tn { o\n\t}\n\tp { q r\ns }\n}\n\n\t{\n{ t\n\t\tu\n\n\tv\n\nw\n}\n}`,\n\t\t\texpect: `a\nb\n\nc {\n\td\n}\n\ne {\n\tf\n}\n\ng {\n\th {\n\t\ti\n\t}\n}\n\nj {\n\tk {\n\t\tl\n\t}\n}\n\nm {\n\tn {\n\t\to\n\t}\n\tp {\n\t\tq r\n\t\ts\n\t}\n}\n\n{\n\t{\n\t\tt\n\t\tu\n\n\t\tv\n\n\t\tw\n\t}\n}`,\n\t\t},\n\t\t{\n\t\t\tdescription: \"block spacing\",\n\t\t\tinput: `a{\n\tb\n}\n\nc{ d\n}`,\n\t\t\texpect: `a {\n\tb\n}\n\nc {\n\td\n}`,\n\t\t},\n\t\t{\n\t\t\tdescription: \"advanced spacing\",\n\t\t\tinput: `abc {\n\tdef\n}ghi{\n\tjkl mno\npqr}`,\n\t\t\texpect: `abc {\n\tdef\n}\n\nghi {\n\tjkl mno\n\tpqr\n}`,\n\t\t},\n\t\t{\n\t\t\tdescription: \"env var placeholders\",\n\t\t\tinput: `{$A}\n\nb {\n{$C}\n}\n\nd { {$E}\n}\n\n{ {$F}\n}\n`,\n\t\t\texpect: `{$A}\n\nb {\n\t{$C}\n}\n\nd {\n\t{$E}\n}\n\n{\n\t{$F}\n}`,\n\t\t},\n\t\t{\n\t\t\tdescription: \"env var placeholders with port\",\n\t\t\tinput:       `:{$PORT}`,\n\t\t\texpect:      `:{$PORT}`,\n\t\t},\n\t\t{\n\t\t\tdescription: \"comments\",\n\t\t\tinput: `#a \"\\n\"\n\n #b {\n\tc\n}\n\nd {\ne#f\n# g\n}\n\nh { # i\n}`,\n\t\t\texpect: `#a \"\\n\"\n\n#b {\nc\n}\n\nd {\n\te#f\n\t# g\n}\n\nh {\n\t# i\n}`,\n\t\t},\n\t\t{\n\t\t\tdescription: \"quotes and escaping\",\n\t\t\tinput: `\"a \\\"b\\\" \"#c\n\td\n\ne {\n\"f\"\n}\n\ng { \"h\"\n}\n\ni {\n\t\"foo\nbar\"\n}\n\nj {\n\"\\\"k\\\" l m\"\n}`,\n\t\t\texpect: `\"a \\\"b\\\" \"#c\nd\n\ne {\n\t\"f\"\n}\n\ng {\n\t\"h\"\n}\n\ni {\n\t\"foo\nbar\"\n}\n\nj {\n\t\"\\\"k\\\" l m\"\n}`,\n\t\t},\n\t\t{\n\t\t\tdescription: \"bad nesting (too many open)\",\n\t\t\tinput: `a\n{\n\t{\n}`,\n\t\t\texpect: `a {\n\t{\n\t}\n`,\n\t\t},\n\t\t{\n\t\t\tdescription: \"bad nesting (too many close)\",\n\t\t\tinput: `a\n{\n\t{\n}}}`,\n\t\t\texpect: `a {\n\t{\n\t}\n}\n}\n`,\n\t\t},\n\t\t{\n\t\t\tdescription: \"json\",\n\t\t\tinput: `foo\nbar      \"{\\\"key\\\":34}\"\n`,\n\t\t\texpect: `foo\nbar \"{\\\"key\\\":34}\"`,\n\t\t},\n\t\t{\n\t\t\tdescription: \"escaping after spaces\",\n\t\t\tinput:       `foo \\\"literal\\\"`,\n\t\t\texpect:      `foo \\\"literal\\\"`,\n\t\t},\n\t\t{\n\t\t\tdescription: \"simple placeholders as standalone tokens\",\n\t\t\tinput:       `foo {bar}`,\n\t\t\texpect:      `foo {bar}`,\n\t\t},\n\t\t{\n\t\t\tdescription: \"simple placeholders within tokens\",\n\t\t\tinput:       `foo{bar} foo{bar}baz`,\n\t\t\texpect:      `foo{bar} foo{bar}baz`,\n\t\t},\n\t\t{\n\t\t\tdescription: \"placeholders and malformed braces\",\n\t\t\tinput:       `foo{bar} foo{ bar}baz`,\n\t\t\texpect: `foo{bar} foo {\n\tbar\n}\n\nbaz`,\n\t\t},\n\t\t{\n\t\t\tdescription: \"hash within string is not a comment\",\n\t\t\tinput:       `redir / /some/#/path`,\n\t\t\texpect:      `redir / /some/#/path`,\n\t\t},\n\t\t{\n\t\t\tdescription: \"brace does not fold into comment above\",\n\t\t\tinput: `# comment\n{\n\tfoo\n}`,\n\t\t\texpect: `# comment\n{\n\tfoo\n}`,\n\t\t},\n\t\t{\n\t\t\tdescription: \"matthewpi/vscode-caddyfile-support#13\",\n\t\t\tinput: `{\n\temail {$ACMEEMAIL}\n\t#debug\n}\n\nblock {\n}\n`,\n\t\t\texpect: `{\n\temail {$ACMEEMAIL}\n\t#debug\n}\n\nblock {\n}\n`,\n\t\t},\n\t\t{\n\t\t\tdescription: \"matthewpi/vscode-caddyfile-support#13 - bad formatting\",\n\t\t\tinput: `{\n\temail {$ACMEEMAIL}\n\t#debug\n\t}\n\n\tblock {\n\t}\n`,\n\t\t\texpect: `{\n\temail {$ACMEEMAIL}\n\t#debug\n}\n\nblock {\n}\n`,\n\t\t},\n\t\t{\n\t\t\tdescription: \"keep heredoc as-is\",\n\t\t\tinput: `block {\n\theredoc <<HEREDOC\n\tHere's more than one space       Here's more than one space\n\tHEREDOC\n}\n`,\n\t\t\texpect: `block {\n\theredoc <<HEREDOC\n\tHere's more than one space       Here's more than one space\n\tHEREDOC\n}\n`,\n\t\t},\n\t\t{\n\t\t\tdescription: \"Mixing heredoc with regular part\",\n\t\t\tinput: `block {\n\theredoc <<HEREDOC\n\tHere's more than one space       Here's more than one space\n\tHEREDOC\n\trespond \"More than one space will be eaten\"     200\n}\n\nblock2 {\n\theredoc <<HEREDOC\n\tHere's more than one space       Here's more than one space\n\tHEREDOC\n\trespond \"More than one space will be eaten\" 200\n}\n`,\n\t\t\texpect: `block {\n\theredoc <<HEREDOC\n\tHere's more than one space       Here's more than one space\n\tHEREDOC\n\trespond \"More than one space will be eaten\" 200\n}\n\nblock2 {\n\theredoc <<HEREDOC\n\tHere's more than one space       Here's more than one space\n\tHEREDOC\n\trespond \"More than one space will be eaten\" 200\n}\n`,\n\t\t},\n\t\t{\n\t\t\tdescription: \"Heredoc as regular token\",\n\t\t\tinput: `block {\n\theredoc <<HEREDOC                                 \"More than one space will be eaten\"\n}\n`,\n\t\t\texpect: `block {\n\theredoc <<HEREDOC \"More than one space will be eaten\"\n}\n`,\n\t\t},\n\t\t{\n\t\t\tdescription: \"Escape heredoc\",\n\t\t\tinput: `block {\n\theredoc \\<<HEREDOC\n\trespond \"More than one space will be eaten\"                           200\n}\n`,\n\t\t\texpect: `block {\n\theredoc \\<<HEREDOC\n\trespond \"More than one space will be eaten\" 200\n}\n`,\n\t\t},\n\t} {\n\t\t// the formatter should output a trailing newline,\n\t\t// even if the tests aren't written to expect that\n\t\tif !strings.HasSuffix(tc.expect, \"\\n\") {\n\t\t\ttc.expect += \"\\n\"\n\t\t}\n\n\t\tactual := Format([]byte(tc.input))\n\n\t\tif string(actual) != tc.expect {\n\t\t\tt.Errorf(\"\\n[TEST %d: %s]\\n====== EXPECTED ======\\n%s\\n====== ACTUAL ======\\n%s^^^^^^^^^^^^^^^^^^^^^\",\n\t\t\t\ti, tc.description, string(tc.expect), string(actual))\n\t\t}\n\t}\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: caddy\nSource File Path(s): ['caddy\\\\lexer_test.go\\\\source_files\\\\lexer.go']\nTest File Path: caddy\\lexer_test.go\\lexer_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: caddy\\lexer_test.go\\source_files\\lexer.go\n// Copyright 2015 Matthew Holt and The Caddy Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage caddyfile\n\nimport (\n\t\"bufio\"\n\t\"bytes\"\n\t\"fmt\"\n\t\"io\"\n\t\"regexp\"\n\t\"strings\"\n\t\"unicode\"\n)\n\ntype (\n\t// lexer is a utility which can get values, token by\n\t// token, from a Reader. A token is a word, and tokens\n\t// are separated by whitespace. A word can be enclosed\n\t// in quotes if it contains whitespace.\n\tlexer struct {\n\t\treader       *bufio.Reader\n\t\ttoken        Token\n\t\tline         int\n\t\tskippedLines int\n\t}\n\n\t// Token represents a single parsable unit.\n\tToken struct {\n\t\tFile          string\n\t\timports       []string\n\t\tLine          int\n\t\tText          string\n\t\twasQuoted     rune // enclosing quote character, if any\n\t\theredocMarker string\n\t\tsnippetName   string\n\t}\n)\n\n// Tokenize takes bytes as input and lexes it into\n// a list of tokens that can be parsed as a Caddyfile.\n// Also takes a filename to fill the token's File as\n// the source of the tokens, which is important to\n// determine relative paths for `import` directives.\nfunc Tokenize(input []byte, filename string) ([]Token, error) {\n\tl := lexer{}\n\tif err := l.load(bytes.NewReader(input)); err != nil {\n\t\treturn nil, err\n\t}\n\tvar tokens []Token\n\tfor {\n\t\tfound, err := l.next()\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif !found {\n\t\t\tbreak\n\t\t}\n\t\tl.token.File = filename\n\t\ttokens = append(tokens, l.token)\n\t}\n\treturn tokens, nil\n}\n\n// load prepares the lexer to scan an input for tokens.\n// It discards any leading byte order mark.\nfunc (l *lexer) load(input io.Reader) error {\n\tl.reader = bufio.NewReader(input)\n\tl.line = 1\n\n\t// discard byte order mark, if present\n\tfirstCh, _, err := l.reader.ReadRune()\n\tif err != nil {\n\t\treturn err\n\t}\n\tif firstCh != 0xFEFF {\n\t\terr := l.reader.UnreadRune()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// next loads the next token into the lexer.\n// A token is delimited by whitespace, unless\n// the token starts with a quotes character (\")\n// in which case the token goes until the closing\n// quotes (the enclosing quotes are not included).\n// Inside quoted strings, quotes may be escaped\n// with a preceding \\ character. No other chars\n// may be escaped. The rest of the line is skipped\n// if a \"#\" character is read in. Returns true if\n// a token was loaded; false otherwise.\nfunc (l *lexer) next() (bool, error) {\n\tvar val []rune\n\tvar comment, quoted, btQuoted, inHeredoc, heredocEscaped, escaped bool\n\tvar heredocMarker string\n\n\tmakeToken := func(quoted rune) bool {\n\t\tl.token.Text = string(val)\n\t\tl.token.wasQuoted = quoted\n\t\tl.token.heredocMarker = heredocMarker\n\t\treturn true\n\t}\n\n\tfor {\n\t\t// Read a character in; if err then if we had\n\t\t// read some characters, make a token. If we\n\t\t// reached EOF, then no more tokens to read.\n\t\t// If no EOF, then we had a problem.\n\t\tch, _, err := l.reader.ReadRune()\n\t\tif err != nil {\n\t\t\tif len(val) > 0 {\n\t\t\t\tif inHeredoc {\n\t\t\t\t\treturn false, fmt.Errorf(\"incomplete heredoc <<%s on line #%d, expected ending marker %s\", heredocMarker, l.line+l.skippedLines, heredocMarker)\n\t\t\t\t}\n\n\t\t\t\treturn makeToken(0), nil\n\t\t\t}\n\t\t\tif err == io.EOF {\n\t\t\t\treturn false, nil\n\t\t\t}\n\t\t\treturn false, err\n\t\t}\n\n\t\t// detect whether we have the start of a heredoc\n\t\tif !(quoted || btQuoted) && !(inHeredoc || heredocEscaped) &&\n\t\t\tlen(val) > 1 && string(val[:2]) == \"<<\" {\n\t\t\t// a space means it's just a regular token and not a heredoc\n\t\t\tif ch == ' ' {\n\t\t\t\treturn makeToken(0), nil\n\t\t\t}\n\n\t\t\t// skip CR, we only care about LF\n\t\t\tif ch == '\\r' {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// after hitting a newline, we know that the heredoc marker\n\t\t\t// is the characters after the two << and the newline.\n\t\t\t// we reset the val because the heredoc is syntax we don't\n\t\t\t// want to keep.\n\t\t\tif ch == '\\n' {\n\t\t\t\tif len(val) == 2 {\n\t\t\t\t\treturn false, fmt.Errorf(\"missing opening heredoc marker on line #%d; must contain only alpha-numeric characters, dashes and underscores; got empty string\", l.line)\n\t\t\t\t}\n\n\t\t\t\t// check if there's too many <\n\t\t\t\tif string(val[:3]) == \"<<<\" {\n\t\t\t\t\treturn false, fmt.Errorf(\"too many '<' for heredoc on line #%d; only use two, for example <<END\", l.line)\n\t\t\t\t}\n\n\t\t\t\theredocMarker = string(val[2:])\n\t\t\t\tif !heredocMarkerRegexp.Match([]byte(heredocMarker)) {\n\t\t\t\t\treturn false, fmt.Errorf(\"heredoc marker on line #%d must contain only alpha-numeric characters, dashes and underscores; got '%s'\", l.line, heredocMarker)\n\t\t\t\t}\n\n\t\t\t\tinHeredoc = true\n\t\t\t\tl.skippedLines++\n\t\t\t\tval = nil\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tval = append(val, ch)\n\t\t\tcontinue\n\t\t}\n\n\t\t// if we're in a heredoc, all characters are read as-is\n\t\tif inHeredoc {\n\t\t\tval = append(val, ch)\n\n\t\t\tif ch == '\\n' {\n\t\t\t\tl.skippedLines++\n\t\t\t}\n\n\t\t\t// check if we're done, i.e. that the last few characters are the marker\n\t\t\tif len(val) >= len(heredocMarker) && heredocMarker == string(val[len(val)-len(heredocMarker):]) {\n\t\t\t\t// set the final value\n\t\t\t\tval, err = l.finalizeHeredoc(val, heredocMarker)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn false, err\n\t\t\t\t}\n\n\t\t\t\t// set the line counter, and make the token\n\t\t\t\tl.line += l.skippedLines\n\t\t\t\tl.skippedLines = 0\n\t\t\t\treturn makeToken('<'), nil\n\t\t\t}\n\n\t\t\t// stay in the heredoc until we find the ending marker\n\t\t\tcontinue\n\t\t}\n\n\t\t// track whether we found an escape '\\' for the next\n\t\t// iteration to be contextually aware\n\t\tif !escaped && !btQuoted && ch == '\\\\' {\n\t\t\tescaped = true\n\t\t\tcontinue\n\t\t}\n\n\t\tif quoted || btQuoted {\n\t\t\tif quoted && escaped {\n\t\t\t\t// all is literal in quoted area,\n\t\t\t\t// so only escape quotes\n\t\t\t\tif ch != '\"' {\n\t\t\t\t\tval = append(val, '\\\\')\n\t\t\t\t}\n\t\t\t\tescaped = false\n\t\t\t} else {\n\t\t\t\tif (quoted && ch == '\"') || (btQuoted && ch == '`') {\n\t\t\t\t\treturn makeToken(ch), nil\n\t\t\t\t}\n\t\t\t}\n\t\t\t// allow quoted text to wrap continue on multiple lines\n\t\t\tif ch == '\\n' {\n\t\t\t\tl.line += 1 + l.skippedLines\n\t\t\t\tl.skippedLines = 0\n\t\t\t}\n\t\t\t// collect this character as part of the quoted token\n\t\t\tval = append(val, ch)\n\t\t\tcontinue\n\t\t}\n\n\t\tif unicode.IsSpace(ch) {\n\t\t\t// ignore CR altogether, we only actually care about LF (\\n)\n\t\t\tif ch == '\\r' {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// end of the line\n\t\t\tif ch == '\\n' {\n\t\t\t\t// newlines can be escaped to chain arguments\n\t\t\t\t// onto multiple lines; else, increment the line count\n\t\t\t\tif escaped {\n\t\t\t\t\tl.skippedLines++\n\t\t\t\t\tescaped = false\n\t\t\t\t} else {\n\t\t\t\t\tl.line += 1 + l.skippedLines\n\t\t\t\t\tl.skippedLines = 0\n\t\t\t\t}\n\t\t\t\t// comments (#) are single-line only\n\t\t\t\tcomment = false\n\t\t\t}\n\t\t\t// any kind of space means we're at the end of this token\n\t\t\tif len(val) > 0 {\n\t\t\t\treturn makeToken(0), nil\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\t// comments must be at the start of a token,\n\t\t// in other words, preceded by space or newline\n\t\tif ch == '#' && len(val) == 0 {\n\t\t\tcomment = true\n\t\t}\n\t\tif comment {\n\t\t\tcontinue\n\t\t}\n\n\t\tif len(val) == 0 {\n\t\t\tl.token = Token{Line: l.line}\n\t\t\tif ch == '\"' {\n\t\t\t\tquoted = true\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif ch == '`' {\n\t\t\t\tbtQuoted = true\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\tif escaped {\n\t\t\t// allow escaping the first < to skip the heredoc syntax\n\t\t\tif ch == '<' {\n\t\t\t\theredocEscaped = true\n\t\t\t} else {\n\t\t\t\tval = append(val, '\\\\')\n\t\t\t}\n\t\t\tescaped = false\n\t\t}\n\n\t\tval = append(val, ch)\n\t}\n}\n\n// finalizeHeredoc takes the runes read as the heredoc text and the marker,\n// and processes the text to strip leading whitespace, returning the final\n// value without the leading whitespace.\nfunc (l *lexer) finalizeHeredoc(val []rune, marker string) ([]rune, error) {\n\tstringVal := string(val)\n\n\t// find the last newline of the heredoc, which is where the contents end\n\tlastNewline := strings.LastIndex(stringVal, \"\\n\")\n\n\t// collapse the content, then split into separate lines\n\tlines := strings.Split(stringVal[:lastNewline+1], \"\\n\")\n\n\t// figure out how much whitespace we need to strip from the front of every line\n\t// by getting the string that precedes the marker, on the last line\n\tpaddingToStrip := stringVal[lastNewline+1 : len(stringVal)-len(marker)]\n\n\t// iterate over each line and strip the whitespace from the front\n\tvar out string\n\tfor lineNum, lineText := range lines[:len(lines)-1] {\n\t\tif lineText == \"\" || lineText == \"\\r\" {\n\t\t\tout += \"\\n\"\n\t\t\tcontinue\n\t\t}\n\n\t\t// find an exact match for the padding\n\t\tindex := strings.Index(lineText, paddingToStrip)\n\n\t\t// if the padding doesn't match exactly at the start then we can't safely strip\n\t\tif index != 0 {\n\t\t\treturn nil, fmt.Errorf(\"mismatched leading whitespace in heredoc <<%s on line #%d [%s], expected whitespace [%s] to match the closing marker\", marker, l.line+lineNum+1, lineText, paddingToStrip)\n\t\t}\n\n\t\t// strip, then append the line, with the newline, to the output.\n\t\t// also removes all \"\\r\" because Windows.\n\t\tout += strings.ReplaceAll(lineText[len(paddingToStrip):]+\"\\n\", \"\\r\", \"\")\n\t}\n\n\t// Remove the trailing newline from the loop\n\tif len(out) > 0 && out[len(out)-1] == '\\n' {\n\t\tout = out[:len(out)-1]\n\t}\n\n\t// return the final value\n\treturn []rune(out), nil\n}\n\n// Quoted returns true if the token was enclosed in quotes\n// (i.e. double quotes, backticks, or heredoc).\nfunc (t Token) Quoted() bool {\n\treturn t.wasQuoted > 0\n}\n\n// NumLineBreaks counts how many line breaks are in the token text.\nfunc (t Token) NumLineBreaks() int {\n\tlineBreaks := strings.Count(t.Text, \"\\n\")\n\tif t.wasQuoted == '<' {\n\t\t// heredocs have an extra linebreak because the opening\n\t\t// delimiter is on its own line and is not included in the\n\t\t// token Text itself, and the trailing newline is removed.\n\t\tlineBreaks += 2\n\t}\n\treturn lineBreaks\n}\n\n// Clone returns a deep copy of the token.\nfunc (t Token) Clone() Token {\n\treturn Token{\n\t\tFile:          t.File,\n\t\timports:       append([]string{}, t.imports...),\n\t\tLine:          t.Line,\n\t\tText:          t.Text,\n\t\twasQuoted:     t.wasQuoted,\n\t\theredocMarker: t.heredocMarker,\n\t\tsnippetName:   t.snippetName,\n\t}\n}\n\nvar heredocMarkerRegexp = regexp.MustCompile(\"^[A-Za-z0-9_-]+$\")\n\n// isNextOnNewLine tests whether t2 is on a different line from t1\nfunc isNextOnNewLine(t1, t2 Token) bool {\n\t// If the second token is from a different file,\n\t// we can assume it's from a different line\n\tif t1.File != t2.File {\n\t\treturn true\n\t}\n\n\t// If the second token is from a different import chain,\n\t// we can assume it's from a different line\n\tif len(t1.imports) != len(t2.imports) {\n\t\treturn true\n\t}\n\tfor i, im := range t1.imports {\n\t\tif im != t2.imports[i] {\n\t\t\treturn true\n\t\t}\n\t}\n\n\t// If the first token (incl line breaks) ends\n\t// on a line earlier than the next token,\n\t// then the second token is on a new line\n\treturn t1.Line+t1.NumLineBreaks() < t2.Line\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:48:09"}, {"role": "assistant", "content": "```go\n// Copyright 2015 Matthew Holt and The Caddy Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage caddyfile\n\nimport (\n\t\"testing\"\n)\n\nfunc TestLexer(t *testing.T) {\n\ttestCases := []struct {\n\t\tinput        []byte\n\t\texpected     []Token\n\t\texpectErr    bool\n\t\terrorMessage string\n\t}{\n\t\t{\n\t\t\tinput: []byte(`host:123`),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: \"host:123\"},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(`host:123\n\n\t\t\t\t\tdirective`),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: \"host:123\"},\n\t\t\t\t{Line: 3, Text: \"directive\"},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(`host:123 {\n\t\t\t\t\t\tdirective\n\t\t\t\t\t}`),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: \"host:123\"},\n\t\t\t\t{Line: 1, Text: \"{\"},\n\t\t\t\t{Line: 2, Text: \"directive\"},\n\t\t\t\t{Line: 3, Text: \"}\"},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(`host:123 { directive }`),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: \"host:123\"},\n\t\t\t\t{Line: 1, Text: \"{\"},\n\t\t\t\t{Line: 1, Text: \"directive\"},\n\t\t\t\t{Line: 1, Text: \"}\"},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(`host:123 {\n\t\t\t\t\t\t#comment\n\t\t\t\t\t\tdirective\n\t\t\t\t\t\t# comment\n\t\t\t\t\t\tfoobar # another comment\n\t\t\t\t\t}`),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: \"host:123\"},\n\t\t\t\t{Line: 1, Text: \"{\"},\n\t\t\t\t{Line: 3, Text: \"directive\"},\n\t\t\t\t{Line: 5, Text: \"foobar\"},\n\t\t\t\t{Line: 6, Text: \"}\"},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(`host:123 {\n\t\t\t\t\t\t# hash inside string is not a comment\n\t\t\t\t\t\tredir / /some/#/path\n\t\t\t\t\t}`),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: \"host:123\"},\n\t\t\t\t{Line: 1, Text: \"{\"},\n\t\t\t\t{Line: 3, Text: \"redir\"},\n\t\t\t\t{Line: 3, Text: \"/\"},\n\t\t\t\t{Line: 3, Text: \"/some/#/path\"},\n\t\t\t\t{Line: 4, Text: \"}\"},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(\"# comment at beginning of file\\n# comment at beginning of line\\nhost:123\"),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 3, Text: \"host:123\"},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(`a \"quoted value\" b\n\t\t\t\t\tfoobar`),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: \"a\"},\n\t\t\t\t{Line: 1, Text: \"quoted value\"},\n\t\t\t\t{Line: 1, Text: \"b\"},\n\t\t\t\t{Line: 2, Text: \"foobar\"},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(`A \"quoted \\\"value\\\" inside\" B`),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: \"A\"},\n\t\t\t\t{Line: 1, Text: `quoted \"value\" inside`},\n\t\t\t\t{Line: 1, Text: \"B\"},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(\"An escaped \\\"newline\\\\\\ninside\\\" quotes\"),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: \"An\"},\n\t\t\t\t{Line: 1, Text: \"escaped\"},\n\t\t\t\t{Line: 1, Text: \"newline\\\\\\ninside\"},\n\t\t\t\t{Line: 2, Text: \"quotes\"},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(\"An escaped newline\\\\\\noutside quotes\"),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: \"An\"},\n\t\t\t\t{Line: 1, Text: \"escaped\"},\n\t\t\t\t{Line: 1, Text: \"newline\"},\n\t\t\t\t{Line: 1, Text: \"outside\"},\n\t\t\t\t{Line: 1, Text: \"quotes\"},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(\"line1\\\\\\nescaped\\nline2\\nline3\"),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: \"line1\"},\n\t\t\t\t{Line: 1, Text: \"escaped\"},\n\t\t\t\t{Line: 3, Text: \"line2\"},\n\t\t\t\t{Line: 4, Text: \"line3\"},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(\"line1\\\\\\nescaped1\\\\\\nescaped2\\nline4\\nline5\"),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: \"line1\"},\n\t\t\t\t{Line: 1, Text: \"escaped1\"},\n\t\t\t\t{Line: 1, Text: \"escaped2\"},\n\t\t\t\t{Line: 4, Text: \"line4\"},\n\t\t\t\t{Line: 5, Text: \"line5\"},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(`\"unescapable\\ in quotes\"`),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: `unescapable\\ in quotes`},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(`\"don't\\escape\"`),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: `don't\\escape`},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(`\"don't\\\\escape\"`),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: `don't\\\\escape`},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(`un\\escapable`),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: `un\\escapable`},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(`A \"quoted value with line\n\t\t\t\t\tbreak inside\" {\n\t\t\t\t\t\tfoobar\n\t\t\t\t\t}`),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: \"A\"},\n\t\t\t\t{Line: 1, Text: \"quoted value with line\\n\\t\\t\\t\\t\\tbreak inside\"},\n\t\t\t\t{Line: 2, Text: \"{\"},\n\t\t\t\t{Line: 3, Text: \"foobar\"},\n\t\t\t\t{Line: 4, Text: \"}\"},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(`\"C:\\php\\php-cgi.exe\"`),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: `C:\\php\\php-cgi.exe`},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(`empty \"\" string`),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: `empty`},\n\t\t\t\t{Line: 1, Text: ``},\n\t\t\t\t{Line: 1, Text: `string`},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(\"skip those\\r\\nCR characters\"),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: \"skip\"},\n\t\t\t\t{Line: 1, Text: \"those\"},\n\t\t\t\t{Line: 2, Text: \"CR\"},\n\t\t\t\t{Line: 2, Text: \"characters\"},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(\"\\xEF\\xBB\\xBF:8080\"), // test with leading byte order mark\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: \":8080\"},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(\"simple `backtick quoted` string\"),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: `simple`},\n\t\t\t\t{Line: 1, Text: `backtick quoted`},\n\t\t\t\t{Line: 1, Text: `string`},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(\"multiline `backtick\\nquoted\\n` string\"),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: `multiline`},\n\t\t\t\t{Line: 1, Text: \"backtick\\nquoted\\n\"},\n\t\t\t\t{Line: 3, Text: `string`},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(\"nested `\\\"quotes inside\\\" backticks` string\"),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: `nested`},\n\t\t\t\t{Line: 1, Text: `\"quotes inside\" backticks`},\n\t\t\t\t{Line: 1, Text: `string`},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(\"reverse-nested \\\"`backticks` inside\\\" quotes\"),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: `reverse-nested`},\n\t\t\t\t{Line: 1, Text: \"`backticks` inside\"},\n\t\t\t\t{Line: 1, Text: `quotes`},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(`heredoc <<EOF\ncontent\nEOF same-line-arg\n\t`),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: `heredoc`},\n\t\t\t\t{Line: 1, Text: \"content\"},\n\t\t\t\t{Line: 3, Text: `same-line-arg`},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(`heredoc <<VERY-LONG-MARKER\ncontent\nVERY-LONG-MARKER same-line-arg\n\t`),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: `heredoc`},\n\t\t\t\t{Line: 1, Text: \"content\"},\n\t\t\t\t{Line: 3, Text: `same-line-arg`},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(`heredoc <<EOF\nextra-newline\n\nEOF same-line-arg\n\t`),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: `heredoc`},\n\t\t\t\t{Line: 1, Text: \"extra-newline\\n\"},\n\t\t\t\t{Line: 4, Text: `same-line-arg`},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(`heredoc <<EOF\nEOF\n\tHERE same-line-arg\n\t`),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: `heredoc`},\n\t\t\t\t{Line: 1, Text: ``},\n\t\t\t\t{Line: 3, Text: `HERE`},\n\t\t\t\t{Line: 3, Text: `same-line-arg`},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(`heredoc <<EOF\n\t\tEOF same-line-arg\n\t`),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: `heredoc`},\n\t\t\t\t{Line: 1, Text: \"\"},\n\t\t\t\t{Line: 2, Text: `same-line-arg`},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(`heredoc <<EOF\n\tcontent\n\tEOF same-line-arg\n\t`),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: `heredoc`},\n\t\t\t\t{Line: 1, Text: \"content\"},\n\t\t\t\t{Line: 3, Text: `same-line-arg`},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(`prev-line\n\theredoc <<EOF\n\t\tmulti\n\t\tline\n\t\tcontent\n\tEOF same-line-arg\n\tnext-line\n\t`),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: `prev-line`},\n\t\t\t\t{Line: 2, Text: `heredoc`},\n\t\t\t\t{Line: 2, Text: \"\\tmulti\\n\\tline\\n\\tcontent\"},\n\t\t\t\t{Line: 6, Text: `same-line-arg`},\n\t\t\t\t{Line: 7, Text: `next-line`},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(`escaped-heredoc \\<< >>`),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: `escaped-heredoc`},\n\t\t\t\t{Line: 1, Text: `<<`},\n\t\t\t\t{Line: 1, Text: `>>`},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(`not-a-heredoc <EOF\n\tcontent\n\t`),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: `not-a-heredoc`},\n\t\t\t\t{Line: 1, Text: `<EOF`},\n\t\t\t\t{Line: 2, Text: `content`},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(`not-a-heredoc <<<EOF content`),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: `not-a-heredoc`},\n\t\t\t\t{Line: 1, Text: `<<<EOF`},\n\t\t\t\t{Line: 1, Text: `content`},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(`not-a-heredoc \"<<\" \">>\"`),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: `not-a-heredoc`},\n\t\t\t\t{Line: 1, Text: `<<`},\n\t\t\t\t{Line: 1, Text: `>>`},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(`not-a-heredoc << >>`),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: `not-a-heredoc`},\n\t\t\t\t{Line: 1, Text: `<<`},\n\t\t\t\t{Line: 1, Text: `>>`},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(`not-a-heredoc <<HERE SAME LINE\n\tcontent\n\tHERE same-line-arg\n\t`),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: `not-a-heredoc`},\n\t\t\t\t{Line: 1, Text: `<<HERE`},\n\t\t\t\t{Line: 1, Text: `SAME`},\n\t\t\t\t{Line: 1, Text: `LINE`},\n\t\t\t\t{Line: 2, Text: `content`},\n\t\t\t\t{Line: 3, Text: `HERE`},\n\t\t\t\t{Line: 3, Text: `same-line-arg`},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(`heredoc <<s\n\t\t\t\ufffd\n\t\t\ts\n\t`),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: `heredoc`},\n\t\t\t\t{Line: 1, Text: \"\ufffd\"},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(\"\\u000Aheredoc \\u003C\\u003C\\u0073\\u0073\\u000A\\u00BF\\u0057\\u0001\\u0000\\u00FF\\u00FF\\u00FF\\u00FF\\u00FF\\u00FF\\u00FF\\u003D\\u001F\\u000A\\u0073\\u0073\\u000A\\u00BF\\u0057\\u0001\\u0000\\u00FF\\u00FF\\u00FF\\u00FF\\u00FF\\u00FF\\u00FF\\u003D\\u001F\\u000A\\u00BF\\u00BF\\u0057\\u0001\\u0000\\u00FF\\u00FF\\u00FF\\u00FF\\u00FF\\u00FF\\u00FF\\u003D\\u001F\"),\n\t\t\texpected: []Token{\n\t\t\t\t{\n\t\t\t\t\tLine: 2,\n\t\t\t\t\tText: \"heredoc\",\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tLine: 2,\n\t\t\t\t\tText: \"\\u00BF\\u0057\\u0001\\u0000\\u00FF\\u00FF\\u00FF\\u00FF\\u00FF\\u00FF\\u00FF\\u003D\\u001F\",\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tLine: 5,\n\t\t\t\t\tText: \"\\u00BF\\u0057\\u0001\\u0000\\u00FF\\u00FF\\u00FF\\u00FF\\u00FF\\u00FF\\u00FF\\u003D\\u001F\",\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\tLine: 6,\n\t\t\t\t\tText: \"\\u00BF\\u00BF\\u0057\\u0001\\u0000\\u00FF\\u00FF\\u00FF\\u00FF\\u00FF\\u00FF\\u00FF\\u003D\\u001F\",\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput:        []byte(\"not-a-heredoc <<\\n\"),\n\t\t\texpectErr:    true,\n\t\t\terrorMessage: \"missing opening heredoc marker on line #1; must contain only alpha-numeric characters, dashes and underscores; got empty string\",\n\t\t},\n\t\t{\n\t\t\tinput: []byte(`heredoc <<<EOF\n\tcontent\n\tEOF same-line-arg\n\t`),\n\t\t\texpectErr:    true,\n\t\t\terrorMessage: \"too many '<' for heredoc on line #1; only use two, for example <<END\",\n\t\t},\n\t\t{\n\t\t\tinput: []byte(`heredoc <<EOF\n\tcontent\n\t`),\n\t\t\texpectErr:    true,\n\t\t\terrorMessage: \"incomplete heredoc <<EOF on line #3, expected ending marker EOF\",\n\t\t},\n\t\t{\n\t\t\tinput: []byte(`heredoc <<EOF\n\tcontent\n\t\tEOF\n\t`),\n\t\t\texpectErr:    true,\n\t\t\terrorMessage: \"mismatched leading whitespace in heredoc <<EOF on line #2 [\\tcontent], expected whitespace [\\t\\t] to match the closing marker\",\n\t\t},\n\t\t{\n\t\t\tinput: []byte(`heredoc <<EOF\n        content\n\t\tEOF\n\t`),\n\t\t\texpectErr:    true,\n\t\t\terrorMessage: \"mismatched leading whitespace in heredoc <<EOF on line #2 [        content], expected whitespace [\\t\\t] to match the closing marker\",\n\t\t},\n\t\t{\n\t\t\tinput: []byte(`heredoc <<EOF\nThe next line is a blank line\n\nThe previous line is a blank line\nEOF`),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: \"heredoc\"},\n\t\t\t\t{Line: 1, Text: \"The next line is a blank line\\n\\nThe previous line is a blank line\"},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(`heredoc <<EOF\n\tOne tab indented heredoc with blank next line\n\n\tOne tab indented heredoc with blank previous line\n\tEOF`),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: \"heredoc\"},\n\t\t\t\t{Line: 1, Text: \"One tab indented heredoc with blank next line\\n\\nOne tab indented heredoc with blank previous line\"},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(`heredoc <<EOF\nThe next line is a blank line with one tab\n\t\nThe previous line is a blank line with one tab\nEOF`),\n\t\t\texpected: []Token{\n\t\t\t\t{Line: 1, Text: \"heredoc\"},\n\t\t\t\t{Line: 1, Text: \"The next line is a blank line with one tab\\n\\t\\nThe previous line is a blank line with one tab\"},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: []byte(`heredoc <<EOF\n\t\tThe next line is a blank line with one tab less than the correct indentation\n\t\n\t\tThe previous line is a blank line with one tab less than the correct indentation\n\t\tEOF`),\n\t\t\texpectErr:    true,\n\t\t\terrorMessage: \"mismatched leading whitespace in heredoc <<EOF on line #3 [\\t], expected whitespace [\\t\\t] to match the closing marker\",\n\t\t},\n\t}\n\n\tfor i, testCase := range testCases {\n\t\tactual, err := Tokenize(testCase.input, \"\")\n\t\tif testCase.expectErr {\n\t\t\tif err == nil {\n\t\t\t\tt.Fatalf(\"expected error, got actual: %v\", actual)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif err.Error() != testCase.errorMessage {\n\t\t\t\tt.Fatalf(\"expected error '%v', got: %v\", testCase.errorMessage, err)\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"%v\", err)\n\t\t}\n\t\tlexerCompare(t, i, testCase.expected, actual)\n\t}\n}\n\nfunc lexerCompare(t *testing.T, n int, expected, actual []Token) {\n\tif len(expected) != len(actual) {\n\t\tt.Fatalf(\"Test case %d: expected %d token(s) but got %d\", n, len(expected), len(actual))\n\t}\n\n\tfor i := 0; i < len(actual) && i < len(expected); i++ {\n\t\tif actual[i].Line != expected[i].Line {\n\t\t\tt.Fatalf(\"Test case %d token %d ('%s'): expected line %d but was line %d\",\n\t\t\t\tn, i, expected[i].Text, expected[i].Line, actual[i].Line)\n\t\t\tbreak\n\t\t}\n\t\tif actual[i].Text != expected[i].Text {\n\t\t\tt.Fatalf(\"Test case %d token %d: expected text '%s' but was '%s'\",\n\t\t\t\tn, i, expected[i].Text, actual[i].Text)\n\t\t\tbreak\n\t\t}\n\t}\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: caddy\nSource File Path(s): ['caddy\\\\modules_test.go\\\\source_files\\\\modules.go']\nTest File Path: caddy\\modules_test.go\\modules_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: caddy\\modules_test.go\\source_files\\modules.go\n// Copyright 2015 Matthew Holt and The Caddy Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage caddy\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"reflect\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n)\n\n// Module is a type that is used as a Caddy module. In\n// addition to this interface, most modules will implement\n// some interface expected by their host module in order\n// to be useful. To learn which interface(s) to implement,\n// see the documentation for the host module. At a bare\n// minimum, this interface, when implemented, only provides\n// the module's ID and constructor function.\n//\n// Modules will often implement additional interfaces\n// including Provisioner, Validator, and CleanerUpper.\n// If a module implements these interfaces, their\n// methods are called during the module's lifespan.\n//\n// When a module is loaded by a host module, the following\n// happens: 1) ModuleInfo.New() is called to get a new\n// instance of the module. 2) The module's configuration is\n// unmarshaled into that instance. 3) If the module is a\n// Provisioner, the Provision() method is called. 4) If the\n// module is a Validator, the Validate() method is called.\n// 5) The module will probably be type-asserted from\n// 'any' to some other, more useful interface expected\n// by the host module. For example, HTTP handler modules are\n// type-asserted as caddyhttp.MiddlewareHandler values.\n// 6) When a module's containing Context is canceled, if it is\n// a CleanerUpper, its Cleanup() method is called.\ntype Module interface {\n\t// This method indicates that the type is a Caddy\n\t// module. The returned ModuleInfo must have both\n\t// a name and a constructor function. This method\n\t// must not have any side-effects.\n\tCaddyModule() ModuleInfo\n}\n\n// ModuleInfo represents a registered Caddy module.\ntype ModuleInfo struct {\n\t// ID is the \"full name\" of the module. It\n\t// must be unique and properly namespaced.\n\tID ModuleID\n\n\t// New returns a pointer to a new, empty\n\t// instance of the module's type. This\n\t// method must not have any side-effects,\n\t// and no other initialization should\n\t// occur within it. Any initialization\n\t// of the returned value should be done\n\t// in a Provision() method (see the\n\t// Provisioner interface).\n\tNew func() Module\n}\n\n// ModuleID is a string that uniquely identifies a Caddy module. A\n// module ID is lightly structured. It consists of dot-separated\n// labels which form a simple hierarchy from left to right. The last\n// label is the module name, and the labels before that constitute\n// the namespace (or scope).\n//\n// Thus, a module ID has the form: <namespace>.<name>\n//\n// An ID with no dot has the empty namespace, which is appropriate\n// for app modules (these are \"top-level\" modules that Caddy core\n// loads and runs).\n//\n// Module IDs should be lowercase and use underscores (_) instead of\n// spaces.\n//\n// Examples of valid IDs:\n// - http\n// - http.handlers.file_server\n// - caddy.logging.encoders.json\ntype ModuleID string\n\n// Namespace returns the namespace (or scope) portion of a module ID,\n// which is all but the last label of the ID. If the ID has only one\n// label, then the namespace is empty.\nfunc (id ModuleID) Namespace() string {\n\tlastDot := strings.LastIndex(string(id), \".\")\n\tif lastDot < 0 {\n\t\treturn \"\"\n\t}\n\treturn string(id)[:lastDot]\n}\n\n// Name returns the Name (last element) of a module ID.\nfunc (id ModuleID) Name() string {\n\tif id == \"\" {\n\t\treturn \"\"\n\t}\n\tparts := strings.Split(string(id), \".\")\n\treturn parts[len(parts)-1]\n}\n\nfunc (mi ModuleInfo) String() string { return string(mi.ID) }\n\n// ModuleMap is a map that can contain multiple modules,\n// where the map key is the module's name. (The namespace\n// is usually read from an associated field's struct tag.)\n// Because the module's name is given as the key in a\n// module map, the name does not have to be given in the\n// json.RawMessage.\ntype ModuleMap map[string]json.RawMessage\n\n// RegisterModule registers a module by receiving a\n// plain/empty value of the module. For registration to\n// be properly recorded, this should be called in the\n// init phase of runtime. Typically, the module package\n// will do this as a side-effect of being imported.\n// This function panics if the module's info is\n// incomplete or invalid, or if the module is already\n// registered.\nfunc RegisterModule(instance Module) {\n\tmod := instance.CaddyModule()\n\n\tif mod.ID == \"\" {\n\t\tpanic(\"module ID missing\")\n\t}\n\tif mod.ID == \"caddy\" || mod.ID == \"admin\" {\n\t\tpanic(fmt.Sprintf(\"module ID '%s' is reserved\", mod.ID))\n\t}\n\tif mod.New == nil {\n\t\tpanic(\"missing ModuleInfo.New\")\n\t}\n\tif val := mod.New(); val == nil {\n\t\tpanic(\"ModuleInfo.New must return a non-nil module instance\")\n\t}\n\n\tmodulesMu.Lock()\n\tdefer modulesMu.Unlock()\n\n\tif _, ok := modules[string(mod.ID)]; ok {\n\t\tpanic(fmt.Sprintf(\"module already registered: %s\", mod.ID))\n\t}\n\tmodules[string(mod.ID)] = mod\n}\n\n// GetModule returns module information from its ID (full name).\nfunc GetModule(name string) (ModuleInfo, error) {\n\tmodulesMu.RLock()\n\tdefer modulesMu.RUnlock()\n\tm, ok := modules[name]\n\tif !ok {\n\t\treturn ModuleInfo{}, fmt.Errorf(\"module not registered: %s\", name)\n\t}\n\treturn m, nil\n}\n\n// GetModuleName returns a module's name (the last label of its ID)\n// from an instance of its value. If the value is not a module, an\n// empty string will be returned.\nfunc GetModuleName(instance any) string {\n\tvar name string\n\tif mod, ok := instance.(Module); ok {\n\t\tname = mod.CaddyModule().ID.Name()\n\t}\n\treturn name\n}\n\n// GetModuleID returns a module's ID from an instance of its value.\n// If the value is not a module, an empty string will be returned.\nfunc GetModuleID(instance any) string {\n\tvar id string\n\tif mod, ok := instance.(Module); ok {\n\t\tid = string(mod.CaddyModule().ID)\n\t}\n\treturn id\n}\n\n// GetModules returns all modules in the given scope/namespace.\n// For example, a scope of \"foo\" returns modules named \"foo.bar\",\n// \"foo.loo\", but not \"bar\", \"foo.bar.loo\", etc. An empty scope\n// returns top-level modules, for example \"foo\" or \"bar\". Partial\n// scopes are not matched (i.e. scope \"foo.ba\" does not match\n// name \"foo.bar\").\n//\n// Because modules are registered to a map under the hood, the\n// returned slice will be sorted to keep it deterministic.\nfunc GetModules(scope string) []ModuleInfo {\n\tmodulesMu.RLock()\n\tdefer modulesMu.RUnlock()\n\n\tscopeParts := strings.Split(scope, \".\")\n\n\t// handle the special case of an empty scope, which\n\t// should match only the top-level modules\n\tif scope == \"\" {\n\t\tscopeParts = []string{}\n\t}\n\n\tvar mods []ModuleInfo\niterateModules:\n\tfor id, m := range modules {\n\t\tmodParts := strings.Split(id, \".\")\n\n\t\t// match only the next level of nesting\n\t\tif len(modParts) != len(scopeParts)+1 {\n\t\t\tcontinue\n\t\t}\n\n\t\t// specified parts must be exact matches\n\t\tfor i := range scopeParts {\n\t\t\tif modParts[i] != scopeParts[i] {\n\t\t\t\tcontinue iterateModules\n\t\t\t}\n\t\t}\n\n\t\tmods = append(mods, m)\n\t}\n\n\t// make return value deterministic\n\tsort.Slice(mods, func(i, j int) bool {\n\t\treturn mods[i].ID < mods[j].ID\n\t})\n\n\treturn mods\n}\n\n// Modules returns the names of all registered modules\n// in ascending lexicographical order.\nfunc Modules() []string {\n\tmodulesMu.RLock()\n\tdefer modulesMu.RUnlock()\n\n\tnames := make([]string, 0, len(modules))\n\tfor name := range modules {\n\t\tnames = append(names, name)\n\t}\n\n\tsort.Strings(names)\n\n\treturn names\n}\n\n// getModuleNameInline loads the string value from raw of moduleNameKey,\n// where raw must be a JSON encoding of a map. It returns that value,\n// along with the result of removing that key from raw.\nfunc getModuleNameInline(moduleNameKey string, raw json.RawMessage) (string, json.RawMessage, error) {\n\tvar tmp map[string]any\n\terr := json.Unmarshal(raw, &tmp)\n\tif err != nil {\n\t\treturn \"\", nil, err\n\t}\n\n\tmoduleName, ok := tmp[moduleNameKey].(string)\n\tif !ok || moduleName == \"\" {\n\t\treturn \"\", nil, fmt.Errorf(\"module name not specified with key '%s' in %+v\", moduleNameKey, tmp)\n\t}\n\n\t// remove key from the object, otherwise decoding it later\n\t// will yield an error because the struct won't recognize it\n\t// (this is only needed because we strictly enforce that\n\t// all keys are recognized when loading modules)\n\tdelete(tmp, moduleNameKey)\n\tresult, err := json.Marshal(tmp)\n\tif err != nil {\n\t\treturn \"\", nil, fmt.Errorf(\"re-encoding module configuration: %v\", err)\n\t}\n\n\treturn moduleName, result, nil\n}\n\n// Provisioner is implemented by modules which may need to perform\n// some additional \"setup\" steps immediately after being loaded.\n// Provisioning should be fast (imperceptible running time). If\n// any side-effects result in the execution of this function (e.g.\n// creating global state, any other allocations which require\n// garbage collection, opening files, starting goroutines etc.),\n// be sure to clean up properly by implementing the CleanerUpper\n// interface to avoid leaking resources.\ntype Provisioner interface {\n\tProvision(Context) error\n}\n\n// Validator is implemented by modules which can verify that their\n// configurations are valid. This method will be called after\n// Provision() (if implemented). Validation should always be fast\n// (imperceptible running time) and an error must be returned if\n// the module's configuration is invalid.\ntype Validator interface {\n\tValidate() error\n}\n\n// CleanerUpper is implemented by modules which may have side-effects\n// such as opened files, spawned goroutines, or allocated some sort\n// of non-stack state when they were provisioned. This method should\n// deallocate/cleanup those resources to prevent memory leaks. Cleanup\n// should be fast and efficient. Cleanup should work even if Provision\n// returns an error, to allow cleaning up from partial provisionings.\ntype CleanerUpper interface {\n\tCleanup() error\n}\n\n// ParseStructTag parses a caddy struct tag into its keys and values.\n// It is very simple. The expected syntax is:\n// `caddy:\"key1=val1 key2=val2 ...\"`\nfunc ParseStructTag(tag string) (map[string]string, error) {\n\tresults := make(map[string]string)\n\tpairs := strings.Split(tag, \" \")\n\tfor i, pair := range pairs {\n\t\tif pair == \"\" {\n\t\t\tcontinue\n\t\t}\n\t\tbefore, after, isCut := strings.Cut(pair, \"=\")\n\t\tif !isCut {\n\t\t\treturn nil, fmt.Errorf(\"missing key in '%s' (pair %d)\", pair, i)\n\t\t}\n\t\tresults[before] = after\n\t}\n\treturn results, nil\n}\n\n// StrictUnmarshalJSON is like json.Unmarshal but returns an error\n// if any of the fields are unrecognized. Useful when decoding\n// module configurations, where you want to be more sure they're\n// correct.\nfunc StrictUnmarshalJSON(data []byte, v any) error {\n\tdec := json.NewDecoder(bytes.NewReader(data))\n\tdec.DisallowUnknownFields()\n\treturn dec.Decode(v)\n}\n\n// isJSONRawMessage returns true if the type is encoding/json.RawMessage.\nfunc isJSONRawMessage(typ reflect.Type) bool {\n\treturn typ.PkgPath() == \"encoding/json\" && typ.Name() == \"RawMessage\"\n}\n\n// isModuleMapType returns true if the type is map[string]json.RawMessage.\n// It assumes that the string key is the module name, but this is not\n// always the case. To know for sure, this function must return true, but\n// also the struct tag where this type appears must NOT define an inline_key\n// attribute, which would mean that the module names appear inline with the\n// values, not in the key.\nfunc isModuleMapType(typ reflect.Type) bool {\n\treturn typ.Kind() == reflect.Map &&\n\t\ttyp.Key().Kind() == reflect.String &&\n\t\tisJSONRawMessage(typ.Elem())\n}\n\nvar (\n\tmodules   = make(map[string]ModuleInfo)\n\tmodulesMu sync.RWMutex\n)\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:48:09"}, {"role": "assistant", "content": "```go\n// Copyright 2015 Matthew Holt and The Caddy Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage caddy\n\nimport (\n\t\"reflect\"\n\t\"testing\"\n)\n\nfunc TestGetModules(t *testing.T) {\n\tmodulesMu.Lock()\n\tmodules = map[string]ModuleInfo{\n\t\t\"a\":      {ID: \"a\"},\n\t\t\"a.b\":    {ID: \"a.b\"},\n\t\t\"a.b.c\":  {ID: \"a.b.c\"},\n\t\t\"a.b.cd\": {ID: \"a.b.cd\"},\n\t\t\"a.c\":    {ID: \"a.c\"},\n\t\t\"a.d\":    {ID: \"a.d\"},\n\t\t\"b\":      {ID: \"b\"},\n\t\t\"b.a\":    {ID: \"b.a\"},\n\t\t\"b.b\":    {ID: \"b.b\"},\n\t\t\"b.a.c\":  {ID: \"b.a.c\"},\n\t\t\"c\":      {ID: \"c\"},\n\t}\n\tmodulesMu.Unlock()\n\n\tfor i, tc := range []struct {\n\t\tinput  string\n\t\texpect []ModuleInfo\n\t}{\n\t\t{\n\t\t\tinput: \"\",\n\t\t\texpect: []ModuleInfo{\n\t\t\t\t{ID: \"a\"},\n\t\t\t\t{ID: \"b\"},\n\t\t\t\t{ID: \"c\"},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: \"a\",\n\t\t\texpect: []ModuleInfo{\n\t\t\t\t{ID: \"a.b\"},\n\t\t\t\t{ID: \"a.c\"},\n\t\t\t\t{ID: \"a.d\"},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: \"a.b\",\n\t\t\texpect: []ModuleInfo{\n\t\t\t\t{ID: \"a.b.c\"},\n\t\t\t\t{ID: \"a.b.cd\"},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: \"a.b.c\",\n\t\t},\n\t\t{\n\t\t\tinput: \"b\",\n\t\t\texpect: []ModuleInfo{\n\t\t\t\t{ID: \"b.a\"},\n\t\t\t\t{ID: \"b.b\"},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tinput: \"asdf\",\n\t\t},\n\t} {\n\t\tactual := GetModules(tc.input)\n\t\tif !reflect.DeepEqual(actual, tc.expect) {\n\t\t\tt.Errorf(\"Test %d: Expected %v but got %v\", i, tc.expect, actual)\n\t\t}\n\t}\n}\n\nfunc TestModuleID(t *testing.T) {\n\tfor i, tc := range []struct {\n\t\tinput           ModuleID\n\t\texpectNamespace string\n\t\texpectName      string\n\t}{\n\t\t{\n\t\t\tinput:           \"foo\",\n\t\t\texpectNamespace: \"\",\n\t\t\texpectName:      \"foo\",\n\t\t},\n\t\t{\n\t\t\tinput:           \"foo.bar\",\n\t\t\texpectNamespace: \"foo\",\n\t\t\texpectName:      \"bar\",\n\t\t},\n\t\t{\n\t\t\tinput:           \"a.b.c\",\n\t\t\texpectNamespace: \"a.b\",\n\t\t\texpectName:      \"c\",\n\t\t},\n\t} {\n\t\tactualNamespace := tc.input.Namespace()\n\t\tif actualNamespace != tc.expectNamespace {\n\t\t\tt.Errorf(\"Test %d: Expected namespace '%s' but got '%s'\", i, tc.expectNamespace, actualNamespace)\n\t\t}\n\t\tactualName := tc.input.Name()\n\t\tif actualName != tc.expectName {\n\t\t\tt.Errorf(\"Test %d: Expected name '%s' but got '%s'\", i, tc.expectName, actualName)\n\t\t}\n\t}\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: caddy\nSource File Path(s): ['caddy\\\\replacer_test.go\\\\source_files\\\\replacer.go']\nTest File Path: caddy\\replacer_test.go\\replacer_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: caddy\\replacer_test.go\\source_files\\replacer.go\n// Copyright 2015 Matthew Holt and The Caddy Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage caddy\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"io\"\n\t\"net/http\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"runtime\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"go.uber.org/zap\"\n)\n\n// NewReplacer returns a new Replacer.\nfunc NewReplacer() *Replacer {\n\trep := &Replacer{\n\t\tstatic:   make(map[string]any),\n\t\tmapMutex: &sync.RWMutex{},\n\t}\n\trep.providers = []replacementProvider{\n\t\tglobalDefaultReplacementProvider{},\n\t\tfileReplacementProvider{},\n\t\tReplacerFunc(rep.fromStatic),\n\t}\n\treturn rep\n}\n\n// NewEmptyReplacer returns a new Replacer,\n// without the global default replacements.\nfunc NewEmptyReplacer() *Replacer {\n\trep := &Replacer{\n\t\tstatic:   make(map[string]any),\n\t\tmapMutex: &sync.RWMutex{},\n\t}\n\trep.providers = []replacementProvider{\n\t\tReplacerFunc(rep.fromStatic),\n\t}\n\treturn rep\n}\n\n// Replacer can replace values in strings.\n// A default/empty Replacer is not valid;\n// use NewReplacer to make one.\ntype Replacer struct {\n\tproviders []replacementProvider\n\tstatic    map[string]any\n\tmapMutex  *sync.RWMutex\n}\n\n// WithoutFile returns a copy of the current Replacer\n// without support for the {file.*} placeholder, which\n// may be unsafe in some contexts.\n//\n// EXPERIMENTAL: Subject to change or removal.\nfunc (r *Replacer) WithoutFile() *Replacer {\n\trep := &Replacer{static: r.static}\n\tfor _, v := range r.providers {\n\t\tif _, ok := v.(fileReplacementProvider); ok {\n\t\t\tcontinue\n\t\t}\n\t\trep.providers = append(rep.providers, v)\n\t}\n\treturn rep\n}\n\n// Map adds mapFunc to the list of value providers.\n// mapFunc will be executed only at replace-time.\nfunc (r *Replacer) Map(mapFunc ReplacerFunc) {\n\tr.providers = append(r.providers, mapFunc)\n}\n\n// Set sets a custom variable to a static value.\nfunc (r *Replacer) Set(variable string, value any) {\n\tr.mapMutex.Lock()\n\tr.static[variable] = value\n\tr.mapMutex.Unlock()\n}\n\n// Get gets a value from the replacer. It returns\n// the value and whether the variable was known.\nfunc (r *Replacer) Get(variable string) (any, bool) {\n\tfor _, mapFunc := range r.providers {\n\t\tif val, ok := mapFunc.replace(variable); ok {\n\t\t\treturn val, true\n\t\t}\n\t}\n\treturn nil, false\n}\n\n// GetString is the same as Get, but coerces the value to a\n// string representation as efficiently as possible.\nfunc (r *Replacer) GetString(variable string) (string, bool) {\n\ts, found := r.Get(variable)\n\treturn ToString(s), found\n}\n\n// Delete removes a variable with a static value\n// that was created using Set.\nfunc (r *Replacer) Delete(variable string) {\n\tr.mapMutex.Lock()\n\tdelete(r.static, variable)\n\tr.mapMutex.Unlock()\n}\n\n// fromStatic provides values from r.static.\nfunc (r *Replacer) fromStatic(key string) (any, bool) {\n\tr.mapMutex.RLock()\n\tdefer r.mapMutex.RUnlock()\n\tval, ok := r.static[key]\n\treturn val, ok\n}\n\n// ReplaceOrErr is like ReplaceAll, but any placeholders\n// that are empty or not recognized will cause an error to\n// be returned.\nfunc (r *Replacer) ReplaceOrErr(input string, errOnEmpty, errOnUnknown bool) (string, error) {\n\treturn r.replace(input, \"\", false, errOnEmpty, errOnUnknown, nil)\n}\n\n// ReplaceKnown is like ReplaceAll but only replaces\n// placeholders that are known (recognized). Unrecognized\n// placeholders will remain in the output.\nfunc (r *Replacer) ReplaceKnown(input, empty string) string {\n\tout, _ := r.replace(input, empty, false, false, false, nil)\n\treturn out\n}\n\n// ReplaceAll efficiently replaces placeholders in input with\n// their values. All placeholders are replaced in the output\n// whether they are recognized or not. Values that are empty\n// string will be substituted with empty.\nfunc (r *Replacer) ReplaceAll(input, empty string) string {\n\tout, _ := r.replace(input, empty, true, false, false, nil)\n\treturn out\n}\n\n// ReplaceFunc is the same as ReplaceAll, but calls f for every\n// replacement to be made, in case f wants to change or inspect\n// the replacement.\nfunc (r *Replacer) ReplaceFunc(input string, f ReplacementFunc) (string, error) {\n\treturn r.replace(input, \"\", true, false, false, f)\n}\n\nfunc (r *Replacer) replace(input, empty string,\n\ttreatUnknownAsEmpty, errOnEmpty, errOnUnknown bool,\n\tf ReplacementFunc,\n) (string, error) {\n\tif !strings.Contains(input, string(phOpen)) && !strings.Contains(input, string(phClose)) {\n\t\treturn input, nil\n\t}\n\n\tvar sb strings.Builder\n\n\t// it is reasonable to assume that the output\n\t// will be approximately as long as the input\n\tsb.Grow(len(input))\n\n\t// iterate the input to find each placeholder\n\tvar lastWriteCursor int\n\n\t// fail fast if too many placeholders are unclosed\n\tvar unclosedCount int\n\nscan:\n\tfor i := 0; i < len(input); i++ {\n\t\t// check for escaped braces\n\t\tif i > 0 && input[i-1] == phEscape && (input[i] == phClose || input[i] == phOpen) {\n\t\t\tsb.WriteString(input[lastWriteCursor : i-1])\n\t\t\tlastWriteCursor = i\n\t\t\tcontinue\n\t\t}\n\n\t\tif input[i] != phOpen {\n\t\t\tcontinue\n\t\t}\n\n\t\t// our iterator is now on an unescaped open brace (start of placeholder)\n\n\t\t// too many unclosed placeholders in absolutely ridiculous input can be extremely slow (issue #4170)\n\t\tif unclosedCount > 100 {\n\t\t\treturn \"\", fmt.Errorf(\"too many unclosed placeholders\")\n\t\t}\n\n\t\t// find the end of the placeholder\n\t\tend := strings.Index(input[i:], string(phClose)) + i\n\t\tif end < i {\n\t\t\tunclosedCount++\n\t\t\tcontinue\n\t\t}\n\n\t\t// if necessary look for the first closing brace that is not escaped\n\t\tfor end > 0 && end < len(input)-1 && input[end-1] == phEscape {\n\t\t\tnextEnd := strings.Index(input[end+1:], string(phClose))\n\t\t\tif nextEnd < 0 {\n\t\t\t\tunclosedCount++\n\t\t\t\tcontinue scan\n\t\t\t}\n\t\t\tend += nextEnd + 1\n\t\t}\n\n\t\t// write the substring from the last cursor to this point\n\t\tsb.WriteString(input[lastWriteCursor:i])\n\n\t\t// trim opening bracket\n\t\tkey := input[i+1 : end]\n\n\t\t// try to get a value for this key, handle empty values accordingly\n\t\tval, found := r.Get(key)\n\t\tif !found {\n\t\t\t// placeholder is unknown (unrecognized); handle accordingly\n\t\t\tif errOnUnknown {\n\t\t\t\treturn \"\", fmt.Errorf(\"unrecognized placeholder %s%s%s\",\n\t\t\t\t\tstring(phOpen), key, string(phClose))\n\t\t\t} else if !treatUnknownAsEmpty {\n\t\t\t\t// if treatUnknownAsEmpty is true, we'll handle an empty\n\t\t\t\t// val later; so only continue otherwise\n\t\t\t\tlastWriteCursor = i\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\t// apply any transformations\n\t\tif f != nil {\n\t\t\tvar err error\n\t\t\tval, err = f(key, val)\n\t\t\tif err != nil {\n\t\t\t\treturn \"\", err\n\t\t\t}\n\t\t}\n\n\t\t// convert val to a string as efficiently as possible\n\t\tvalStr := ToString(val)\n\n\t\t// write the value; if it's empty, either return\n\t\t// an error or write a default value\n\t\tif valStr == \"\" {\n\t\t\tif errOnEmpty {\n\t\t\t\treturn \"\", fmt.Errorf(\"evaluated placeholder %s%s%s is empty\",\n\t\t\t\t\tstring(phOpen), key, string(phClose))\n\t\t\t} else if empty != \"\" {\n\t\t\t\tsb.WriteString(empty)\n\t\t\t}\n\t\t} else {\n\t\t\tsb.WriteString(valStr)\n\t\t}\n\n\t\t// advance cursor to end of placeholder\n\t\ti = end\n\t\tlastWriteCursor = i + 1\n\t}\n\n\t// flush any unwritten remainder\n\tsb.WriteString(input[lastWriteCursor:])\n\n\treturn sb.String(), nil\n}\n\n// ToString returns val as a string, as efficiently as possible.\n// EXPERIMENTAL: may be changed or removed later.\nfunc ToString(val any) string {\n\tswitch v := val.(type) {\n\tcase nil:\n\t\treturn \"\"\n\tcase string:\n\t\treturn v\n\tcase fmt.Stringer:\n\t\treturn v.String()\n\tcase error:\n\t\treturn v.Error()\n\tcase byte:\n\t\treturn string(v)\n\tcase []byte:\n\t\treturn string(v)\n\tcase []rune:\n\t\treturn string(v)\n\tcase int:\n\t\treturn strconv.Itoa(v)\n\tcase int32:\n\t\treturn strconv.Itoa(int(v))\n\tcase int64:\n\t\treturn strconv.Itoa(int(v))\n\tcase uint:\n\t\treturn strconv.FormatUint(uint64(v), 10)\n\tcase uint32:\n\t\treturn strconv.FormatUint(uint64(v), 10)\n\tcase uint64:\n\t\treturn strconv.FormatUint(v, 10)\n\tcase float32:\n\t\treturn strconv.FormatFloat(float64(v), 'f', -1, 32)\n\tcase float64:\n\t\treturn strconv.FormatFloat(v, 'f', -1, 64)\n\tcase bool:\n\t\tif v {\n\t\t\treturn \"true\"\n\t\t}\n\t\treturn \"false\"\n\tdefault:\n\t\treturn fmt.Sprintf(\"%+v\", v)\n\t}\n}\n\n// ReplacerFunc is a function that returns a replacement for the\n// given key along with true if the function is able to service\n// that key (even if the value is blank). If the function does\n// not recognize the key, false should be returned.\ntype ReplacerFunc func(key string) (any, bool)\n\nfunc (f ReplacerFunc) replace(key string) (any, bool) {\n\treturn f(key)\n}\n\n// replacementProvider is a type that can provide replacements\n// for placeholders. Allows for type assertion to determine\n// which type of provider it is.\ntype replacementProvider interface {\n\treplace(key string) (any, bool)\n}\n\n// fileReplacementsProvider handles {file.*} replacements,\n// reading a file from disk and replacing with its contents.\ntype fileReplacementProvider struct{}\n\nfunc (f fileReplacementProvider) replace(key string) (any, bool) {\n\tif !strings.HasPrefix(key, filePrefix) {\n\t\treturn nil, false\n\t}\n\n\tfilename := key[len(filePrefix):]\n\tmaxSize := 1024 * 1024\n\tbody, err := readFileIntoBuffer(filename, maxSize)\n\tif err != nil {\n\t\twd, _ := os.Getwd()\n\t\tLog().Error(\"placeholder: failed to read file\",\n\t\t\tzap.String(\"file\", filename),\n\t\t\tzap.String(\"working_dir\", wd),\n\t\t\tzap.Error(err))\n\t\treturn nil, true\n\t}\n\tbody = bytes.TrimSuffix(body, []byte(\"\\n\"))\n\tbody = bytes.TrimSuffix(body, []byte(\"\\r\"))\n\treturn string(body), true\n}\n\n// globalDefaultReplacementsProvider handles replacements\n// that can be used in any context, such as system variables,\n// time, or environment variables.\ntype globalDefaultReplacementProvider struct{}\n\nfunc (f globalDefaultReplacementProvider) replace(key string) (any, bool) {\n\t// check environment variable\n\tconst envPrefix = \"env.\"\n\tif strings.HasPrefix(key, envPrefix) {\n\t\treturn os.Getenv(key[len(envPrefix):]), true\n\t}\n\n\tswitch key {\n\tcase \"system.hostname\":\n\t\t// OK if there is an error; just return empty string\n\t\tname, _ := os.Hostname()\n\t\treturn name, true\n\tcase \"system.slash\":\n\t\treturn string(filepath.Separator), true\n\tcase \"system.os\":\n\t\treturn runtime.GOOS, true\n\tcase \"system.wd\":\n\t\t// OK if there is an error; just return empty string\n\t\twd, _ := os.Getwd()\n\t\treturn wd, true\n\tcase \"system.arch\":\n\t\treturn runtime.GOARCH, true\n\tcase \"time.now\":\n\t\treturn nowFunc(), true\n\tcase \"time.now.http\":\n\t\t// According to the comment for http.TimeFormat, the timezone must be in UTC\n\t\t// to generate the correct format.\n\t\t// https://github.com/caddyserver/caddy/issues/5773\n\t\treturn nowFunc().UTC().Format(http.TimeFormat), true\n\tcase \"time.now.common_log\":\n\t\treturn nowFunc().Format(\"02/Jan/2006:15:04:05 -0700\"), true\n\tcase \"time.now.year\":\n\t\treturn strconv.Itoa(nowFunc().Year()), true\n\tcase \"time.now.unix\":\n\t\treturn strconv.FormatInt(nowFunc().Unix(), 10), true\n\tcase \"time.now.unix_ms\":\n\t\treturn strconv.FormatInt(nowFunc().UnixNano()/int64(time.Millisecond), 10), true\n\t}\n\n\treturn nil, false\n}\n\n// readFileIntoBuffer reads the file at filePath into a size limited buffer.\nfunc readFileIntoBuffer(filename string, size int) ([]byte, error) {\n\tfile, err := os.Open(filename)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer file.Close()\n\n\tbuffer := make([]byte, size)\n\tn, err := file.Read(buffer)\n\tif err != nil && err != io.EOF {\n\t\treturn nil, err\n\t}\n\n\t// slice the buffer to the actual size\n\treturn buffer[:n], nil\n}\n\n// ReplacementFunc is a function that is called when a\n// replacement is being performed. It receives the\n// variable (i.e. placeholder name) and the value that\n// will be the replacement, and returns the value that\n// will actually be the replacement, or an error. Note\n// that errors are sometimes ignored by replacers.\ntype ReplacementFunc func(variable string, val any) (any, error)\n\n// nowFunc is a variable so tests can change it\n// in order to obtain a deterministic time.\nvar nowFunc = time.Now\n\n// ReplacerCtxKey is the context key for a replacer.\nconst ReplacerCtxKey CtxKey = \"replacer\"\n\nconst phOpen, phClose, phEscape = '{', '}', '\\\\'\n\nconst filePrefix = \"file.\"\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:48:09"}, {"role": "assistant", "content": "```go\n// Copyright 2015 Matthew Holt and The Caddy Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage caddy\n\nimport (\n\t\"fmt\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"runtime\"\n\t\"sync\"\n\t\"testing\"\n)\n\nfunc TestReplacer(t *testing.T) {\n\ttype testCase struct {\n\t\tinput, expect, empty string\n\t}\n\n\trep := testReplacer()\n\n\t// ReplaceAll\n\tfor i, tc := range []testCase{\n\t\t{\n\t\t\tinput:  \"{\",\n\t\t\texpect: \"{\",\n\t\t},\n\t\t{\n\t\t\tinput:  `\\{`,\n\t\t\texpect: `{`,\n\t\t},\n\t\t{\n\t\t\tinput:  \"foo{\",\n\t\t\texpect: \"foo{\",\n\t\t},\n\t\t{\n\t\t\tinput:  `foo\\{`,\n\t\t\texpect: `foo{`,\n\t\t},\n\t\t{\n\t\t\tinput:  \"foo{bar\",\n\t\t\texpect: \"foo{bar\",\n\t\t},\n\t\t{\n\t\t\tinput:  `foo\\{bar`,\n\t\t\texpect: `foo{bar`,\n\t\t},\n\t\t{\n\t\t\tinput:  \"foo{bar}\",\n\t\t\texpect: \"foo\",\n\t\t},\n\t\t{\n\t\t\tinput:  `foo\\{bar\\}`,\n\t\t\texpect: `foo{bar}`,\n\t\t},\n\t\t{\n\t\t\tinput:  \"}\",\n\t\t\texpect: \"}\",\n\t\t},\n\t\t{\n\t\t\tinput:  `\\}`,\n\t\t\texpect: `}`,\n\t\t},\n\t\t{\n\t\t\tinput:  \"{}\",\n\t\t\texpect: \"\",\n\t\t},\n\t\t{\n\t\t\tinput:  `\\{\\}`,\n\t\t\texpect: `{}`,\n\t\t},\n\t\t{\n\t\t\tinput:  `{\"json\": \"object\"}`,\n\t\t\texpect: \"\",\n\t\t},\n\t\t{\n\t\t\tinput:  `\\{\"json\": \"object\"}`,\n\t\t\texpect: `{\"json\": \"object\"}`,\n\t\t},\n\t\t{\n\t\t\tinput:  `\\{\"json\": \"object\"\\}`,\n\t\t\texpect: `{\"json\": \"object\"}`,\n\t\t},\n\t\t{\n\t\t\tinput:  `\\{\"json\": \"object{bar}\"\\}`,\n\t\t\texpect: `{\"json\": \"object\"}`,\n\t\t},\n\t\t{\n\t\t\tinput:  `\\{\"json\": \\{\"nested\": \"object\"\\}\\}`,\n\t\t\texpect: `{\"json\": {\"nested\": \"object\"}}`,\n\t\t},\n\t\t{\n\t\t\tinput:  `\\{\"json\": \\{\"nested\": \"{bar}\"\\}\\}`,\n\t\t\texpect: `{\"json\": {\"nested\": \"\"}}`,\n\t\t},\n\t\t{\n\t\t\tinput:  `pre \\{\"json\": \\{\"nested\": \"{bar}\"\\}\\}`,\n\t\t\texpect: `pre {\"json\": {\"nested\": \"\"}}`,\n\t\t},\n\t\t{\n\t\t\tinput:  `\\{\"json\": \\{\"nested\": \"{bar}\"\\}\\} post`,\n\t\t\texpect: `{\"json\": {\"nested\": \"\"}} post`,\n\t\t},\n\t\t{\n\t\t\tinput:  `pre \\{\"json\": \\{\"nested\": \"{bar}\"\\}\\} post`,\n\t\t\texpect: `pre {\"json\": {\"nested\": \"\"}} post`,\n\t\t},\n\t\t{\n\t\t\tinput:  `{{`,\n\t\t\texpect: \"{{\",\n\t\t},\n\t\t{\n\t\t\tinput:  `{{}`,\n\t\t\texpect: \"\",\n\t\t},\n\t\t{\n\t\t\tinput:  `{\"json\": \"object\"\\}`,\n\t\t\texpect: \"\",\n\t\t},\n\t\t{\n\t\t\tinput:  `{unknown}`,\n\t\t\tempty:  \"-\",\n\t\t\texpect: \"-\",\n\t\t},\n\t\t{\n\t\t\tinput:  `back\\slashes`,\n\t\t\texpect: `back\\slashes`,\n\t\t},\n\t\t{\n\t\t\tinput:  `double back\\\\slashes`,\n\t\t\texpect: `double back\\\\slashes`,\n\t\t},\n\t\t{\n\t\t\tinput:  `placeholder {with \\{ brace} in name`,\n\t\t\texpect: `placeholder  in name`,\n\t\t},\n\t\t{\n\t\t\tinput:  `placeholder {with \\} brace} in name`,\n\t\t\texpect: `placeholder  in name`,\n\t\t},\n\t\t{\n\t\t\tinput:  `placeholder {with \\} \\} braces} in name`,\n\t\t\texpect: `placeholder  in name`,\n\t\t},\n\t\t{\n\t\t\tinput:  `\\{'group':'default','max_age':3600,'endpoints':[\\{'url':'https://some.domain.local/a/d/g'\\}],'include_subdomains':true\\}`,\n\t\t\texpect: `{'group':'default','max_age':3600,'endpoints':[{'url':'https://some.domain.local/a/d/g'}],'include_subdomains':true}`,\n\t\t},\n\t\t{\n\t\t\tinput:  `{}{}{}{\\\\\\\\}\\\\\\\\`,\n\t\t\texpect: `{\\\\\\}\\\\\\\\`,\n\t\t},\n\t\t{\n\t\t\tinput:  string([]byte{0x26, 0x00, 0x83, 0x7B, 0x84, 0x07, 0x5C, 0x7D, 0x84}),\n\t\t\texpect: string([]byte{0x26, 0x00, 0x83, 0x7B, 0x84, 0x07, 0x7D, 0x84}),\n\t\t},\n\t\t{\n\t\t\tinput:  `\\\\}`,\n\t\t\texpect: `\\}`,\n\t\t},\n\t} {\n\t\tactual := rep.ReplaceAll(tc.input, tc.empty)\n\t\tif actual != tc.expect {\n\t\t\tt.Errorf(\"Test %d: '%s': expected '%s' but got '%s'\",\n\t\t\t\ti, tc.input, tc.expect, actual)\n\t\t}\n\t}\n}\n\nfunc TestReplacerSet(t *testing.T) {\n\trep := testReplacer()\n\n\tfor _, tc := range []struct {\n\t\tvariable string\n\t\tvalue    any\n\t}{\n\t\t{\n\t\t\tvariable: \"test1\",\n\t\t\tvalue:    \"val1\",\n\t\t},\n\t\t{\n\t\t\tvariable: \"asdf\",\n\t\t\tvalue:    \"123\",\n\t\t},\n\t\t{\n\t\t\tvariable: \"numbers\",\n\t\t\tvalue:    123.456,\n\t\t},\n\t\t{\n\t\t\tvariable: \"\u00e4\u00f6\u00fc\",\n\t\t\tvalue:    \"\u00f6\u00f6_\u00e4\u00fc\",\n\t\t},\n\t\t{\n\t\t\tvariable: \"with space\",\n\t\t\tvalue:    \"space value\",\n\t\t},\n\t\t{\n\t\t\tvariable: \"1\",\n\t\t\tvalue:    \"test-123\",\n\t\t},\n\t\t{\n\t\t\tvariable: \"mySuper_IP\",\n\t\t\tvalue:    \"1.2.3.4\",\n\t\t},\n\t\t{\n\t\t\tvariable: \"testEmpty\",\n\t\t\tvalue:    \"\",\n\t\t},\n\t} {\n\t\trep.Set(tc.variable, tc.value)\n\n\t\t// test if key is added\n\t\tif val, ok := rep.static[tc.variable]; ok {\n\t\t\tif val != tc.value {\n\t\t\t\tt.Errorf(\"Expected value '%s' for key '%s' got '%s'\", tc.value, tc.variable, val)\n\t\t\t}\n\t\t} else {\n\t\t\tt.Errorf(\"Expected existing key '%s' found nothing\", tc.variable)\n\t\t}\n\t}\n\n\t// test if all keys are still there (by length)\n\tlength := len(rep.static)\n\tif len(rep.static) != 8 {\n\t\tt.Errorf(\"Expected length '%v' got '%v'\", 7, length)\n\t}\n}\n\nfunc TestReplacerReplaceKnown(t *testing.T) {\n\trep := Replacer{\n\t\tmapMutex: &sync.RWMutex{},\n\t\tproviders: []replacementProvider{\n\t\t\t// split our possible vars to two functions (to test if both functions are called)\n\t\t\tReplacerFunc(func(key string) (val any, ok bool) {\n\t\t\t\tswitch key {\n\t\t\t\tcase \"test1\":\n\t\t\t\t\treturn \"val1\", true\n\t\t\t\tcase \"asdf\":\n\t\t\t\t\treturn \"123\", true\n\t\t\t\tcase \"\u00e4\u00f6\u00fc\":\n\t\t\t\t\treturn \"\u00f6\u00f6_\u00e4\u00fc\", true\n\t\t\t\tcase \"with space\":\n\t\t\t\t\treturn \"space value\", true\n\t\t\t\tdefault:\n\t\t\t\t\treturn \"NOOO\", false\n\t\t\t\t}\n\t\t\t}),\n\t\t\tReplacerFunc(func(key string) (val any, ok bool) {\n\t\t\t\tswitch key {\n\t\t\t\tcase \"1\":\n\t\t\t\t\treturn \"test-123\", true\n\t\t\t\tcase \"mySuper_IP\":\n\t\t\t\t\treturn \"1.2.3.4\", true\n\t\t\t\tcase \"testEmpty\":\n\t\t\t\t\treturn \"\", true\n\t\t\t\tdefault:\n\t\t\t\t\treturn \"NOOO\", false\n\t\t\t\t}\n\t\t\t}),\n\t\t},\n\t}\n\n\tfor _, tc := range []struct {\n\t\ttestInput string\n\t\texpected  string\n\t}{\n\t\t{\n\t\t\t// test vars without space\n\t\t\ttestInput: \"{test1}{asdf}{\u00e4\u00f6\u00fc}{1}{with space}{mySuper_IP}\",\n\t\t\texpected:  \"val1123\u00f6\u00f6_\u00e4\u00fctest-123space value1.2.3.4\",\n\t\t},\n\t\t{\n\t\t\t// test vars with space\n\t\t\ttestInput: \"{test1} {asdf} {\u00e4\u00f6\u00fc} {1} {with space} {mySuper_IP} \",\n\t\t\texpected:  \"val1 123 \u00f6\u00f6_\u00e4\u00fc test-123 space value 1.2.3.4 \",\n\t\t},\n\t\t{\n\t\t\t// test with empty val\n\t\t\ttestInput: \"{test1} {testEmpty} {asdf} {1} \",\n\t\t\texpected:  \"val1 EMPTY 123 test-123 \",\n\t\t},\n\t\t{\n\t\t\t// test vars with not finished placeholders\n\t\t\ttestInput: \"{te{test1}{as{{df{1}\",\n\t\t\texpected:  \"{teval1{as{{dftest-123\",\n\t\t},\n\t\t{\n\t\t\t// test with non existing vars\n\t\t\ttestInput: \"{test1} {nope} {1} \",\n\t\t\texpected:  \"val1 {nope} test-123 \",\n\t\t},\n\t} {\n\t\tactual := rep.ReplaceKnown(tc.testInput, \"EMPTY\")\n\n\t\t// test if all are replaced as expected\n\t\tif actual != tc.expected {\n\t\t\tt.Errorf(\"Expected '%s' got '%s' for '%s'\", tc.expected, actual, tc.testInput)\n\t\t}\n\t}\n}\n\nfunc TestReplacerDelete(t *testing.T) {\n\trep := Replacer{\n\t\tmapMutex: &sync.RWMutex{},\n\t\tstatic: map[string]any{\n\t\t\t\"key1\": \"val1\",\n\t\t\t\"key2\": \"val2\",\n\t\t\t\"key3\": \"val3\",\n\t\t\t\"key4\": \"val4\",\n\t\t},\n\t}\n\n\tstartLen := len(rep.static)\n\n\ttoDel := []string{\n\t\t\"key2\", \"key4\",\n\t}\n\n\tfor _, key := range toDel {\n\t\trep.Delete(key)\n\n\t\t// test if key is removed from static map\n\t\tif _, ok := rep.static[key]; ok {\n\t\t\tt.Errorf(\"Expected '%s' to be removed. It is still in static map.\", key)\n\t\t}\n\t}\n\n\t// check if static slice is smaller\n\texpected := startLen - len(toDel)\n\tactual := len(rep.static)\n\tif len(rep.static) != expected {\n\t\tt.Errorf(\"Expected length '%v' got length '%v'\", expected, actual)\n\t}\n}\n\nfunc TestReplacerMap(t *testing.T) {\n\trep := testReplacer()\n\n\tfor i, tc := range []ReplacerFunc{\n\t\tfunc(key string) (val any, ok bool) {\n\t\t\treturn \"\", false\n\t\t},\n\t\tfunc(key string) (val any, ok bool) {\n\t\t\treturn \"\", false\n\t\t},\n\t} {\n\t\trep.Map(tc)\n\n\t\t// test if function (which listens on specific key) is added by checking length\n\t\tif len(rep.providers) == i+1 {\n\t\t\t// check if the last function is the one we just added\n\t\t\tpTc := fmt.Sprintf(\"%p\", tc)\n\t\t\tpRep := fmt.Sprintf(\"%p\", rep.providers[i])\n\t\t\tif pRep != pTc {\n\t\t\t\tt.Errorf(\"Expected func pointer '%s' got '%s'\", pTc, pRep)\n\t\t\t}\n\t\t} else {\n\t\t\tt.Errorf(\"Expected providers length '%v' got length '%v'\", i+1, len(rep.providers))\n\t\t}\n\t}\n}\n\nfunc TestReplacerNew(t *testing.T) {\n\trepl := NewReplacer()\n\n\tif len(repl.providers) != 3 {\n\t\tt.Errorf(\"Expected providers length '%v' got length '%v'\", 3, len(repl.providers))\n\t}\n\n\t// test if default global replacements are added as the first provider\n\thostname, _ := os.Hostname()\n\twd, _ := os.Getwd()\n\tos.Setenv(\"CADDY_REPLACER_TEST\", \"envtest\")\n\tdefer os.Setenv(\"CADDY_REPLACER_TEST\", \"\")\n\n\tfor _, tc := range []struct {\n\t\tvariable string\n\t\tvalue    string\n\t}{\n\t\t{\n\t\t\tvariable: \"system.hostname\",\n\t\t\tvalue:    hostname,\n\t\t},\n\t\t{\n\t\t\tvariable: \"system.slash\",\n\t\t\tvalue:    string(filepath.Separator),\n\t\t},\n\t\t{\n\t\t\tvariable: \"system.os\",\n\t\t\tvalue:    runtime.GOOS,\n\t\t},\n\t\t{\n\t\t\tvariable: \"system.arch\",\n\t\t\tvalue:    runtime.GOARCH,\n\t\t},\n\t\t{\n\t\t\tvariable: \"system.wd\",\n\t\t\tvalue:    wd,\n\t\t},\n\t\t{\n\t\t\tvariable: \"env.CADDY_REPLACER_TEST\",\n\t\t\tvalue:    \"envtest\",\n\t\t},\n\t} {\n\t\tif val, ok := repl.providers[0].replace(tc.variable); ok {\n\t\t\tif val != tc.value {\n\t\t\t\tt.Errorf(\"Expected value '%s' for key '%s' got '%s'\", tc.value, tc.variable, val)\n\t\t\t}\n\t\t} else {\n\t\t\tt.Errorf(\"Expected key '%s' to be recognized by first provider\", tc.variable)\n\t\t}\n\t}\n\n\t// test if file provider is added as the second provider\n\tfor _, tc := range []struct {\n\t\tvariable string\n\t\tvalue    string\n\t}{\n\t\t{\n\t\t\tvariable: \"file.caddytest/integration/testdata/foo.txt\",\n\t\t\tvalue:    \"foo\",\n\t\t},\n\t\t{\n\t\t\tvariable: \"file.caddytest/integration/testdata/foo_with_trailing_newline.txt\",\n\t\t\tvalue:    \"foo\",\n\t\t},\n\t\t{\n\t\t\tvariable: \"file.caddytest/integration/testdata/foo_with_multiple_trailing_newlines.txt\",\n\t\t\tvalue:    \"foo\" + getEOL(),\n\t\t},\n\t} {\n\t\tif val, ok := repl.providers[1].replace(tc.variable); ok {\n\t\t\tif val != tc.value {\n\t\t\t\tt.Errorf(\"Expected value '%s' for key '%s' got '%s'\", tc.value, tc.variable, val)\n\t\t\t}\n\t\t} else {\n\t\t\tt.Errorf(\"Expected key '%s' to be recognized by second provider\", tc.variable)\n\t\t}\n\t}\n}\n\nfunc getEOL() string {\n\tif os.PathSeparator == '\\\\' {\n\t\treturn \"\\r\\n\" // Windows EOL\n\t}\n\treturn \"\\n\" // Unix and modern macOS EOL\n}\n\nfunc TestReplacerNewWithoutFile(t *testing.T) {\n\trepl := NewReplacer().WithoutFile()\n\n\tfor _, tc := range []struct {\n\t\tvariable string\n\t\tvalue    string\n\t\tnotFound bool\n\t}{\n\t\t{\n\t\t\tvariable: \"file.caddytest/integration/testdata/foo.txt\",\n\t\t\tnotFound: true,\n\t\t},\n\t\t{\n\t\t\tvariable: \"system.os\",\n\t\t\tvalue:    runtime.GOOS,\n\t\t},\n\t} {\n\t\tif val, ok := repl.Get(tc.variable); ok && !tc.notFound {\n\t\t\tif val != tc.value {\n\t\t\t\tt.Errorf(\"Expected value '%s' for key '%s' got '%s'\", tc.value, tc.variable, val)\n\t\t\t}\n\t\t} else if !tc.notFound {\n\t\t\tt.Errorf(\"Expected key '%s' to be recognized\", tc.variable)\n\t\t}\n\t}\n}\n\nfunc BenchmarkReplacer(b *testing.B) {\n\ttype testCase struct {\n\t\tname, input, empty string\n\t}\n\n\trep := testReplacer()\n\trep.Set(\"str\", \"a string\")\n\trep.Set(\"int\", 123.456)\n\n\tfor _, bm := range []testCase{\n\t\t{\n\t\t\tname:  \"no placeholder\",\n\t\t\tinput: `simple string`,\n\t\t},\n\t\t{\n\t\t\tname:  \"string replacement\",\n\t\t\tinput: `str={str}`,\n\t\t},\n\t\t{\n\t\t\tname:  \"int replacement\",\n\t\t\tinput: `int={int}`,\n\t\t},\n\t\t{\n\t\t\tname:  \"placeholder\",\n\t\t\tinput: `{\"json\": \"object\"}`,\n\t\t},\n\t\t{\n\t\t\tname:  \"escaped placeholder\",\n\t\t\tinput: `\\{\"json\": \\{\"nested\": \"{bar}\"\\}\\}`,\n\t\t},\n\t} {\n\t\tb.Run(bm.name, func(b *testing.B) {\n\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\trep.ReplaceAll(bm.input, bm.empty)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc testReplacer() Replacer {\n\treturn Replacer{\n\t\tproviders: make([]replacementProvider, 0),\n\t\tstatic:    make(map[string]any),\n\t\tmapMutex:  &sync.RWMutex{},\n\t}\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: cli\nSource File Path(s): ['cli\\\\args_test.go\\\\source_files\\\\args.go']\nTest File Path: cli\\args_test.go\\args_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: cli\\args_test.go\\source_files\\args.go\npackage cli\n\nimport (\n\t\"fmt\"\n\t\"time\"\n)\n\ntype Args interface {\n\t// Get returns the nth argument, or else a blank string\n\tGet(n int) string\n\t// First returns the first argument, or else a blank string\n\tFirst() string\n\t// Tail returns the rest of the arguments (not the first one)\n\t// or else an empty string slice\n\tTail() []string\n\t// Len returns the length of the wrapped slice\n\tLen() int\n\t// Present checks if there are any arguments present\n\tPresent() bool\n\t// Slice returns a copy of the internal slice\n\tSlice() []string\n}\n\ntype stringSliceArgs struct {\n\tv []string\n}\n\nfunc (a *stringSliceArgs) Get(n int) string {\n\tif len(a.v) > n {\n\t\treturn a.v[n]\n\t}\n\treturn \"\"\n}\n\nfunc (a *stringSliceArgs) First() string {\n\treturn a.Get(0)\n}\n\nfunc (a *stringSliceArgs) Tail() []string {\n\tif a.Len() >= 2 {\n\t\ttail := a.v[1:]\n\t\tret := make([]string, len(tail))\n\t\tcopy(ret, tail)\n\t\treturn ret\n\t}\n\n\treturn []string{}\n}\n\nfunc (a *stringSliceArgs) Len() int {\n\treturn len(a.v)\n}\n\nfunc (a *stringSliceArgs) Present() bool {\n\treturn a.Len() != 0\n}\n\nfunc (a *stringSliceArgs) Slice() []string {\n\tret := make([]string, len(a.v))\n\tcopy(ret, a.v)\n\treturn ret\n}\n\ntype Argument interface {\n\tParse([]string) ([]string, error)\n\tUsage() string\n}\n\n// AnyArguments to differentiate between no arguments(nil) vs aleast one\nvar AnyArguments = []Argument{\n\t&StringArg{\n\t\tMax: -1,\n\t},\n}\n\ntype ArgumentBase[T any, C any, VC ValueCreator[T, C]] struct {\n\tName        string `json:\"name\"`      // the name of this argument\n\tValue       T      `json:\"value\"`     // the default value of this argument\n\tDestination *T     `json:\"-\"`         // the destination point for this argument\n\tValues      *[]T   `json:\"-\"`         // all the values of this argument, only if multiple are supported\n\tUsageText   string `json:\"usageText\"` // the usage text to show\n\tMin         int    `json:\"minTimes\"`  // the min num of occurrences of this argument\n\tMax         int    `json:\"maxTimes\"`  // the max num of occurrences of this argument, set to -1 for unlimited\n\tConfig      C      `json:\"config\"`    // config for this argument similar to Flag Config\n}\n\nfunc (a *ArgumentBase[T, C, VC]) Usage() string {\n\tif a.UsageText != \"\" {\n\t\treturn a.UsageText\n\t}\n\n\tusageFormat := \"\"\n\tif a.Min == 0 {\n\t\tif a.Max == 1 {\n\t\t\tusageFormat = \"[%[1]s]\"\n\t\t} else {\n\t\t\tusageFormat = \"[%[1]s ...]\"\n\t\t}\n\t} else {\n\t\tusageFormat = \"%[1]s [%[1]s ...]\"\n\t}\n\treturn fmt.Sprintf(usageFormat, a.Name)\n}\n\nfunc (a *ArgumentBase[T, C, VC]) Parse(s []string) ([]string, error) {\n\ttracef(\"calling arg%[1] parse with args %[2]\", &a.Name, s)\n\tif a.Max == 0 {\n\t\tfmt.Printf(\"WARNING args %s has max 0, not parsing argument\\n\", a.Name)\n\t\treturn s, nil\n\t}\n\tif a.Max != -1 && a.Min > a.Max {\n\t\tfmt.Printf(\"WARNING args %s has min[%d] > max[%d], not parsing argument\\n\", a.Name, a.Min, a.Max)\n\t\treturn s, nil\n\t}\n\n\tcount := 0\n\tvar vc VC\n\tvar t T\n\tvalue := vc.Create(a.Value, &t, a.Config)\n\tvalues := []T{}\n\n\tfor _, arg := range s {\n\t\tif err := value.Set(arg); err != nil {\n\t\t\treturn s, err\n\t\t}\n\t\tvalues = append(values, value.Get().(T))\n\t\tcount++\n\t\tif count >= a.Max && a.Max > -1 {\n\t\t\tbreak\n\t\t}\n\t}\n\tif count < a.Min {\n\t\treturn s, fmt.Errorf(\"sufficient count of arg %s not provided, given %d expected %d\", a.Name, count, a.Min)\n\t}\n\n\tif a.Values == nil {\n\t\ta.Values = &values\n\t} else if count > 0 {\n\t\t*a.Values = values\n\t}\n\n\tif a.Max == 1 && a.Destination != nil {\n\t\tif len(values) > 0 {\n\t\t\t*a.Destination = values[0]\n\t\t} else {\n\t\t\t*a.Destination = t\n\t\t}\n\t}\n\n\treturn s[count:], nil\n}\n\ntype (\n\tFloatArg     = ArgumentBase[float64, NoConfig, floatValue]\n\tIntArg       = ArgumentBase[int64, IntegerConfig, intValue]\n\tStringArg    = ArgumentBase[string, StringConfig, stringValue]\n\tStringMapArg = ArgumentBase[map[string]string, StringConfig, StringMap]\n\tTimestampArg = ArgumentBase[time.Time, TimestampConfig, timestampValue]\n\tUintArg      = ArgumentBase[uint64, IntegerConfig, uintValue]\n)\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:51:16"}, {"role": "assistant", "content": "```go\npackage cli\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestArgumentsRootCommand(t *testing.T) {\n\tcmd := buildMinimalTestCommand()\n\tvar ival int64\n\tvar fval float64\n\tvar fvals []float64\n\tcmd.Arguments = []Argument{\n\t\t&IntArg{\n\t\t\tName:        \"ia\",\n\t\t\tMin:         1,\n\t\t\tMax:         1,\n\t\t\tDestination: &ival,\n\t\t},\n\t\t&FloatArg{\n\t\t\tName:        \"fa\",\n\t\t\tMin:         0,\n\t\t\tMax:         2,\n\t\t\tDestination: &fval,\n\t\t\tValues:      &fvals,\n\t\t},\n\t}\n\n\trequire.NoError(t, cmd.Run(context.Background(), []string{\"foo\", \"10\"}))\n\trequire.Equal(t, int64(10), ival)\n\n\trequire.NoError(t, cmd.Run(context.Background(), []string{\"foo\", \"12\", \"10.1\"}))\n\trequire.Equal(t, int64(12), ival)\n\trequire.Equal(t, []float64{10.1}, fvals)\n\n\trequire.NoError(t, cmd.Run(context.Background(), []string{\"foo\", \"13\", \"10.1\", \"11.09\"}))\n\trequire.Equal(t, int64(13), ival)\n\trequire.Equal(t, []float64{10.1, 11.09}, fvals)\n\n\trequire.Error(t, errors.New(\"No help topic for '12.1\"), cmd.Run(context.Background(), []string{\"foo\", \"13\", \"10.1\", \"11.09\", \"12.1\"}))\n\trequire.Equal(t, int64(13), ival)\n\trequire.Equal(t, []float64{10.1, 11.09}, fvals)\n\n\tcmd.Arguments = append(cmd.Arguments,\n\t\t&StringArg{\n\t\t\tName: \"sa\",\n\t\t},\n\t\t&UintArg{\n\t\t\tName: \"ua\",\n\t\t\tMin:  2,\n\t\t\tMax:  1, // max is less than min\n\t\t},\n\t)\n\n\trequire.NoError(t, cmd.Run(context.Background(), []string{\"foo\", \"10\"}))\n}\n\nfunc TestArgumentsSubcommand(t *testing.T) {\n\tcmd := buildMinimalTestCommand()\n\tvar ifval int64\n\tvar svals []string\n\tvar tval time.Time\n\tcmd.Commands = []*Command{\n\t\t{\n\t\t\tName: \"subcmd\",\n\t\t\tFlags: []Flag{\n\t\t\t\t&IntFlag{\n\t\t\t\t\tName:        \"foo\",\n\t\t\t\t\tValue:       10,\n\t\t\t\t\tDestination: &ifval,\n\t\t\t\t},\n\t\t\t},\n\t\t\tArguments: []Argument{\n\t\t\t\t&TimestampArg{\n\t\t\t\t\tName:        \"ta\",\n\t\t\t\t\tMin:         1,\n\t\t\t\t\tMax:         1,\n\t\t\t\t\tDestination: &tval,\n\t\t\t\t\tConfig: TimestampConfig{\n\t\t\t\t\t\tLayouts: []string{time.RFC3339},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t&StringArg{\n\t\t\t\t\tName:   \"sa\",\n\t\t\t\t\tMin:    1,\n\t\t\t\t\tMax:    3,\n\t\t\t\t\tValues: &svals,\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\n\trequire.Error(t, errors.New(\"sufficient count of arg sa not provided, given 0 expected 1\"), cmd.Run(context.Background(), []string{\"foo\", \"subcmd\", \"2006-01-02T15:04:05Z\"}))\n\n\trequire.NoError(t, cmd.Run(context.Background(), []string{\"foo\", \"subcmd\", \"2006-01-02T15:04:05Z\", \"fubar\"}))\n\trequire.Equal(t, time.Date(2006, time.January, 2, 15, 4, 5, 0, time.UTC), tval)\n\trequire.Equal(t, []string{\"fubar\"}, svals)\n\n\trequire.NoError(t, cmd.Run(context.Background(), []string{\"foo\", \"subcmd\", \"--foo\", \"100\", \"2006-01-02T15:04:05Z\", \"fubar\", \"some\"}))\n\trequire.Equal(t, int64(100), ifval)\n\trequire.Equal(t, time.Date(2006, time.January, 2, 15, 4, 5, 0, time.UTC), tval)\n\trequire.Equal(t, []string{\"fubar\", \"some\"}, svals)\n}\n\nfunc TestArgsUsage(t *testing.T) {\n\targ := &IntArg{\n\t\tName: \"ia\",\n\t\tMin:  0,\n\t\tMax:  1,\n\t}\n\ttests := []struct {\n\t\tname     string\n\t\tmin      int\n\t\tmax      int\n\t\tusage    string\n\t\texpected string\n\t}{\n\t\t{\n\t\t\tname:     \"optional\",\n\t\t\tmin:      0,\n\t\t\tmax:      1,\n\t\t\texpected: \"[ia]\",\n\t\t},\n\t\t{\n\t\t\tname:     \"optional\",\n\t\t\tmin:      0,\n\t\t\tmax:      1,\n\t\t\tusage:    \"[my optional usage]\",\n\t\t\texpected: \"[my optional usage]\",\n\t\t},\n\t\t{\n\t\t\tname:     \"zero or more\",\n\t\t\tmin:      0,\n\t\t\tmax:      2,\n\t\t\texpected: \"[ia ...]\",\n\t\t},\n\t\t{\n\t\t\tname:     \"one\",\n\t\t\tmin:      1,\n\t\t\tmax:      1,\n\t\t\texpected: \"ia [ia ...]\",\n\t\t},\n\t\t{\n\t\t\tname:     \"many\",\n\t\t\tmin:      2,\n\t\t\tmax:      1,\n\t\t\texpected: \"ia [ia ...]\",\n\t\t},\n\t\t{\n\t\t\tname:     \"many2\",\n\t\t\tmin:      2,\n\t\t\tmax:      0,\n\t\t\texpected: \"ia [ia ...]\",\n\t\t},\n\t\t{\n\t\t\tname:     \"unlimited\",\n\t\t\tmin:      2,\n\t\t\tmax:      -1,\n\t\t\texpected: \"ia [ia ...]\",\n\t\t},\n\t}\n\tfor _, test := range tests {\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\targ.Min, arg.Max, arg.UsageText = test.min, test.max, test.usage\n\t\t\trequire.Equal(t, test.expected, arg.Usage())\n\t\t})\n\t}\n}\n\nfunc TestSingleOptionalArg(t *testing.T) {\n\tcmd := buildMinimalTestCommand()\n\tvar s1 string\n\targ := &StringArg{\n\t\tMin:         0,\n\t\tMax:         1,\n\t\tDestination: &s1,\n\t}\n\tcmd.Arguments = []Argument{\n\t\targ,\n\t}\n\n\trequire.NoError(t, cmd.Run(context.Background(), []string{\"foo\"}))\n\trequire.Equal(t, \"\", s1)\n\n\targ.Value = \"bar\"\n\trequire.NoError(t, cmd.Run(context.Background(), []string{\"foo\"}))\n\trequire.Equal(t, \"bar\", s1)\n\n\trequire.NoError(t, cmd.Run(context.Background(), []string{\"foo\", \"zbar\"}))\n\trequire.Equal(t, \"zbar\", s1)\n}\n\nfunc TestUnboundedArgs(t *testing.T) {\n\targ := &StringArg{\n\t\tMin: 0,\n\t\tMax: -1,\n\t}\n\ttests := []struct {\n\t\tname     string\n\t\targs     []string\n\t\tvalues   []string\n\t\texpected []string\n\t}{\n\t\t{\n\t\t\tname:     \"cmd accepts no args\",\n\t\t\targs:     []string{\"foo\"},\n\t\t\texpected: nil,\n\t\t},\n\t\t{\n\t\t\tname:     \"cmd uses given args\",\n\t\t\targs:     []string{\"foo\", \"bar\", \"baz\"},\n\t\t\texpected: []string{\"bar\", \"baz\"},\n\t\t},\n\t\t{\n\t\t\tname:     \"cmd uses default values\",\n\t\t\targs:     []string{\"foo\"},\n\t\t\tvalues:   []string{\"zbar\", \"zbaz\"},\n\t\t\texpected: []string{\"zbar\", \"zbaz\"},\n\t\t},\n\t\t{\n\t\t\tname:     \"given args override default values\",\n\t\t\targs:     []string{\"foo\", \"bar\", \"baz\"},\n\t\t\tvalues:   []string{\"zbar\", \"zbaz\"},\n\t\t\texpected: []string{\"bar\", \"baz\"},\n\t\t},\n\t}\n\n\tcmd := buildMinimalTestCommand()\n\tcmd.Arguments = []Argument{arg}\n\tfor _, test := range tests {\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\targ.Values = &test.values\n\t\t\trequire.NoError(t, cmd.Run(context.Background(), test.args))\n\t\t\trequire.Equal(t, test.expected, *arg.Values)\n\t\t})\n\t}\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: cli\nSource File Path(s): ['cli\\\\cli_test.go\\\\source_files\\\\cli.go']\nTest File Path: cli\\cli_test.go\\cli_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: cli\\cli_test.go\\source_files\\cli.go\n// Package cli provides a minimal framework for creating and organizing command line\n// Go applications. cli is designed to be easy to understand and write, the most simple\n// cli application can be written as follows:\n//\n//\tfunc main() {\n//\t\t(&cli.Command{}).Run(context.Background(), os.Args)\n//\t}\n//\n// Of course this application does not do much, so let's make this an actual application:\n//\n//\tfunc main() {\n//\t\tcmd := &cli.Command{\n//\t  \t\tName: \"greet\",\n//\t  \t\tUsage: \"say a greeting\",\n//\t  \t\tAction: func(c *cli.Context) error {\n//\t  \t\t\tfmt.Println(\"Greetings\")\n//\t  \t\t\treturn nil\n//\t  \t\t},\n//\t\t}\n//\n//\t\tcmd.Run(context.Background(), os.Args)\n//\t}\npackage cli\n\nimport (\n\t\"fmt\"\n\t\"os\"\n\t\"runtime\"\n\t\"strings\"\n)\n\nvar isTracingOn = os.Getenv(\"URFAVE_CLI_TRACING\") == \"on\"\n\nfunc tracef(format string, a ...any) {\n\tif !isTracingOn {\n\t\treturn\n\t}\n\n\tif !strings.HasSuffix(format, \"\\n\") {\n\t\tformat = format + \"\\n\"\n\t}\n\n\tpc, file, line, _ := runtime.Caller(1)\n\tcf := runtime.FuncForPC(pc)\n\n\tfmt.Fprintf(\n\t\tos.Stderr,\n\t\tstrings.Join([]string{\n\t\t\t\"## URFAVE CLI TRACE \",\n\t\t\tfile,\n\t\t\t\":\",\n\t\t\tfmt.Sprintf(\"%v\", line),\n\t\t\t\" \",\n\t\t\tfmt.Sprintf(\"(%s)\", cf.Name()),\n\t\t\t\" \",\n\t\t\tformat,\n\t\t}, \"\"),\n\t\ta...,\n\t)\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:51:16"}, {"role": "assistant", "content": "```go\npackage cli\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"os\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc expectFileContent(t *testing.T, file, got string) {\n\tdata, err := os.ReadFile(file)\n\t// Ignore windows line endings\n\tdata = bytes.ReplaceAll(data, []byte(\"\\r\\n\"), []byte(\"\\n\"))\n\n\tr := require.New(t)\n\tr.NoError(err)\n\tr.Equal(got, string(data))\n}\n\nfunc buildTestContext(t *testing.T) context.Context {\n\tctx, cancel := context.WithTimeout(context.Background(), 100*time.Millisecond)\n\tt.Cleanup(cancel)\n\n\treturn ctx\n}\n\nfunc TestTracing(t *testing.T) {\n\tolderr := os.Stderr\n\toldtracing := isTracingOn\n\tdefer func() {\n\t\tos.Stderr = olderr\n\t\tisTracingOn = oldtracing\n\t}()\n\n\tfile, err := os.CreateTemp(os.TempDir(), \"cli*\")\n\tassert.NoError(t, err)\n\tos.Stderr = file\n\n\t// Note we cant really set the env since the isTracingOn\n\t// is read at module startup so any changes mid code\n\t// wont take effect\n\tisTracingOn = false\n\ttracef(\"something\")\n\n\tisTracingOn = true\n\ttracef(\"foothing\")\n\n\tassert.NoError(t, file.Close())\n\n\tb, err := os.ReadFile(file.Name())\n\tassert.NoError(t, err)\n\n\tassert.Contains(t, string(b), \"foothing\")\n\tassert.NotContains(t, string(b), \"something\")\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: cli\nSource File Path(s): ['cli\\\\completion_test.go\\\\source_files\\\\completion.go']\nTest File Path: cli\\completion_test.go\\completion_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: cli\\completion_test.go\\source_files\\completion.go\npackage cli\n\nimport (\n\t\"context\"\n\t\"embed\"\n\t\"fmt\"\n\t\"sort\"\n)\n\nconst (\n\tcompletionCommandName = \"completion\"\n\n\t// This flag is supposed to only be used by the completion script itself to generate completions on the fly.\n\tcompletionFlag = \"--generate-shell-completion\"\n)\n\ntype renderCompletion func(cmd *Command, appName string) (string, error)\n\nvar (\n\t//go:embed autocomplete\n\tautoCompleteFS embed.FS\n\n\tshellCompletions = map[string]renderCompletion{\n\t\t\"bash\": func(c *Command, appName string) (string, error) {\n\t\t\tb, err := autoCompleteFS.ReadFile(\"autocomplete/bash_autocomplete\")\n\t\t\treturn fmt.Sprintf(string(b), appName), err\n\t\t},\n\t\t\"zsh\": func(c *Command, appName string) (string, error) {\n\t\t\tb, err := autoCompleteFS.ReadFile(\"autocomplete/zsh_autocomplete\")\n\t\t\treturn fmt.Sprintf(string(b), appName), err\n\t\t},\n\t\t\"fish\": func(c *Command, appName string) (string, error) {\n\t\t\treturn c.ToFishCompletion()\n\t\t},\n\t\t\"pwsh\": func(c *Command, appName string) (string, error) {\n\t\t\tb, err := autoCompleteFS.ReadFile(\"autocomplete/powershell_autocomplete.ps1\")\n\t\t\treturn string(b), err\n\t\t},\n\t}\n)\n\nfunc buildCompletionCommand(appName string) *Command {\n\treturn &Command{\n\t\tName:   completionCommandName,\n\t\tHidden: true,\n\t\tAction: func(ctx context.Context, cmd *Command) error {\n\t\t\treturn printShellCompletion(ctx, cmd, appName)\n\t\t},\n\t}\n}\n\nfunc printShellCompletion(_ context.Context, cmd *Command, appName string) error {\n\tvar shells []string\n\tfor k := range shellCompletions {\n\t\tshells = append(shells, k)\n\t}\n\n\tsort.Strings(shells)\n\n\tif cmd.Args().Len() == 0 {\n\t\treturn Exit(fmt.Sprintf(\"no shell provided for completion command. available shells are %+v\", shells), 1)\n\t}\n\ts := cmd.Args().First()\n\n\trenderCompletion, ok := shellCompletions[s]\n\tif !ok {\n\t\treturn Exit(fmt.Sprintf(\"unknown shell %s, available shells are %+v\", s, shells), 1)\n\t}\n\n\tcompletionScript, err := renderCompletion(cmd, appName)\n\tif err != nil {\n\t\treturn Exit(err, 1)\n\t}\n\n\t_, err = cmd.Writer.Write([]byte(completionScript))\n\tif err != nil {\n\t\treturn Exit(err, 1)\n\t}\n\n\treturn nil\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:51:16"}, {"role": "assistant", "content": "```go\npackage cli\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestCompletionDisable(t *testing.T) {\n\tcmd := &Command{}\n\n\terr := cmd.Run(buildTestContext(t), []string{\"foo\", completionCommandName})\n\tassert.Error(t, err, \"Expected error for no help topic for completion\")\n}\n\nfunc TestCompletionEnable(t *testing.T) {\n\tcmd := &Command{\n\t\tEnableShellCompletion: true,\n\t}\n\n\terr := cmd.Run(buildTestContext(t), []string{\"foo\", completionCommandName})\n\tassert.ErrorContains(t, err, \"no shell provided\")\n}\n\nfunc TestCompletionEnableDiffCommandName(t *testing.T) {\n\tcmd := &Command{\n\t\tEnableShellCompletion:      true,\n\t\tShellCompletionCommandName: \"junky\",\n\t}\n\n\terr := cmd.Run(buildTestContext(t), []string{\"foo\", \"junky\"})\n\tassert.ErrorContains(t, err, \"no shell provided\")\n}\n\nfunc TestCompletionShell(t *testing.T) {\n\tfor k := range shellCompletions {\n\t\tout := &bytes.Buffer{}\n\n\t\tt.Run(k, func(t *testing.T) {\n\t\t\tcmd := &Command{\n\t\t\t\tEnableShellCompletion: true,\n\t\t\t\tWriter:                out,\n\t\t\t}\n\n\t\t\tr := require.New(t)\n\n\t\t\tr.NoError(cmd.Run(buildTestContext(t), []string{\"foo\", completionCommandName, k}))\n\t\t\tr.Containsf(\n\t\t\t\tk, out.String(),\n\t\t\t\t\"Expected output to contain shell name %[1]q\", k,\n\t\t\t)\n\t\t})\n\t}\n}\n\nfunc TestCompletionSubcommand(t *testing.T) {\n\ttests := []struct {\n\t\tname        string\n\t\targs        []string\n\t\tcontains    string\n\t\tmsg         string\n\t\tmsgArgs     []interface{}\n\t\tnotContains bool\n\t}{\n\t\t{\n\t\t\tname:     \"subcommand general completion\",\n\t\t\targs:     []string{\"foo\", \"bar\", completionFlag},\n\t\t\tcontains: \"xyz\",\n\t\t\tmsg:      \"Expected output to contain shell name %[1]q\",\n\t\t\tmsgArgs: []interface{}{\n\t\t\t\t\"xyz\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:     \"subcommand flag completion\",\n\t\t\targs:     []string{\"foo\", \"bar\", \"-\", completionFlag},\n\t\t\tcontains: \"l1\",\n\t\t\tmsg:      \"Expected output to contain shell name %[1]q\",\n\t\t\tmsgArgs: []interface{}{\n\t\t\t\t\"l1\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:     \"subcommand flag no completion\",\n\t\t\targs:     []string{\"foo\", \"bar\", \"--\", completionFlag},\n\t\t\tcontains: \"l1\",\n\t\t\tmsg:      \"Expected output to contain shell name %[1]q\",\n\t\t\tmsgArgs: []interface{}{\n\t\t\t\t\"l1\",\n\t\t\t},\n\t\t\tnotContains: true,\n\t\t},\n\t\t{\n\t\t\tname:     \"sub sub command general completion\",\n\t\t\targs:     []string{\"foo\", \"bar\", \"xyz\", completionFlag},\n\t\t\tcontains: \"-g\",\n\t\t\tmsg:      \"Expected output to contain flag %[1]q\",\n\t\t\tmsgArgs: []interface{}{\n\t\t\t\t\"-g\",\n\t\t\t},\n\t\t\tnotContains: true,\n\t\t},\n\t\t{\n\t\t\tname:     \"sub sub command flag completion\",\n\t\t\targs:     []string{\"foo\", \"bar\", \"xyz\", \"-\", completionFlag},\n\t\t\tcontains: \"-g\",\n\t\t\tmsg:      \"Expected output to contain flag %[1]q\",\n\t\t\tmsgArgs: []interface{}{\n\t\t\t\t\"-g\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:     \"sub sub command no completion\",\n\t\t\targs:     []string{\"foo\", \"bar\", \"xyz\", \"--\", completionFlag},\n\t\t\tcontains: \"-g\",\n\t\t\tmsg:      \"Expected output to contain flag %[1]q\",\n\t\t\tmsgArgs: []interface{}{\n\t\t\t\t\"-g\",\n\t\t\t},\n\t\t\tnotContains: true,\n\t\t},\n\t\t{\n\t\t\tname:     \"sub sub command no completion extra args\",\n\t\t\targs:     []string{\"foo\", \"bar\", \"xyz\", \"--\", \"sargs\", completionFlag},\n\t\t\tcontains: \"-g\",\n\t\t\tmsg:      \"Expected output to contain flag %[1]q\",\n\t\t\tmsgArgs: []interface{}{\n\t\t\t\t\"-g\",\n\t\t\t},\n\t\t\tnotContains: true,\n\t\t},\n\t}\n\n\tfor _, test := range tests {\n\t\tt.Run(test.name, func(t *testing.T) {\n\t\t\tout := &bytes.Buffer{}\n\n\t\t\tcmd := &Command{\n\t\t\t\tEnableShellCompletion: true,\n\t\t\t\tWriter:                out,\n\t\t\t\tCommands: []*Command{\n\t\t\t\t\t{\n\t\t\t\t\t\tName: \"bar\",\n\t\t\t\t\t\tFlags: []Flag{\n\t\t\t\t\t\t\t&StringFlag{\n\t\t\t\t\t\t\t\tName: \"l1\",\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t\tCommands: []*Command{\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tName: \"xyz\",\n\t\t\t\t\t\t\t\tFlags: []Flag{\n\t\t\t\t\t\t\t\t\t&StringFlag{\n\t\t\t\t\t\t\t\t\t\tName: \"g\",\n\t\t\t\t\t\t\t\t\t\tAliases: []string{\n\t\t\t\t\t\t\t\t\t\t\t\"t\",\n\t\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t}\n\n\t\t\tr := require.New(t)\n\n\t\t\tr.NoError(cmd.Run(buildTestContext(t), test.args))\n\t\t\tt.Log(out.String())\n\t\t\tif test.notContains {\n\t\t\t\tr.NotContainsf(out.String(), test.contains, test.msg, test.msgArgs...)\n\t\t\t} else {\n\t\t\t\tr.Containsf(out.String(), test.contains, test.msg, test.msgArgs...)\n\t\t\t}\n\t\t})\n\t}\n}\n\ntype mockWriter struct {\n\terr error\n}\n\nfunc (mw *mockWriter) Write(p []byte) (int, error) {\n\tif mw.err != nil {\n\t\treturn 0, mw.err\n\t}\n\treturn len(p), nil\n}\n\nfunc TestCompletionInvalidShell(t *testing.T) {\n\tcmd := &Command{\n\t\tEnableShellCompletion: true,\n\t}\n\n\tunknownShellName := \"junky-sheell\"\n\terr := cmd.Run(buildTestContext(t), []string{\"foo\", completionCommandName, unknownShellName})\n\tassert.ErrorContains(t, err, \"unknown shell junky-sheell\")\n\n\tenableError := true\n\tshellCompletions[unknownShellName] = func(c *Command, appName string) (string, error) {\n\t\tif enableError {\n\t\t\treturn \"\", fmt.Errorf(\"cant do completion\")\n\t\t}\n\t\treturn \"something\", nil\n\t}\n\tdefer func() {\n\t\tdelete(shellCompletions, unknownShellName)\n\t}()\n\n\terr = cmd.Run(buildTestContext(t), []string{\"foo\", completionCommandName, unknownShellName})\n\tassert.ErrorContains(t, err, \"cant do completion\")\n\n\t// now disable shell completion error\n\tenableError = false\n\tc := cmd.Command(completionCommandName)\n\tassert.NotNil(t, c)\n\tc.Writer = &mockWriter{\n\t\terr: fmt.Errorf(\"writer error\"),\n\t}\n\terr = cmd.Run(buildTestContext(t), []string{\"foo\", completionCommandName, unknownShellName})\n\tassert.ErrorContains(t, err, \"writer error\")\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: cli\nSource File Path(s): ['cli\\\\errors_test.go\\\\source_files\\\\errors.go']\nTest File Path: cli\\errors_test.go\\errors_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: cli\\errors_test.go\\source_files\\errors.go\npackage cli\n\nimport (\n\t\"fmt\"\n\t\"io\"\n\t\"os\"\n\t\"strings\"\n)\n\n// OsExiter is the function used when the app exits. If not set defaults to os.Exit.\nvar OsExiter = os.Exit\n\n// ErrWriter is used to write errors to the user. This can be anything\n// implementing the io.Writer interface and defaults to os.Stderr.\nvar ErrWriter io.Writer = os.Stderr\n\n// MultiError is an error that wraps multiple errors.\ntype MultiError interface {\n\terror\n\tErrors() []error\n}\n\n// newMultiError creates a new MultiError. Pass in one or more errors.\nfunc newMultiError(err ...error) MultiError {\n\tret := multiError(err)\n\treturn &ret\n}\n\ntype multiError []error\n\n// Error implements the error interface.\nfunc (m *multiError) Error() string {\n\terrs := make([]string, len(*m))\n\tfor i, err := range *m {\n\t\terrs[i] = err.Error()\n\t}\n\n\treturn strings.Join(errs, \"\\n\")\n}\n\n// Errors returns a copy of the errors slice\nfunc (m *multiError) Errors() []error {\n\terrs := make([]error, len(*m))\n\tcopy(errs, *m)\n\treturn errs\n}\n\ntype requiredFlagsErr interface {\n\terror\n}\n\ntype errRequiredFlags struct {\n\tmissingFlags []string\n}\n\nfunc (e *errRequiredFlags) Error() string {\n\tif len(e.missingFlags) == 1 {\n\t\treturn fmt.Sprintf(\"Required flag %q not set\", e.missingFlags[0])\n\t}\n\tjoinedMissingFlags := strings.Join(e.missingFlags, \", \")\n\treturn fmt.Sprintf(\"Required flags %q not set\", joinedMissingFlags)\n}\n\ntype mutuallyExclusiveGroup struct {\n\tflag1Name string\n\tflag2Name string\n}\n\nfunc (e *mutuallyExclusiveGroup) Error() string {\n\treturn fmt.Sprintf(\"option %s cannot be set along with option %s\", e.flag1Name, e.flag2Name)\n}\n\ntype mutuallyExclusiveGroupRequiredFlag struct {\n\tflags *MutuallyExclusiveFlags\n}\n\nfunc (e *mutuallyExclusiveGroupRequiredFlag) Error() string {\n\tvar missingFlags []string\n\tfor _, grpf := range e.flags.Flags {\n\t\tvar grpString []string\n\t\tfor _, f := range grpf {\n\t\t\tgrpString = append(grpString, f.Names()...)\n\t\t}\n\t\tmissingFlags = append(missingFlags, strings.Join(grpString, \" \"))\n\t}\n\n\treturn fmt.Sprintf(\"one of these flags needs to be provided: %s\", strings.Join(missingFlags, \", \"))\n}\n\n// ErrorFormatter is the interface that will suitably format the error output\ntype ErrorFormatter interface {\n\tFormat(s fmt.State, verb rune)\n}\n\n// ExitCoder is the interface checked by `App` and `Command` for a custom exit\n// code\ntype ExitCoder interface {\n\terror\n\tExitCode() int\n}\n\ntype exitError struct {\n\texitCode int\n\terr      error\n}\n\n// Exit wraps a message and exit code into an error, which by default is\n// handled with a call to os.Exit during default error handling.\n//\n// This is the simplest way to trigger a non-zero exit code for an App without\n// having to call os.Exit manually. During testing, this behavior can be avoided\n// by overriding the ExitErrHandler function on an App or the package-global\n// OsExiter function.\nfunc Exit(message interface{}, exitCode int) ExitCoder {\n\tvar err error\n\n\tswitch e := message.(type) {\n\tcase ErrorFormatter:\n\t\terr = fmt.Errorf(\"%+v\", message)\n\tcase error:\n\t\terr = e\n\tdefault:\n\t\terr = fmt.Errorf(\"%+v\", message)\n\t}\n\n\treturn &exitError{\n\t\terr:      err,\n\t\texitCode: exitCode,\n\t}\n}\n\nfunc (ee *exitError) Error() string {\n\treturn ee.err.Error()\n}\n\nfunc (ee *exitError) ExitCode() int {\n\treturn ee.exitCode\n}\n\n// HandleExitCoder handles errors implementing ExitCoder by printing their\n// message and calling OsExiter with the given exit code.\n//\n// If the given error instead implements MultiError, each error will be checked\n// for the ExitCoder interface, and OsExiter will be called with the last exit\n// code found, or exit code 1 if no ExitCoder is found.\n//\n// This function is the default error-handling behavior for an App.\nfunc HandleExitCoder(err error) {\n\tif err == nil {\n\t\treturn\n\t}\n\n\tif exitErr, ok := err.(ExitCoder); ok {\n\t\tif err.Error() != \"\" {\n\t\t\tif _, ok := exitErr.(ErrorFormatter); ok {\n\t\t\t\t_, _ = fmt.Fprintf(ErrWriter, \"%+v\\n\", err)\n\t\t\t} else {\n\t\t\t\t_, _ = fmt.Fprintln(ErrWriter, err)\n\t\t\t}\n\t\t}\n\t\tOsExiter(exitErr.ExitCode())\n\t\treturn\n\t}\n\n\tif multiErr, ok := err.(MultiError); ok {\n\t\tcode := handleMultiError(multiErr)\n\t\tOsExiter(code)\n\t\treturn\n\t}\n}\n\nfunc handleMultiError(multiErr MultiError) int {\n\tcode := 1\n\tfor _, merr := range multiErr.Errors() {\n\t\tif multiErr2, ok := merr.(MultiError); ok {\n\t\t\tcode = handleMultiError(multiErr2)\n\t\t} else if merr != nil {\n\t\t\tfmt.Fprintln(ErrWriter, merr)\n\t\t\tif exitErr, ok := merr.(ExitCoder); ok {\n\t\t\t\tcode = exitErr.ExitCode()\n\t\t\t}\n\t\t}\n\t}\n\treturn code\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:51:16"}, {"role": "assistant", "content": "```go\npackage cli\n\nimport (\n\t\"bytes\"\n\t\"errors\"\n\t\"fmt\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestHandleExitCoder_nil(t *testing.T) {\n\texitCode := 0\n\tcalled := false\n\n\tOsExiter = func(rc int) {\n\t\tif !called {\n\t\t\texitCode = rc\n\t\t\tcalled = true\n\t\t}\n\t}\n\n\tdefer func() { OsExiter = fakeOsExiter }()\n\n\tHandleExitCoder(nil)\n\n\tassert.Equal(t, 0, exitCode)\n\tassert.False(t, called)\n}\n\nfunc TestHandleExitCoder_ExitCoder(t *testing.T) {\n\texitCode := 0\n\tcalled := false\n\n\tOsExiter = func(rc int) {\n\t\tif !called {\n\t\t\texitCode = rc\n\t\t\tcalled = true\n\t\t}\n\t}\n\n\tdefer func() { OsExiter = fakeOsExiter }()\n\n\tHandleExitCoder(Exit(\"galactic perimeter breach\", 9))\n\n\tassert.Equal(t, 9, exitCode)\n\tassert.True(t, called)\n}\n\nfunc TestHandleExitCoder_ErrorExitCoder(t *testing.T) {\n\texitCode := 0\n\tcalled := false\n\n\tOsExiter = func(rc int) {\n\t\tif !called {\n\t\t\texitCode = rc\n\t\t\tcalled = true\n\t\t}\n\t}\n\n\tdefer func() { OsExiter = fakeOsExiter }()\n\n\tHandleExitCoder(Exit(errors.New(\"galactic perimeter breach\"), 9))\n\n\tassert.Equal(t, 9, exitCode)\n\tassert.True(t, called)\n}\n\nfunc TestHandleExitCoder_MultiErrorWithExitCoder(t *testing.T) {\n\texitCode := 0\n\tcalled := false\n\n\tOsExiter = func(rc int) {\n\t\tif !called {\n\t\t\texitCode = rc\n\t\t\tcalled = true\n\t\t}\n\t}\n\n\tdefer func() { OsExiter = fakeOsExiter }()\n\n\texitErr := Exit(\"galactic perimeter breach\", 9)\n\texitErr2 := Exit(\"last ExitCoder\", 11)\n\n\terr := newMultiError(errors.New(\"wowsa\"), errors.New(\"egad\"), exitErr, exitErr2)\n\tHandleExitCoder(err)\n\n\tassert.Equal(t, 11, exitCode)\n\tassert.True(t, called)\n}\n\ntype exitFormatter struct {\n\tcode int\n}\n\nfunc (f *exitFormatter) Format(s fmt.State, verb rune) {\n\t_, _ = s.Write([]byte(\"some other special\"))\n}\n\nfunc (f *exitFormatter) ExitCode() int {\n\treturn f.code\n}\n\nfunc (f *exitFormatter) Error() string {\n\treturn fmt.Sprintf(\"my special error code %d\", f.code)\n}\n\nfunc TestHandleExitCoder_ErrorFormatter(t *testing.T) {\n\texitCode := 0\n\tcalled := false\n\n\tOsExiter = func(rc int) {\n\t\tif !called {\n\t\t\texitCode = rc\n\t\t\tcalled = true\n\t\t}\n\t}\n\n\toldWriter := ErrWriter\n\tvar buf bytes.Buffer\n\tErrWriter = &buf\n\tdefer func() {\n\t\tOsExiter = fakeOsExiter\n\t\tErrWriter = oldWriter\n\t}()\n\n\texitErr := Exit(\"galactic perimeter breach\", 9)\n\texitErr2 := Exit(\"last ExitCoder\", 11)\n\texitErr3 := &exitFormatter{code: 12}\n\n\t// add some recursion for multi error to fix test coverage\n\terr := newMultiError(errors.New(\"wowsa\"), errors.New(\"egad\"), exitErr3, newMultiError(exitErr, exitErr2))\n\tHandleExitCoder(err)\n\n\tassert.Equal(t, 11, exitCode)\n\tassert.True(t, called)\n\tassert.Contains(t, buf.String(), \"some other special\")\n}\n\nfunc TestHandleExitCoder_MultiErrorWithoutExitCoder(t *testing.T) {\n\texitCode := 0\n\tcalled := false\n\n\tOsExiter = func(rc int) {\n\t\tif !called {\n\t\t\texitCode = rc\n\t\t\tcalled = true\n\t\t}\n\t}\n\n\tdefer func() { OsExiter = fakeOsExiter }()\n\n\terr := newMultiError(errors.New(\"wowsa\"), errors.New(\"egad\"))\n\tHandleExitCoder(err)\n\n\tassert.Equal(t, 1, exitCode)\n\tassert.True(t, called)\n}\n\n// make a stub to not import pkg/errors\ntype ErrorWithFormat struct {\n\terror\n}\n\nfunc NewErrorWithFormat(m string) *ErrorWithFormat {\n\treturn &ErrorWithFormat{error: errors.New(m)}\n}\n\nfunc (f *ErrorWithFormat) Format(s fmt.State, verb rune) {\n\tfmt.Fprintf(s, \"This the format: %v\", f.error)\n}\n\nfunc TestHandleExitCoder_ErrorWithFormat(t *testing.T) {\n\tcalled := false\n\n\tOsExiter = func(int) {\n\t\tif !called {\n\t\t\tcalled = true\n\t\t}\n\t}\n\tErrWriter = &bytes.Buffer{}\n\n\tdefer func() {\n\t\tOsExiter = fakeOsExiter\n\t\tErrWriter = fakeErrWriter\n\t}()\n\n\terr := Exit(NewErrorWithFormat(\"I am formatted\"), 1)\n\tHandleExitCoder(err)\n\n\tassert.True(t, called)\n\tassert.Equal(t, ErrWriter.(*bytes.Buffer).String(), \"This the format: I am formatted\\n\")\n}\n\nfunc TestHandleExitCoder_MultiErrorWithFormat(t *testing.T) {\n\tcalled := false\n\n\tOsExiter = func(int) {\n\t\tif !called {\n\t\t\tcalled = true\n\t\t}\n\t}\n\tErrWriter = &bytes.Buffer{}\n\n\tdefer func() { OsExiter = fakeOsExiter }()\n\n\terr := newMultiError(NewErrorWithFormat(\"err1\"), NewErrorWithFormat(\"err2\"))\n\tHandleExitCoder(err)\n\n\tassert.True(t, called)\n\tassert.Equal(t, ErrWriter.(*bytes.Buffer).String(), \"This the format: err1\\nThis the format: err2\\n\")\n}\n\nfunc TestMultiErrorErrorsCopy(t *testing.T) {\n\terrList := []error{\n\t\terrors.New(\"foo\"),\n\t\terrors.New(\"bar\"),\n\t\terrors.New(\"baz\"),\n\t}\n\tme := newMultiError(errList...)\n\tassert.Equal(t, errList, me.Errors())\n}\n\nfunc TestErrRequiredFlags_Error(t *testing.T) {\n\tmissingFlags := []string{\"flag1\", \"flag2\"}\n\terr := &errRequiredFlags{missingFlags: missingFlags}\n\texpectedMsg := \"Required flags \\\"flag1, flag2\\\" not set\"\n\tassert.Equal(t, expectedMsg, err.Error())\n\n\tmissingFlags = []string{\"flag1\"}\n\terr = &errRequiredFlags{missingFlags: missingFlags}\n\texpectedMsg = \"Required flag \\\"flag1\\\" not set\"\n\tassert.Equal(t, expectedMsg, err.Error())\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: cli\nSource File Path(s): ['cli\\\\fish_test.go\\\\source_files\\\\fish.go']\nTest File Path: cli\\fish_test.go\\fish_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: cli\\fish_test.go\\source_files\\fish.go\npackage cli\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"io\"\n\t\"strings\"\n\t\"text/template\"\n)\n\n// ToFishCompletion creates a fish completion string for the `*App`\n// The function errors if either parsing or writing of the string fails.\nfunc (cmd *Command) ToFishCompletion() (string, error) {\n\tvar w bytes.Buffer\n\tif err := cmd.writeFishCompletionTemplate(&w); err != nil {\n\t\treturn \"\", err\n\t}\n\treturn w.String(), nil\n}\n\ntype fishCommandCompletionTemplate struct {\n\tCommand     *Command\n\tCompletions []string\n\tAllCommands []string\n}\n\nfunc (cmd *Command) writeFishCompletionTemplate(w io.Writer) error {\n\tconst name = \"cli\"\n\tt, err := template.New(name).Parse(FishCompletionTemplate)\n\tif err != nil {\n\t\treturn err\n\t}\n\tallCommands := []string{}\n\n\t// Add global flags\n\tcompletions := cmd.prepareFishFlags(cmd.VisibleFlags(), allCommands)\n\n\t// Add help flag\n\tif !cmd.HideHelp {\n\t\tcompletions = append(\n\t\t\tcompletions,\n\t\t\tcmd.prepareFishFlags([]Flag{HelpFlag}, allCommands)...,\n\t\t)\n\t}\n\n\t// Add version flag\n\tif !cmd.HideVersion {\n\t\tcompletions = append(\n\t\t\tcompletions,\n\t\t\tcmd.prepareFishFlags([]Flag{VersionFlag}, allCommands)...,\n\t\t)\n\t}\n\n\t// Add commands and their flags\n\tcompletions = append(\n\t\tcompletions,\n\t\tcmd.prepareFishCommands(cmd.VisibleCommands(), &allCommands, []string{})...,\n\t)\n\n\treturn t.ExecuteTemplate(w, name, &fishCommandCompletionTemplate{\n\t\tCommand:     cmd,\n\t\tCompletions: completions,\n\t\tAllCommands: allCommands,\n\t})\n}\n\nfunc (cmd *Command) prepareFishCommands(commands []*Command, allCommands *[]string, previousCommands []string) []string {\n\tcompletions := []string{}\n\tfor _, command := range commands {\n\t\tvar completion strings.Builder\n\t\tcompletion.WriteString(fmt.Sprintf(\n\t\t\t\"complete -r -c %s -n '%s' -a '%s'\",\n\t\t\tcmd.Name,\n\t\t\tcmd.fishSubcommandHelper(previousCommands),\n\t\t\tstrings.Join(command.Names(), \" \"),\n\t\t))\n\n\t\tif command.Usage != \"\" {\n\t\t\tcompletion.WriteString(fmt.Sprintf(\" -d '%s'\",\n\t\t\t\tescapeSingleQuotes(command.Usage)))\n\t\t}\n\n\t\tif !command.HideHelp {\n\t\t\tcompletions = append(\n\t\t\t\tcompletions,\n\t\t\t\tcmd.prepareFishFlags([]Flag{HelpFlag}, command.Names())...,\n\t\t\t)\n\t\t}\n\n\t\t*allCommands = append(*allCommands, command.Names()...)\n\t\tcompletions = append(completions, completion.String())\n\t\tcompletions = append(\n\t\t\tcompletions,\n\t\t\tcmd.prepareFishFlags(command.VisibleFlags(), command.Names())...,\n\t\t)\n\n\t\t// recursively iterate subcommands\n\t\tif len(command.Commands) > 0 {\n\t\t\tcompletions = append(\n\t\t\t\tcompletions,\n\t\t\t\tcmd.prepareFishCommands(\n\t\t\t\t\tcommand.Commands, allCommands, command.Names(),\n\t\t\t\t)...,\n\t\t\t)\n\t\t}\n\t}\n\n\treturn completions\n}\n\nfunc (cmd *Command) prepareFishFlags(flags []Flag, previousCommands []string) []string {\n\tcompletions := []string{}\n\tfor _, f := range flags {\n\t\tcompletion := &strings.Builder{}\n\t\tcompletion.WriteString(fmt.Sprintf(\n\t\t\t\"complete -c %s -n '%s'\",\n\t\t\tcmd.Name,\n\t\t\tcmd.fishSubcommandHelper(previousCommands),\n\t\t))\n\n\t\tfishAddFileFlag(f, completion)\n\n\t\tfor idx, opt := range f.Names() {\n\t\t\tif idx == 0 {\n\t\t\t\tcompletion.WriteString(fmt.Sprintf(\n\t\t\t\t\t\" -l %s\", strings.TrimSpace(opt),\n\t\t\t\t))\n\t\t\t} else {\n\t\t\t\tcompletion.WriteString(fmt.Sprintf(\n\t\t\t\t\t\" -s %s\", strings.TrimSpace(opt),\n\t\t\t\t))\n\t\t\t}\n\t\t}\n\n\t\tif flag, ok := f.(DocGenerationFlag); ok {\n\t\t\tif flag.TakesValue() {\n\t\t\t\tcompletion.WriteString(\" -r\")\n\t\t\t}\n\n\t\t\tif flag.GetUsage() != \"\" {\n\t\t\t\tcompletion.WriteString(fmt.Sprintf(\" -d '%s'\",\n\t\t\t\t\tescapeSingleQuotes(flag.GetUsage())))\n\t\t\t}\n\t\t}\n\n\t\tcompletions = append(completions, completion.String())\n\t}\n\n\treturn completions\n}\n\nfunc fishAddFileFlag(flag Flag, completion *strings.Builder) {\n\tswitch f := flag.(type) {\n\tcase *StringFlag:\n\t\tif f.TakesFile {\n\t\t\treturn\n\t\t}\n\tcase *StringSliceFlag:\n\t\tif f.TakesFile {\n\t\t\treturn\n\t\t}\n\t}\n\tcompletion.WriteString(\" -f\")\n}\n\nfunc (cmd *Command) fishSubcommandHelper(allCommands []string) string {\n\tfishHelper := fmt.Sprintf(\"__fish_%s_no_subcommand\", cmd.Name)\n\tif len(allCommands) > 0 {\n\t\tfishHelper = fmt.Sprintf(\n\t\t\t\"__fish_seen_subcommand_from %s\",\n\t\t\tstrings.Join(allCommands, \" \"),\n\t\t)\n\t}\n\treturn fishHelper\n}\n\nfunc escapeSingleQuotes(input string) string {\n\treturn strings.Replace(input, `'`, `\\'`, -1)\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:51:16"}, {"role": "assistant", "content": "```go\npackage cli\n\nimport (\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestFishCompletion(t *testing.T) {\n\t// Given\n\tcmd := buildExtendedTestCommand()\n\tcmd.Flags = append(cmd.Flags,\n\t\t&StringFlag{\n\t\t\tName:      \"logfile\",\n\t\t\tTakesFile: true,\n\t\t},\n\t\t&StringSliceFlag{\n\t\t\tName:      \"foofile\",\n\t\t\tTakesFile: true,\n\t\t})\n\n\toldTemplate := FishCompletionTemplate\n\tdefer func() { FishCompletionTemplate = oldTemplate }()\n\tFishCompletionTemplate = \"{{something\"\n\n\t// test error case\n\t_, err1 := cmd.ToFishCompletion()\n\tassert.Error(t, err1)\n\n\t// reset the template\n\tFishCompletionTemplate = oldTemplate\n\t// When\n\tres, err := cmd.ToFishCompletion()\n\n\t// Then\n\trequire.NoError(t, err)\n\texpectFileContent(t, \"testdata/expected-fish-full.fish\", res)\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: cli\nSource File Path(s): ['cli\\\\flag_bool_with_inverse_test.go\\\\source_files\\\\flag_bool_with_inverse.go']\nTest File Path: cli\\flag_bool_with_inverse_test.go\\flag_bool_with_inverse_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: cli\\flag_bool_with_inverse_test.go\\source_files\\flag_bool_with_inverse.go\npackage cli\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"fmt\"\n\t\"strings\"\n)\n\nvar DefaultInverseBoolPrefix = \"no-\"\n\ntype BoolWithInverseFlag struct {\n\t// The BoolFlag which the positive and negative flags are generated from\n\t*BoolFlag\n\n\t// The prefix used to indicate a negative value\n\t// Default: `env` becomes `no-env`\n\tInversePrefix string\n\n\tpositiveFlag *BoolFlag\n\tnegativeFlag *BoolFlag\n\n\t// pointers obtained from the embedded bool flag\n\tposDest  *bool\n\tposCount *int\n\n\tnegDest *bool\n}\n\nfunc (parent *BoolWithInverseFlag) Flags() []Flag {\n\treturn []Flag{parent.positiveFlag, parent.negativeFlag}\n}\n\nfunc (parent *BoolWithInverseFlag) IsSet() bool {\n\treturn (*parent.posCount > 0) || (parent.positiveFlag.IsSet() || parent.negativeFlag.IsSet())\n}\n\nfunc (parent *BoolWithInverseFlag) Value() bool {\n\treturn *parent.posDest\n}\n\nfunc (parent *BoolWithInverseFlag) RunAction(ctx context.Context, cmd *Command) error {\n\tif *parent.negDest && *parent.posDest {\n\t\treturn fmt.Errorf(\"cannot set both flags `--%s` and `--%s`\", parent.positiveFlag.Name, parent.negativeFlag.Name)\n\t}\n\n\tif *parent.negDest {\n\t\t_ = cmd.Set(parent.positiveFlag.Name, \"false\")\n\t}\n\n\tif parent.BoolFlag.Action != nil {\n\t\treturn parent.BoolFlag.Action(ctx, cmd, parent.Value())\n\t}\n\n\treturn nil\n}\n\n// Initialize creates a new BoolFlag that has an inverse flag\n//\n// consider a bool flag `--env`, there is no way to set it to false\n// this function allows you to set `--env` or `--no-env` and in the command action\n// it can be determined that BoolWithInverseFlag.IsSet().\nfunc (parent *BoolWithInverseFlag) initialize() {\n\tchild := parent.BoolFlag\n\n\tparent.negDest = new(bool)\n\tif child.Destination != nil {\n\t\tparent.posDest = child.Destination\n\t} else {\n\t\tparent.posDest = new(bool)\n\t}\n\n\tif child.Config.Count != nil {\n\t\tparent.posCount = child.Config.Count\n\t} else {\n\t\tparent.posCount = new(int)\n\t}\n\n\tparent.positiveFlag = child\n\tparent.positiveFlag.Destination = parent.posDest\n\tparent.positiveFlag.Config.Count = parent.posCount\n\n\tparent.negativeFlag = &BoolFlag{\n\t\tCategory:    child.Category,\n\t\tDefaultText: child.DefaultText,\n\t\tSources:     NewValueSourceChain(child.Sources.Chain...),\n\t\tUsage:       child.Usage,\n\t\tRequired:    child.Required,\n\t\tHidden:      child.Hidden,\n\t\tLocal:       child.Local,\n\t\tValue:       child.Value,\n\t\tDestination: parent.negDest,\n\t\tTakesFile:   child.TakesFile,\n\t\tOnlyOnce:    child.OnlyOnce,\n\t\thasBeenSet:  child.hasBeenSet,\n\t\tapplied:     child.applied,\n\t\tcreator:     boolValue{},\n\t\tvalue:       child.value,\n\t}\n\n\t// Set inverse names ex: --env => --no-env\n\tparent.negativeFlag.Name = parent.inverseName()\n\tparent.negativeFlag.Aliases = parent.inverseAliases()\n\n\tif len(child.Sources.EnvKeys()) > 0 {\n\t\tsources := []ValueSource{}\n\n\t\tfor _, envVar := range child.GetEnvVars() {\n\t\t\tsources = append(sources, EnvVar(strings.ToUpper(parent.InversePrefix)+envVar))\n\t\t}\n\t\tparent.negativeFlag.Sources = NewValueSourceChain(sources...)\n\t}\n}\n\nfunc (parent *BoolWithInverseFlag) inverseName() string {\n\treturn parent.inversePrefix() + parent.BoolFlag.Name\n}\n\nfunc (parent *BoolWithInverseFlag) inversePrefix() string {\n\tif parent.InversePrefix == \"\" {\n\t\tparent.InversePrefix = DefaultInverseBoolPrefix\n\t}\n\n\treturn parent.InversePrefix\n}\n\nfunc (parent *BoolWithInverseFlag) inverseAliases() (aliases []string) {\n\tif len(parent.BoolFlag.Aliases) > 0 {\n\t\taliases = make([]string, len(parent.BoolFlag.Aliases))\n\t\tfor idx, alias := range parent.BoolFlag.Aliases {\n\t\t\taliases[idx] = parent.InversePrefix + alias\n\t\t}\n\t}\n\n\treturn\n}\n\nfunc (parent *BoolWithInverseFlag) PostParse() error {\n\tif parent.positiveFlag != nil {\n\t\tif err := parent.positiveFlag.PostParse(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tif parent.negativeFlag != nil {\n\t\tif err := parent.negativeFlag.PostParse(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (parent *BoolWithInverseFlag) Apply(set *flag.FlagSet) error {\n\tif parent.positiveFlag == nil {\n\t\tparent.initialize()\n\t}\n\n\tif err := parent.positiveFlag.Apply(set); err != nil {\n\t\treturn err\n\t}\n\n\tif err := parent.negativeFlag.Apply(set); err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\nfunc (parent *BoolWithInverseFlag) Names() []string {\n\t// Get Names when flag has not been initialized\n\tif parent.positiveFlag == nil {\n\t\treturn append(parent.BoolFlag.Names(), FlagNames(parent.inverseName(), parent.inverseAliases())...)\n\t}\n\n\tif *parent.negDest {\n\t\treturn parent.negativeFlag.Names()\n\t}\n\n\tif *parent.posDest {\n\t\treturn parent.positiveFlag.Names()\n\t}\n\n\treturn append(parent.negativeFlag.Names(), parent.positiveFlag.Names()...)\n}\n\n// String implements the standard Stringer interface.\n//\n// Example for BoolFlag{Name: \"env\"}\n// --[no-]env\t(default: false)\nfunc (parent *BoolWithInverseFlag) String() string {\n\tout := FlagStringer(parent)\n\ti := strings.Index(out, \"\\t\")\n\n\tprefix := \"--\"\n\n\t// single character flags are prefixed with `-` instead of `--`\n\tif len(parent.Name) == 1 {\n\t\tprefix = \"-\"\n\t}\n\n\treturn fmt.Sprintf(\"%s[%s]%s%s\", prefix, parent.inversePrefix(), parent.Name, out[i:])\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:51:16"}, {"role": "assistant", "content": "```go\npackage cli\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"strings\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/require\"\n)\n\nvar errBothEnvFlagsAreSet = fmt.Errorf(\"cannot set both flags `--env` and `--no-env`\")\n\ntype boolWithInverseTestCase struct {\n\targs    []string\n\ttoBeSet bool\n\tvalue   bool\n\terr     error\n\tenvVars map[string]string\n}\n\nfunc (tc *boolWithInverseTestCase) Run(t *testing.T, flagWithInverse *BoolWithInverseFlag) error {\n\tcmd := &Command{\n\t\tFlags:  []Flag{flagWithInverse},\n\t\tAction: func(context.Context, *Command) error { return nil },\n\t}\n\n\tfor key, val := range tc.envVars {\n\t\tt.Setenv(key, val)\n\t}\n\n\terr := cmd.Run(buildTestContext(t), append([]string{\"prog\"}, tc.args...))\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif flagWithInverse.IsSet() != tc.toBeSet {\n\t\treturn fmt.Errorf(\"flag should be set %t, but got %t\", tc.toBeSet, flagWithInverse.IsSet())\n\t}\n\n\tif flagWithInverse.Value() != tc.value {\n\t\treturn fmt.Errorf(\"flag value should be %t, but got %t\", tc.value, flagWithInverse.Value())\n\t}\n\n\treturn nil\n}\n\nfunc runBoolWithInverseFlagTests(t *testing.T, newFlagMethod func() *BoolWithInverseFlag, cases []*boolWithInverseTestCase) error {\n\tfor _, tc := range cases {\n\t\tt.Run(strings.Join(tc.args, \" \")+fmt.Sprintf(\"%[1]v %[2]v %[3]v\", tc.value, tc.toBeSet, tc.err), func(t *testing.T) {\n\t\t\tr := require.New(t)\n\n\t\t\tfl := newFlagMethod()\n\n\t\t\terr := tc.Run(t, fl)\n\t\t\tif err != nil && tc.err == nil {\n\t\t\t\tr.NoError(err)\n\t\t\t}\n\n\t\t\tif err == nil && tc.err != nil {\n\t\t\t\tr.Error(err)\n\t\t\t}\n\n\t\t\tif err != nil && tc.err != nil {\n\t\t\t\tr.EqualError(err, tc.err.Error())\n\t\t\t}\n\t\t})\n\t}\n\n\treturn nil\n}\n\nfunc TestBoolWithInverseBasic(t *testing.T) {\n\tflagMethod := func() *BoolWithInverseFlag {\n\t\treturn &BoolWithInverseFlag{\n\t\t\tBoolFlag: &BoolFlag{\n\t\t\t\tName: \"env\",\n\t\t\t},\n\t\t}\n\t}\n\n\ttestCases := []*boolWithInverseTestCase{\n\t\t{\n\t\t\targs:    []string{\"--no-env\"},\n\t\t\ttoBeSet: true,\n\t\t\tvalue:   false,\n\t\t},\n\t\t{\n\t\t\targs:    []string{\"--env\"},\n\t\t\ttoBeSet: true,\n\t\t\tvalue:   true,\n\t\t},\n\t\t{\n\t\t\ttoBeSet: false,\n\t\t\tvalue:   false,\n\t\t},\n\t\t{\n\t\t\targs: []string{\"--env\", \"--no-env\"},\n\t\t\terr:  errBothEnvFlagsAreSet,\n\t\t},\n\t}\n\n\terr := runBoolWithInverseFlagTests(t, flagMethod, testCases)\n\tif err != nil {\n\t\tt.Error(err)\n\t\treturn\n\t}\n}\n\nfunc TestBoolWithInverseAction(t *testing.T) {\n\tflagMethod := func() *BoolWithInverseFlag {\n\t\treturn &BoolWithInverseFlag{\n\t\t\tBoolFlag: &BoolFlag{\n\t\t\t\tName: \"env\",\n\n\t\t\t\t// Setting env to the opposite to test flag Action is working as intended\n\t\t\t\tAction: func(_ context.Context, cmd *Command, value bool) error {\n\t\t\t\t\tif value {\n\t\t\t\t\t\treturn cmd.Set(\"env\", \"false\")\n\t\t\t\t\t}\n\n\t\t\t\t\treturn cmd.Set(\"env\", \"true\")\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\t}\n\n\ttestCases := []*boolWithInverseTestCase{\n\t\t{\n\t\t\targs:    []string{\"--no-env\"},\n\t\t\ttoBeSet: true,\n\t\t\tvalue:   true,\n\t\t},\n\t\t{\n\t\t\targs:    []string{\"--env\"},\n\t\t\ttoBeSet: true,\n\t\t\tvalue:   false,\n\t\t},\n\n\t\t// This test is not inverse because the flag action is never called\n\t\t{\n\t\t\ttoBeSet: false,\n\t\t\tvalue:   false,\n\t\t},\n\t\t{\n\t\t\targs: []string{\"--env\", \"--no-env\"},\n\t\t\terr:  errBothEnvFlagsAreSet,\n\t\t},\n\t}\n\n\terr := runBoolWithInverseFlagTests(t, flagMethod, testCases)\n\tif err != nil {\n\t\tt.Error(err)\n\t\treturn\n\t}\n}\n\nfunc TestBoolWithInverseAlias(t *testing.T) {\n\tflagMethod := func() *BoolWithInverseFlag {\n\t\treturn &BoolWithInverseFlag{\n\t\t\tBoolFlag: &BoolFlag{\n\t\t\t\tName:    \"env\",\n\t\t\t\tAliases: []string{\"e\", \"do-env\"},\n\t\t\t},\n\t\t}\n\t}\n\n\ttestCases := []*boolWithInverseTestCase{\n\t\t{\n\t\t\targs:    []string{\"--no-e\"},\n\t\t\ttoBeSet: true,\n\t\t\tvalue:   false,\n\t\t},\n\t\t{\n\t\t\targs:    []string{\"--e\"},\n\t\t\ttoBeSet: true,\n\t\t\tvalue:   true,\n\t\t},\n\t\t{\n\t\t\ttoBeSet: false,\n\t\t\tvalue:   false,\n\t\t},\n\t\t{\n\t\t\targs: []string{\"--do-env\", \"--no-do-env\"},\n\t\t\terr:  errBothEnvFlagsAreSet,\n\t\t},\n\t}\n\n\terr := runBoolWithInverseFlagTests(t, flagMethod, testCases)\n\tif err != nil {\n\t\tt.Error(err)\n\t\treturn\n\t}\n}\n\nfunc TestBoolWithInverseEnvVars(t *testing.T) {\n\tflagMethod := func() *BoolWithInverseFlag {\n\t\treturn &BoolWithInverseFlag{\n\t\t\tBoolFlag: &BoolFlag{\n\t\t\t\tName:    \"env\",\n\t\t\t\tSources: EnvVars(\"ENV\"),\n\t\t\t\tLocal:   true,\n\t\t\t},\n\t\t}\n\t}\n\n\ttestCases := []*boolWithInverseTestCase{\n\t\t{\n\t\t\ttoBeSet: true,\n\t\t\tvalue:   false,\n\t\t\tenvVars: map[string]string{\n\t\t\t\t\"NO-ENV\": \"true\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\ttoBeSet: true,\n\t\t\tvalue:   true,\n\t\t\tenvVars: map[string]string{\n\t\t\t\t\"ENV\": \"true\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\ttoBeSet: true,\n\t\t\tvalue:   false,\n\t\t\tenvVars: map[string]string{\n\t\t\t\t\"ENV\": \"false\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\ttoBeSet: false,\n\t\t\tvalue:   false,\n\t\t},\n\t\t{\n\t\t\terr: errBothEnvFlagsAreSet,\n\t\t\tenvVars: map[string]string{\n\t\t\t\t\"ENV\":    \"true\",\n\t\t\t\t\"NO-ENV\": \"true\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\terr: fmt.Errorf(\"could not parse \\\"true_env\\\" as bool value from environment variable \\\"ENV\\\" for flag env: parse error\"),\n\t\t\tenvVars: map[string]string{\n\t\t\t\t\"ENV\": \"true_env\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\terr: fmt.Errorf(\"could not parse \\\"false_env\\\" as bool value from environment variable \\\"NO-ENV\\\" for flag no-env: parse error\"),\n\t\t\tenvVars: map[string]string{\n\t\t\t\t\"NO-ENV\": \"false_env\",\n\t\t\t},\n\t\t},\n\t}\n\n\terr := runBoolWithInverseFlagTests(t, flagMethod, testCases)\n\tif err != nil {\n\t\tt.Error(err)\n\t\treturn\n\t}\n}\n\nfunc TestBoolWithInverseWithPrefix(t *testing.T) {\n\tflagMethod := func() *BoolWithInverseFlag {\n\t\treturn &BoolWithInverseFlag{\n\t\t\tBoolFlag: &BoolFlag{\n\t\t\t\tName: \"env\",\n\t\t\t},\n\t\t\tInversePrefix: \"without-\",\n\t\t}\n\t}\n\n\ttestCases := []*boolWithInverseTestCase{\n\t\t{\n\t\t\targs:    []string{\"--without-env\"},\n\t\t\ttoBeSet: true,\n\t\t\tvalue:   false,\n\t\t},\n\t\t{\n\t\t\targs:    []string{\"--env\"},\n\t\t\ttoBeSet: true,\n\t\t\tvalue:   true,\n\t\t},\n\t\t{\n\t\t\ttoBeSet: false,\n\t\t\tvalue:   false,\n\t\t},\n\t\t{\n\t\t\targs: []string{\"--env\", \"--without-env\"},\n\t\t\terr:  fmt.Errorf(\"cannot set both flags `--env` and `--without-env`\"),\n\t\t},\n\t}\n\n\terr := runBoolWithInverseFlagTests(t, flagMethod, testCases)\n\tif err != nil {\n\t\tt.Error(err)\n\t\treturn\n\t}\n}\n\nfunc TestBoolWithInverseRequired(t *testing.T) {\n\tflagMethod := func() *BoolWithInverseFlag {\n\t\treturn &BoolWithInverseFlag{\n\t\t\tBoolFlag: &BoolFlag{\n\t\t\t\tName:     \"env\",\n\t\t\t\tRequired: true,\n\t\t\t},\n\t\t}\n\t}\n\n\ttestCases := []*boolWithInverseTestCase{\n\t\t{\n\t\t\targs:    []string{\"--no-env\"},\n\t\t\ttoBeSet: true,\n\t\t\tvalue:   false,\n\t\t},\n\t\t{\n\t\t\targs:    []string{\"--env\"},\n\t\t\ttoBeSet: true,\n\t\t\tvalue:   true,\n\t\t},\n\t\t{\n\t\t\ttoBeSet: false,\n\t\t\tvalue:   false,\n\t\t\terr:     fmt.Errorf(`Required flag \"no-env\" not set`),\n\t\t},\n\t\t{\n\t\t\targs: []string{\"--env\", \"--no-env\"},\n\t\t\terr:  errBothEnvFlagsAreSet,\n\t\t},\n\t}\n\n\terr := runBoolWithInverseFlagTests(t, flagMethod, testCases)\n\tif err != nil {\n\t\tt.Error(err)\n\t\treturn\n\t}\n}\n\nfunc TestBoolWithInverseNames(t *testing.T) {\n\tflag := &BoolWithInverseFlag{\n\t\tBoolFlag: &BoolFlag{\n\t\t\tName:     \"env\",\n\t\t\tRequired: true,\n\t\t},\n\t}\n\tnames := flag.Names()\n\n\trequire.Len(t, names, 2)\n\trequire.Equal(t, \"env\", names[0], \"expected first name to be `env`\")\n\trequire.Equal(t, \"no-env\", names[1], \"expected first name to be `no-env`\")\n}\n\nfunc TestBoolWithInverseString(t *testing.T) {\n\ttcs := []struct {\n\t\ttestName      string\n\t\tflagName      string\n\t\trequired      bool\n\t\tusage         string\n\t\tinversePrefix string\n\t\texpected      string\n\t}{\n\t\t{\n\t\t\ttestName: \"empty inverse prefix\",\n\t\t\tflagName: \"\",\n\t\t\trequired: true,\n\t\t\texpected: \"--[no-]\\t\",\n\t\t},\n\t\t{\n\t\t\ttestName: \"single-char flag name\",\n\t\t\tflagName: \"e\",\n\t\t\trequired: true,\n\t\t\texpected: \"-[no-]e\\t\",\n\t\t},\n\t\t{\n\t\t\ttestName: \"multi-char flag name\",\n\t\t\tflagName: \"env\",\n\t\t\trequired: true,\n\t\t\texpected: \"--[no-]env\\t\",\n\t\t},\n\t\t{\n\t\t\ttestName: \"required with usage\",\n\t\t\tflagName: \"env\",\n\t\t\trequired: true,\n\t\t\tusage:    \"env usage\",\n\t\t\texpected: \"--[no-]env\\tenv usage\",\n\t\t},\n\t\t{\n\t\t\ttestName: \"required without usage\",\n\t\t\tflagName: \"env\",\n\t\t\trequired: true,\n\t\t\texpected: \"--[no-]env\\t\",\n\t\t},\n\t\t{\n\t\t\ttestName: \"not required with default usage\",\n\t\t\tflagName: \"env\",\n\t\t\trequired: false,\n\t\t\texpected: \"--[no-]env\\t(default: false)\",\n\t\t},\n\t\t{\n\t\t\ttestName:      \"custom inverse prefix\",\n\t\t\tflagName:      \"env\",\n\t\t\trequired:      true,\n\t\t\tinversePrefix: \"nope-\",\n\t\t\texpected:      \"--[nope-]env\\t\",\n\t\t},\n\t\t{\n\t\t\ttestName: \"empty inverse prefix\",\n\t\t\tflagName: \"env\",\n\t\t\trequired: true,\n\t\t\texpected: \"--[no-]env\\t\",\n\t\t},\n\t}\n\n\tfor _, tc := range tcs {\n\t\tt.Run(tc.testName, func(t *testing.T) {\n\t\t\tflag := &BoolWithInverseFlag{\n\t\t\t\tBoolFlag: &BoolFlag{\n\t\t\t\t\tName:     tc.flagName,\n\t\t\t\t\tUsage:    tc.usage,\n\t\t\t\t\tRequired: tc.required,\n\t\t\t\t},\n\t\t\t\tInversePrefix: tc.inversePrefix,\n\t\t\t}\n\n\t\t\trequire.Equal(t, tc.expected, flag.String())\n\t\t})\n\t}\n}\n\nfunc TestBoolWithInverseDestination(t *testing.T) {\n\tdestination := new(bool)\n\tcount := new(int)\n\n\tflagMethod := func() *BoolWithInverseFlag {\n\t\treturn &BoolWithInverseFlag{\n\t\t\tBoolFlag: &BoolFlag{\n\t\t\t\tName:        \"env\",\n\t\t\t\tDestination: destination,\n\t\t\t\tConfig: BoolConfig{\n\t\t\t\t\tCount: count,\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\t}\n\n\tcheckAndReset := func(expectedCount int, expectedValue bool) error {\n\t\tif *count != expectedCount {\n\t\t\treturn fmt.Errorf(\"expected count to be %d, got %d\", expectedCount, *count)\n\t\t}\n\n\t\tif *destination != expectedValue {\n\t\t\treturn fmt.Errorf(\"expected destination to be %t, got %t\", expectedValue, *destination)\n\t\t}\n\n\t\t*count = 0\n\t\t*destination = false\n\n\t\treturn nil\n\t}\n\n\terr := (&boolWithInverseTestCase{\n\t\targs:    []string{\"--env\"},\n\t\ttoBeSet: true,\n\t\tvalue:   true,\n\t}).Run(t, flagMethod())\n\tif err != nil {\n\t\tt.Error(err)\n\t\treturn\n\t}\n\n\terr = checkAndReset(1, true)\n\tif err != nil {\n\t\tt.Error(err)\n\t\treturn\n\t}\n\n\terr = (&boolWithInverseTestCase{\n\t\targs:    []string{\"--no-env\"},\n\t\ttoBeSet: true,\n\t\tvalue:   false,\n\t}).Run(t, flagMethod())\n\tif err != nil {\n\t\tt.Error(err)\n\t\treturn\n\t}\n\n\terr = checkAndReset(1, false)\n\tif err != nil {\n\t\tt.Error(err)\n\t\treturn\n\t}\n\n\terr = (&boolWithInverseTestCase{\n\t\targs:    []string{},\n\t\ttoBeSet: false,\n\t\tvalue:   false,\n\t}).Run(t, flagMethod())\n\tif err != nil {\n\t\tt.Error(err)\n\t\treturn\n\t}\n\n\terr = checkAndReset(0, false)\n\tif err != nil {\n\t\tt.Error(err)\n\t\treturn\n\t}\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: cli\nSource File Path(s): ['cli\\\\flag_mutex_test.go\\\\source_files\\\\flag_mutex.go']\nTest File Path: cli\\flag_mutex_test.go\\flag_mutex_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: cli\\flag_mutex_test.go\\source_files\\flag_mutex.go\npackage cli\n\n// MutuallyExclusiveFlags defines a mutually exclusive flag group\n// Multiple option paths can be provided out of which\n// only one can be defined on cmdline\n// So for example\n// [ --foo | [ --bar something --darth somethingelse ] ]\ntype MutuallyExclusiveFlags struct {\n\t// Flag list\n\tFlags [][]Flag\n\n\t// whether this group is required\n\tRequired bool\n\n\t// Category to apply to all flags within group\n\tCategory string\n}\n\nfunc (grp MutuallyExclusiveFlags) check(cmd *Command) error {\n\toneSet := false\n\te := &mutuallyExclusiveGroup{}\n\n\tfor _, grpf := range grp.Flags {\n\t\tfor _, f := range grpf {\n\t\t\tfor _, name := range f.Names() {\n\t\t\t\tif cmd.IsSet(name) {\n\t\t\t\t\tif oneSet {\n\t\t\t\t\t\te.flag2Name = name\n\t\t\t\t\t\treturn e\n\t\t\t\t\t}\n\t\t\t\t\te.flag1Name = name\n\t\t\t\t\toneSet = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tif oneSet {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\n\tif !oneSet && grp.Required {\n\t\treturn &mutuallyExclusiveGroupRequiredFlag{flags: &grp}\n\t}\n\treturn nil\n}\n\nfunc (grp MutuallyExclusiveFlags) propagateCategory() {\n\tfor _, grpf := range grp.Flags {\n\t\tfor _, f := range grpf {\n\t\t\tif cf, ok := f.(CategorizableFlag); ok {\n\t\t\t\tcf.SetCategory(grp.Category)\n\t\t\t}\n\t\t}\n\t}\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:51:16"}, {"role": "assistant", "content": "```go\npackage cli\n\nimport (\n\t\"strings\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestFlagMutuallyExclusiveFlags(t *testing.T) {\n\tcmd := &Command{\n\t\tMutuallyExclusiveFlags: []MutuallyExclusiveFlags{\n\t\t\t{\n\t\t\t\tFlags: [][]Flag{\n\t\t\t\t\t{\n\t\t\t\t\t\t&IntFlag{\n\t\t\t\t\t\t\tName: \"i\",\n\t\t\t\t\t\t},\n\t\t\t\t\t\t&StringFlag{\n\t\t\t\t\t\t\tName: \"s\",\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t\t{\n\t\t\t\t\t\t&IntFlag{\n\t\t\t\t\t\t\tName:    \"t\",\n\t\t\t\t\t\t\tAliases: []string{\"ai\"},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n\n\terr := cmd.Run(buildTestContext(t), []string{\"foo\"})\n\tassert.NoError(t, err)\n\n\terr = cmd.Run(buildTestContext(t), []string{\"foo\", \"--i\", \"10\"})\n\tassert.NoError(t, err)\n\n\terr = cmd.Run(buildTestContext(t), []string{\"foo\", \"--i\", \"11\", \"--ai\", \"12\"})\n\tif err == nil {\n\t\tt.Error(\"Expected mutual exclusion error\")\n\t} else if err1, ok := err.(*mutuallyExclusiveGroup); !ok {\n\t\tt.Errorf(\"Got invalid error %v\", err)\n\t} else if !strings.Contains(err1.Error(), \"option i cannot be set along with option ai\") {\n\t\tt.Errorf(\"Invalid error string %v\", err1)\n\t}\n\n\tcmd.MutuallyExclusiveFlags[0].Required = true\n\n\terr = cmd.Run(buildTestContext(t), []string{\"foo\"})\n\tif err == nil {\n\t\tt.Error(\"Required flags error\")\n\t} else if err1, ok := err.(*mutuallyExclusiveGroupRequiredFlag); !ok {\n\t\tt.Errorf(\"Got invalid error %v\", err)\n\t} else if !strings.Contains(err1.Error(), \"one of\") {\n\t\tt.Errorf(\"Invalid error string %v\", err1)\n\t}\n\n\terr = cmd.Run(buildTestContext(t), []string{\"foo\", \"--i\", \"10\"})\n\tassert.NoError(t, err)\n\n\terr = cmd.Run(buildTestContext(t), []string{\"foo\", \"--i\", \"11\", \"--ai\", \"12\"})\n\tif err == nil {\n\t\tt.Error(\"Expected mutual exclusion error\")\n\t} else if err1, ok := err.(*mutuallyExclusiveGroup); !ok {\n\t\tt.Errorf(\"Got invalid error %v\", err)\n\t} else if !strings.Contains(err1.Error(), \"option i cannot be set along with option ai\") {\n\t\tt.Errorf(\"Invalid error string %v\", err1)\n\t}\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: cli\nSource File Path(s): ['cli\\\\sort_test.go\\\\source_files\\\\sort.go']\nTest File Path: cli\\sort_test.go\\sort_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: cli\\sort_test.go\\source_files\\sort.go\npackage cli\n\nimport \"unicode\"\n\n// lexicographicLess compares strings alphabetically considering case.\nfunc lexicographicLess(i, j string) bool {\n\tiRunes := []rune(i)\n\tjRunes := []rune(j)\n\n\tlenShared := len(iRunes)\n\tif lenShared > len(jRunes) {\n\t\tlenShared = len(jRunes)\n\t}\n\n\tfor index := 0; index < lenShared; index++ {\n\t\tir := iRunes[index]\n\t\tjr := jRunes[index]\n\n\t\tif lir, ljr := unicode.ToLower(ir), unicode.ToLower(jr); lir != ljr {\n\t\t\treturn lir < ljr\n\t\t}\n\n\t\tif ir != jr {\n\t\t\treturn ir < jr\n\t\t}\n\t}\n\n\treturn i < j\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:51:16"}, {"role": "assistant", "content": "```go\npackage cli\n\nimport (\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n)\n\nvar lexicographicLessTests = []struct {\n\ti        string\n\tj        string\n\texpected bool\n}{\n\t{\"\", \"a\", true},\n\t{\"a\", \"\", false},\n\t{\"a\", \"a\", false},\n\t{\"a\", \"A\", false},\n\t{\"A\", \"a\", true},\n\t{\"aa\", \"a\", false},\n\t{\"a\", \"aa\", true},\n\t{\"a\", \"b\", true},\n\t{\"a\", \"B\", true},\n\t{\"A\", \"b\", true},\n\t{\"A\", \"B\", true},\n}\n\nfunc TestLexicographicLess(t *testing.T) {\n\tfor _, test := range lexicographicLessTests {\n\t\tactual := lexicographicLess(test.i, test.j)\n\t\tassert.Equal(t, test.expected, actual)\n\t}\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: cli\nSource File Path(s): ['cli\\\\suggestions_test.go\\\\source_files\\\\suggestions.go']\nTest File Path: cli\\suggestions_test.go\\suggestions_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: cli\\suggestions_test.go\\source_files\\suggestions.go\npackage cli\n\nimport (\n\t\"math\"\n)\n\nconst suggestDidYouMeanTemplate = \"Did you mean %q?\"\n\nvar (\n\tSuggestFlag               SuggestFlagFunc    = suggestFlag\n\tSuggestCommand            SuggestCommandFunc = suggestCommand\n\tSuggestDidYouMeanTemplate string             = suggestDidYouMeanTemplate\n)\n\ntype SuggestFlagFunc func(flags []Flag, provided string, hideHelp bool) string\n\ntype SuggestCommandFunc func(commands []*Command, provided string) string\n\n// jaroDistance is the measure of similarity between two strings. It returns a\n// value between 0 and 1, where 1 indicates identical strings and 0 indicates\n// completely different strings.\n//\n// Adapted from https://github.com/xrash/smetrics/blob/5f08fbb34913bc8ab95bb4f2a89a0637ca922666/jaro.go.\nfunc jaroDistance(a, b string) float64 {\n\tif len(a) == 0 && len(b) == 0 {\n\t\treturn 1\n\t}\n\tif len(a) == 0 || len(b) == 0 {\n\t\treturn 0\n\t}\n\n\tlenA := float64(len(a))\n\tlenB := float64(len(b))\n\thashA := make([]bool, len(a))\n\thashB := make([]bool, len(b))\n\tmaxDistance := int(math.Max(0, math.Floor(math.Max(lenA, lenB)/2.0)-1))\n\n\tvar matches float64\n\tfor i := 0; i < len(a); i++ {\n\t\tstart := int(math.Max(0, float64(i-maxDistance)))\n\t\tend := int(math.Min(lenB-1, float64(i+maxDistance)))\n\n\t\tfor j := start; j <= end; j++ {\n\t\t\tif hashB[j] {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif a[i] == b[j] {\n\t\t\t\thashA[i] = true\n\t\t\t\thashB[j] = true\n\t\t\t\tmatches++\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\tif matches == 0 {\n\t\treturn 0\n\t}\n\n\tvar transpositions float64\n\tvar j int\n\tfor i := 0; i < len(a); i++ {\n\t\tif !hashA[i] {\n\t\t\tcontinue\n\t\t}\n\t\tfor !hashB[j] {\n\t\t\tj++\n\t\t}\n\t\tif a[i] != b[j] {\n\t\t\ttranspositions++\n\t\t}\n\t\tj++\n\t}\n\n\ttranspositions /= 2\n\treturn ((matches / lenA) + (matches / lenB) + ((matches - transpositions) / matches)) / 3.0\n}\n\n// jaroWinkler is more accurate when strings have a common prefix up to a\n// defined maximum length.\n//\n// Adapted from https://github.com/xrash/smetrics/blob/5f08fbb34913bc8ab95bb4f2a89a0637ca922666/jaro-winkler.go.\nfunc jaroWinkler(a, b string) float64 {\n\tconst (\n\t\tboostThreshold = 0.7\n\t\tprefixSize     = 4\n\t)\n\tjaroDist := jaroDistance(a, b)\n\tif jaroDist <= boostThreshold {\n\t\treturn jaroDist\n\t}\n\n\tprefix := int(math.Min(float64(len(a)), math.Min(float64(prefixSize), float64(len(b)))))\n\n\tvar prefixMatch float64\n\tfor i := 0; i < prefix; i++ {\n\t\tif a[i] == b[i] {\n\t\t\tprefixMatch++\n\t\t} else {\n\t\t\tbreak\n\t\t}\n\t}\n\treturn jaroDist + 0.1*prefixMatch*(1.0-jaroDist)\n}\n\nfunc suggestFlag(flags []Flag, provided string, hideHelp bool) string {\n\tdistance := 0.0\n\tsuggestion := \"\"\n\n\tfor _, flag := range flags {\n\t\tflagNames := flag.Names()\n\t\tif !hideHelp && HelpFlag != nil {\n\t\t\tflagNames = append(flagNames, HelpFlag.Names()...)\n\t\t}\n\t\tfor _, name := range flagNames {\n\t\t\tnewDistance := jaroWinkler(name, provided)\n\t\t\tif newDistance > distance {\n\t\t\t\tdistance = newDistance\n\t\t\t\tsuggestion = name\n\t\t\t}\n\t\t}\n\t}\n\n\tif len(suggestion) == 1 {\n\t\tsuggestion = \"-\" + suggestion\n\t} else if len(suggestion) > 1 {\n\t\tsuggestion = \"--\" + suggestion\n\t}\n\n\treturn suggestion\n}\n\n// suggestCommand takes a list of commands and a provided string to suggest a\n// command name\nfunc suggestCommand(commands []*Command, provided string) (suggestion string) {\n\tdistance := 0.0\n\tfor _, command := range commands {\n\t\tfor _, name := range append(command.Names(), helpName, helpAlias) {\n\t\t\tnewDistance := jaroWinkler(name, provided)\n\t\t\tif newDistance > distance {\n\t\t\t\tdistance = newDistance\n\t\t\t\tsuggestion = name\n\t\t\t}\n\t\t}\n\t}\n\n\treturn suggestion\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:51:16"}, {"role": "assistant", "content": "```go\npackage cli\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestJaroWinkler(t *testing.T) {\n\t// Given\n\tfor _, testCase := range []struct {\n\t\ta, b     string\n\t\texpected float64\n\t}{\n\t\t{\"\", \"\", 1},\n\t\t{\"a\", \"\", 0},\n\t\t{\"\", \"a\", 0},\n\t\t{\"a\", \"a\", 1},\n\t\t{\"a\", \"b\", 0},\n\t\t{\"aa\", \"aa\", 1},\n\t\t{\"aa\", \"bb\", 0},\n\t\t{\"aaa\", \"aaa\", 1},\n\t\t{\"aa\", \"ab\", 0.6666666666666666},\n\t\t{\"aa\", \"ba\", 0.6666666666666666},\n\t\t{\"ba\", \"aa\", 0.6666666666666666},\n\t\t{\"ab\", \"aa\", 0.6666666666666666},\n\t} {\n\t\t// When\n\t\tres := jaroWinkler(testCase.a, testCase.b)\n\n\t\t// Then\n\t\tassert.Equal(t, testCase.expected, res)\n\t}\n}\n\nfunc TestSuggestFlag(t *testing.T) {\n\t// Given\n\tapp := buildExtendedTestCommand()\n\n\tfor _, testCase := range []struct {\n\t\tprovided, expected string\n\t}{\n\t\t{\"\", \"\"},\n\t\t{\"a\", \"--another-flag\"},\n\t\t{\"hlp\", \"--help\"},\n\t\t{\"k\", \"\"},\n\t\t{\"s\", \"-s\"},\n\t} {\n\t\t// When\n\t\tres := suggestFlag(app.Flags, testCase.provided, false)\n\n\t\t// Then\n\t\tassert.Equal(t, testCase.expected, res)\n\t}\n}\n\nfunc TestSuggestFlagHideHelp(t *testing.T) {\n\t// Given\n\tapp := buildExtendedTestCommand()\n\n\t// When\n\tres := suggestFlag(app.Flags, \"hlp\", true)\n\n\t// Then\n\tassert.Equal(t, \"--fl\", res)\n}\n\nfunc TestSuggestFlagFromError(t *testing.T) {\n\t// Given\n\tapp := buildExtendedTestCommand()\n\n\tfor _, testCase := range []struct {\n\t\tcommand, provided, expected string\n\t}{\n\t\t{\"\", \"hel\", \"--help\"},\n\t\t{\"\", \"soccer\", \"--socket\"},\n\t\t{\"config\", \"anot\", \"--another-flag\"},\n\t} {\n\t\t// When\n\t\tres, _ := app.suggestFlagFromError(\n\t\t\terrors.New(providedButNotDefinedErrMsg+testCase.provided),\n\t\t\ttestCase.command,\n\t\t)\n\n\t\t// Then\n\t\tassert.Equal(t, fmt.Sprintf(SuggestDidYouMeanTemplate+\"\\n\\n\", testCase.expected), res)\n\t}\n}\n\nfunc TestSuggestFlagFromErrorWrongError(t *testing.T) {\n\t// Given\n\tapp := buildExtendedTestCommand()\n\n\t// When\n\t_, err := app.suggestFlagFromError(errors.New(\"invalid\"), \"\")\n\n\t// Then\n\tassert.Error(t, err)\n}\n\nfunc TestSuggestFlagFromErrorWrongCommand(t *testing.T) {\n\t// Given\n\tapp := buildExtendedTestCommand()\n\n\t// When\n\t_, err := app.suggestFlagFromError(\n\t\terrors.New(providedButNotDefinedErrMsg+\"flag\"),\n\t\t\"invalid\",\n\t)\n\n\t// Then\n\tassert.Error(t, err)\n}\n\nfunc TestSuggestFlagFromErrorNoSuggestion(t *testing.T) {\n\t// Given\n\tapp := buildExtendedTestCommand()\n\n\t// When\n\t_, err := app.suggestFlagFromError(\n\t\terrors.New(providedButNotDefinedErrMsg+\"\"),\n\t\t\"\",\n\t)\n\n\t// Then\n\tassert.Error(t, err)\n}\n\nfunc TestSuggestCommand(t *testing.T) {\n\t// Given\n\tapp := buildExtendedTestCommand()\n\n\tfor _, testCase := range []struct {\n\t\tprovided, expected string\n\t}{\n\t\t{\"\", \"\"},\n\t\t{\"conf\", \"config\"},\n\t\t{\"i\", \"i\"},\n\t\t{\"information\", \"info\"},\n\t\t{\"inf\", \"info\"},\n\t\t{\"con\", \"config\"},\n\t\t{\"not-existing\", \"info\"},\n\t} {\n\t\t// When\n\t\tres := suggestCommand(app.Commands, testCase.provided)\n\n\t\t// Then\n\t\tassert.Equal(t, testCase.expected, res)\n\t}\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: cli\nSource File Path(s): ['cli\\\\value_source_test.go\\\\source_files\\\\value_source.go']\nTest File Path: cli\\value_source_test.go\\value_source_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: cli\\value_source_test.go\\source_files\\value_source.go\npackage cli\n\nimport (\n\t\"fmt\"\n\t\"os\"\n\t\"strings\"\n)\n\n// ValueSource is a source which can be used to look up a value,\n// typically for use with a cli.Flag\ntype ValueSource interface {\n\tfmt.Stringer\n\tfmt.GoStringer\n\n\t// Lookup returns the value from the source and if it was found\n\t// or returns an empty string and false\n\tLookup() (string, bool)\n}\n\n// EnvValueSource is to specifically detect env sources when\n// printing help text\ntype EnvValueSource interface {\n\tIsFromEnv() bool\n\tKey() string\n}\n\n// MapSource is a source which can be used to look up a value\n// based on a key\n// typically for use with a cli.Flag\ntype MapSource interface {\n\tfmt.Stringer\n\tfmt.GoStringer\n\n\t// Lookup returns the value from the source based on key\n\t// and if it was found\n\t// or returns an empty string and false\n\tLookup(string) (any, bool)\n}\n\n// ValueSourceChain contains an ordered series of ValueSource that\n// allows for lookup where the first ValueSource to resolve is\n// returned\ntype ValueSourceChain struct {\n\tChain []ValueSource\n}\n\nfunc NewValueSourceChain(src ...ValueSource) ValueSourceChain {\n\treturn ValueSourceChain{\n\t\tChain: src,\n\t}\n}\n\nfunc (vsc *ValueSourceChain) Append(other ValueSourceChain) {\n\tvsc.Chain = append(vsc.Chain, other.Chain...)\n}\n\nfunc (vsc *ValueSourceChain) EnvKeys() []string {\n\tvals := []string{}\n\n\tfor _, src := range vsc.Chain {\n\t\tif v, ok := src.(EnvValueSource); ok && v.IsFromEnv() {\n\t\t\tvals = append(vals, v.Key())\n\t\t}\n\t}\n\n\treturn vals\n}\n\nfunc (vsc *ValueSourceChain) String() string {\n\ts := []string{}\n\n\tfor _, vs := range vsc.Chain {\n\t\ts = append(s, vs.String())\n\t}\n\n\treturn strings.Join(s, \",\")\n}\n\nfunc (vsc *ValueSourceChain) GoString() string {\n\ts := []string{}\n\n\tfor _, vs := range vsc.Chain {\n\t\ts = append(s, vs.GoString())\n\t}\n\n\treturn fmt.Sprintf(\"&ValueSourceChain{Chain:{%[1]s}}\", strings.Join(s, \",\"))\n}\n\nfunc (vsc *ValueSourceChain) Lookup() (string, bool) {\n\ts, _, ok := vsc.LookupWithSource()\n\treturn s, ok\n}\n\nfunc (vsc *ValueSourceChain) LookupWithSource() (string, ValueSource, bool) {\n\tfor _, src := range vsc.Chain {\n\t\tif value, found := src.Lookup(); found {\n\t\t\treturn value, src, true\n\t\t}\n\t}\n\n\treturn \"\", nil, false\n}\n\n// envVarValueSource encapsulates a ValueSource from an environment variable\ntype envVarValueSource struct {\n\tkey string\n}\n\nfunc (e *envVarValueSource) Lookup() (string, bool) {\n\treturn os.LookupEnv(strings.TrimSpace(string(e.key)))\n}\n\nfunc (e *envVarValueSource) IsFromEnv() bool {\n\treturn true\n}\n\nfunc (e *envVarValueSource) Key() string {\n\treturn e.key\n}\n\nfunc (e *envVarValueSource) String() string { return fmt.Sprintf(\"environment variable %[1]q\", e.key) }\nfunc (e *envVarValueSource) GoString() string {\n\treturn fmt.Sprintf(\"&envVarValueSource{Key:%[1]q}\", e.key)\n}\n\nfunc EnvVar(key string) ValueSource {\n\treturn &envVarValueSource{\n\t\tkey: key,\n\t}\n}\n\n// EnvVars is a helper function to encapsulate a number of\n// envVarValueSource together as a ValueSourceChain\nfunc EnvVars(keys ...string) ValueSourceChain {\n\tvsc := ValueSourceChain{Chain: []ValueSource{}}\n\n\tfor _, key := range keys {\n\t\tvsc.Chain = append(vsc.Chain, EnvVar(key))\n\t}\n\n\treturn vsc\n}\n\n// fileValueSource encapsulates a ValueSource from a file\ntype fileValueSource struct {\n\tPath string\n}\n\nfunc (f *fileValueSource) Lookup() (string, bool) {\n\tdata, err := os.ReadFile(f.Path)\n\treturn string(data), err == nil\n}\n\nfunc (f *fileValueSource) String() string { return fmt.Sprintf(\"file %[1]q\", f.Path) }\nfunc (f *fileValueSource) GoString() string {\n\treturn fmt.Sprintf(\"&fileValueSource{Path:%[1]q}\", f.Path)\n}\n\nfunc File(path string) ValueSource {\n\treturn &fileValueSource{Path: path}\n}\n\n// Files is a helper function to encapsulate a number of\n// fileValueSource together as a ValueSourceChain\nfunc Files(paths ...string) ValueSourceChain {\n\tvsc := ValueSourceChain{Chain: []ValueSource{}}\n\n\tfor _, path := range paths {\n\t\tvsc.Chain = append(vsc.Chain, File(path))\n\t}\n\n\treturn vsc\n}\n\ntype mapSource struct {\n\tname string\n\tm    map[any]any\n}\n\nfunc NewMapSource(name string, m map[any]any) MapSource {\n\treturn &mapSource{\n\t\tname: name,\n\t\tm:    m,\n\t}\n}\n\nfunc (ms *mapSource) String() string { return fmt.Sprintf(\"map source %[1]q\", ms.name) }\nfunc (ms *mapSource) GoString() string {\n\treturn fmt.Sprintf(\"&mapSource{name:%[1]q}\", ms.name)\n}\n\n// Lookup returns a value from the map source. The lookup name may be a dot-separated path into the map.\n// If that is the case, it will recursively traverse the map based on the '.' delimited sections to find\n// a nested value for the key.\nfunc (ms *mapSource) Lookup(name string) (any, bool) {\n\tsections := strings.Split(name, \".\")\n\tif name == \"\" || len(sections) == 0 {\n\t\treturn nil, false\n\t}\n\n\tnode := ms.m\n\n\t// traverse into the map based on the dot-separated sections\n\tif len(sections) >= 2 { // the last section is the value we want, we will return it directly at the end\n\t\tfor _, section := range sections[:len(sections)-1] {\n\t\t\tchild, ok := node[section]\n\t\t\tif !ok {\n\t\t\t\treturn nil, false\n\t\t\t}\n\n\t\t\tswitch child := child.(type) {\n\t\t\tcase map[string]any:\n\t\t\t\tnode = make(map[any]any, len(child))\n\t\t\t\tfor k, v := range child {\n\t\t\t\t\tnode[k] = v\n\t\t\t\t}\n\t\t\tcase map[any]any:\n\t\t\t\tnode = child\n\t\t\tdefault:\n\t\t\t\treturn nil, false\n\t\t\t}\n\t\t}\n\t}\n\n\tif val, ok := node[sections[len(sections)-1]]; ok {\n\t\treturn val, true\n\t}\n\treturn nil, false\n}\n\ntype mapValueSource struct {\n\tkey string\n\tms  MapSource\n}\n\nfunc NewMapValueSource(key string, ms MapSource) ValueSource {\n\treturn &mapValueSource{\n\t\tkey: key,\n\t\tms:  ms,\n\t}\n}\n\nfunc (mvs *mapValueSource) String() string {\n\treturn fmt.Sprintf(\"key %[1]q from %[2]s\", mvs.key, mvs.ms.String())\n}\n\nfunc (mvs *mapValueSource) GoString() string {\n\treturn fmt.Sprintf(\"&mapValueSource{key:%[1]q, src:%[2]s}\", mvs.key, mvs.ms.GoString())\n}\n\nfunc (mvs *mapValueSource) Lookup() (string, bool) {\n\tif v, ok := mvs.ms.Lookup(mvs.key); !ok {\n\t\treturn \"\", false\n\t} else {\n\t\treturn fmt.Sprintf(\"%+v\", v), true\n\t}\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:51:16"}, {"role": "assistant", "content": "```go\npackage cli\n\nimport (\n\t\"fmt\"\n\t\"math/rand\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestZeroValueSourceChain(t *testing.T) {\n\tvar vc ValueSourceChain\n\tassert.Empty(t, vc.EnvKeys())\n\tassert.NotEmpty(t, vc.GoString())\n\tassert.Empty(t, vc.Chain)\n\tassert.Empty(t, vc.String())\n}\n\nfunc TestEnvVarValueSource(t *testing.T) {\n\tt.Run(\"implements ValueSource\", func(t *testing.T) {\n\t\tsrc := EnvVar(\"foo\")\n\t\trequire.Implements(t, (*ValueSource)(nil), src)\n\n\t\tt.Run(\"not found\", func(t *testing.T) {\n\t\t\tt.Setenv(\"foo\", \"bar\")\n\n\t\t\tsrc := EnvVar(\"foo_1\")\n\t\t\t_, ok := src.Lookup()\n\t\t\trequire.False(t, ok)\n\t\t})\n\n\t\tt.Run(\"found\", func(t *testing.T) {\n\t\t\tt.Setenv(\"foo\", \"bar\")\n\n\t\t\tr := require.New(t)\n\t\t\tsrc := EnvVar(\"foo\")\n\n\t\t\tstr, ok := src.Lookup()\n\t\t\tr.True(ok)\n\t\t\tr.Equal(str, \"bar\")\n\t\t})\n\t})\n\n\tt.Run(\"implements fmt.Stringer\", func(t *testing.T) {\n\t\tsrc := EnvVar(\"foo\")\n\t\tr := require.New(t)\n\n\t\tr.Implements((*fmt.Stringer)(nil), src)\n\t\tr.Equal(\"environment variable \\\"foo\\\"\", src.String())\n\t})\n\n\tt.Run(\"implements fmt.GoStringer\", func(t *testing.T) {\n\t\tsrc := EnvVar(\"foo\")\n\t\tr := require.New(t)\n\n\t\tr.Implements((*fmt.GoStringer)(nil), src)\n\t\tr.Equal(\"&envVarValueSource{Key:\\\"foo\\\"}\", src.GoString())\n\t})\n}\n\nfunc TestEnvVars(t *testing.T) {\n\tt.Setenv(\"myfoo\", \"mybar\")\n\n\tsource := EnvVars(\"foo1\", \"myfoo\")\n\tstr, src, ok := source.LookupWithSource()\n\n\tr := require.New(t)\n\tr.True(ok)\n\tr.Equal(str, \"mybar\")\n\tr.Contains(src.String(), \"\\\"myfoo\\\"\")\n}\n\nfunc TestFileValueSource(t *testing.T) {\n\tt.Run(\"implements ValueSource\", func(t *testing.T) {\n\t\tr := require.New(t)\n\n\t\tr.Implements((*ValueSource)(nil), &fileValueSource{})\n\n\t\tt.Run(\"not found\", func(t *testing.T) {\n\t\t\tsrc := File(fmt.Sprintf(\"junk_file_name-%[1]v\", rand.Int()))\n\t\t\t_, ok := src.Lookup()\n\t\t\tr.False(ok)\n\t\t})\n\n\t\tfileName := filepath.Join(os.TempDir(), fmt.Sprintf(\"urfave-cli-testing-existing_file-%[1]v\", rand.Int()))\n\t\tt.Cleanup(func() { _ = os.Remove(fileName) })\n\n\t\tr.Nil(os.WriteFile(fileName, []byte(\"pita\"), 0o644))\n\n\t\tt.Run(\"found\", func(t *testing.T) {\n\t\t\tsrc := File(fileName)\n\t\t\tstr, ok := src.Lookup()\n\t\t\tr.True(ok)\n\t\t\tr.Equal(\"pita\", str)\n\t\t})\n\t})\n\n\tt.Run(\"implements fmt.Stringer\", func(t *testing.T) {\n\t\tsrc := File(\"/dev/null\")\n\t\tr := require.New(t)\n\n\t\tr.Implements((*ValueSource)(nil), src)\n\t\tr.Equal(\"file \\\"/dev/null\\\"\", src.String())\n\t})\n\n\tt.Run(\"implements fmt.GoStringer\", func(t *testing.T) {\n\t\tsrc := File(\"/dev/null\")\n\t\tr := require.New(t)\n\n\t\tr.Implements((*ValueSource)(nil), src)\n\t\tr.Equal(\"&fileValueSource{Path:\\\"/dev/null\\\"}\", src.GoString())\n\t})\n}\n\nfunc TestFilePaths(t *testing.T) {\n\tr := require.New(t)\n\n\tfileName := filepath.Join(os.TempDir(), fmt.Sprintf(\"urfave-cli-tests-some_file_name_%[1]v\", rand.Int()))\n\tt.Cleanup(func() { _ = os.Remove(fileName) })\n\n\tr.Nil(os.WriteFile(fileName, []byte(\"Hello\"), 0o644))\n\n\tsources := Files(\"junk_file_name\", fileName)\n\tstr, src, ok := sources.LookupWithSource()\n\tr.True(ok)\n\tr.Equal(str, \"Hello\")\n\tr.Contains(src.String(), fmt.Sprintf(\"%[1]q\", fileName))\n}\n\nfunc TestValueSourceChainEnvKeys(t *testing.T) {\n\tchain := NewValueSourceChain(\n\t\t&staticValueSource{\"hello\"},\n\t)\n\tchain.Append(EnvVars(\"foo\", \"bar\"))\n\n\tr := require.New(t)\n\tr.Equal([]string{\"foo\", \"bar\"}, chain.EnvKeys())\n}\n\nfunc TestValueSourceChain(t *testing.T) {\n\tt.Run(\"implements ValueSource\", func(t *testing.T) {\n\t\tvsc := &ValueSourceChain{}\n\t\tr := require.New(t)\n\n\t\tr.Implements((*ValueSource)(nil), vsc)\n\n\t\t_, ok := vsc.Lookup()\n\t\tr.False(ok)\n\t})\n\n\tt.Run(\"implements fmt.GoStringer\", func(t *testing.T) {\n\t\tvsc := &ValueSourceChain{}\n\t\tr := require.New(t)\n\n\t\tr.Implements((*fmt.GoStringer)(nil), vsc)\n\t\tr.Equal(\"&ValueSourceChain{Chain:{}}\", vsc.GoString())\n\n\t\tvsc1 := NewValueSourceChain(&staticValueSource{v: \"yahtzee\"},\n\t\t\t&staticValueSource{v: \"matzoh\"},\n\t\t)\n\t\tr.Equal(\"&ValueSourceChain{Chain:{&staticValueSource{v:\\\"yahtzee\\\"},&staticValueSource{v:\\\"matzoh\\\"}}}\", vsc1.GoString())\n\t})\n\n\tt.Run(\"implements fmt.Stringer\", func(t *testing.T) {\n\t\tvsc := &ValueSourceChain{}\n\t\tr := require.New(t)\n\n\t\tr.Implements((*fmt.Stringer)(nil), vsc)\n\t\tr.Equal(\"\", vsc.String())\n\n\t\tvsc1 := NewValueSourceChain(\n\t\t\t&staticValueSource{v: \"soup\"},\n\t\t\t&staticValueSource{v: \"salad\"},\n\t\t\t&staticValueSource{v: \"pumpkins\"},\n\t\t)\n\t\tr.Equal(\"soup,salad,pumpkins\", vsc1.String())\n\t})\n}\n\ntype staticValueSource struct {\n\tv string\n}\n\nfunc (svs *staticValueSource) GoString() string {\n\treturn fmt.Sprintf(\"&staticValueSource{v:%[1]q}\", svs.v)\n}\nfunc (svs *staticValueSource) String() string         { return svs.v }\nfunc (svs *staticValueSource) Lookup() (string, bool) { return svs.v, true }\n\nfunc TestMapValueSource(t *testing.T) {\n\ttests := []struct {\n\t\tname  string\n\t\tm     map[any]any\n\t\tkey   string\n\t\tval   string\n\t\tfound bool\n\t}{\n\t\t{\n\t\t\tname: \"No map no key\",\n\t\t},\n\t\t{\n\t\t\tname: \"No map with key\",\n\t\t\tkey:  \"foo\",\n\t\t},\n\t\t{\n\t\t\tname: \"Empty map no key\",\n\t\t\tm:    map[any]any{},\n\t\t},\n\t\t{\n\t\t\tname: \"Empty map with key\",\n\t\t\tkey:  \"foo\",\n\t\t\tm:    map[any]any{},\n\t\t},\n\t\t{\n\t\t\tname: \"Level 1 no key\",\n\t\t\tkey:  \".foob\",\n\t\t\tm: map[any]any{\n\t\t\t\t\"foo\": 10,\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"Level 1\",\n\t\t\tkey:  \"foobar\",\n\t\t\tm: map[any]any{\n\t\t\t\t\"foobar\": 10,\n\t\t\t},\n\t\t\tval:   \"10\",\n\t\t\tfound: true,\n\t\t},\n\t\t{\n\t\t\tname: \"Level 2\",\n\t\t\tkey:  \"foo.bar\",\n\t\t\tm: map[any]any{\n\t\t\t\t\"foo\": map[any]any{\n\t\t\t\t\t\"bar\": 10,\n\t\t\t\t},\n\t\t\t},\n\t\t\tval:   \"10\",\n\t\t\tfound: true,\n\t\t},\n\t\t{\n\t\t\tname: \"Level 2 invalid key\",\n\t\t\tkey:  \"foo.bar1\",\n\t\t\tm: map[any]any{\n\t\t\t\t\"foo\": map[any]any{\n\t\t\t\t\t\"bar\": \"10\",\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"Level 2 string map type\",\n\t\t\tkey:  \"foo.bar1\",\n\t\t\tm: map[any]any{\n\t\t\t\t\"foo\": map[string]any{\n\t\t\t\t\t\"bar\": \"10\",\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"Level 3 no entry\",\n\t\t\tkey:  \"foo.bar.t\",\n\t\t\tm: map[any]any{\n\t\t\t\t\"foo\": map[any]any{\n\t\t\t\t\t\"bar\": \"sss\",\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"Level 3\",\n\t\t\tkey:  \"foo.bar.t\",\n\t\t\tm: map[any]any{\n\t\t\t\t\"foo\": map[any]any{\n\t\t\t\t\t\"bar\": map[any]any{\n\t\t\t\t\t\t\"t\": \"sss\",\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\tval:   \"sss\",\n\t\t\tfound: true,\n\t\t},\n\t\t{\n\t\t\tname: \"Level 3 invalid key\",\n\t\t\tkey:  \"foo.bar.t\",\n\t\t\tm: map[any]any{\n\t\t\t\t\"foo\": map[any]any{\n\t\t\t\t\t\"bar\": map[any]any{\n\t\t\t\t\t\t\"t1\": 10,\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"Level 4 no entry\",\n\t\t\tkey:  \"foo.bar.t.gh\",\n\t\t\tm: map[any]any{\n\t\t\t\t\"foo\": map[any]any{\n\t\t\t\t\t\"bar\": map[any]any{\n\t\t\t\t\t\t\"t1\": 10,\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"Level 4 slice entry\",\n\t\t\tkey:  \"foo.bar.t.gh\",\n\t\t\tm: map[any]any{\n\t\t\t\t\"foo\": map[any]any{\n\t\t\t\t\t\"bar\": map[string]any{\n\t\t\t\t\t\t\"t\": map[any]any{\n\t\t\t\t\t\t\t\"gh\": []int{10},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\tval:   \"[10]\",\n\t\t\tfound: true,\n\t\t},\n\t}\n\n\tfor _, test := range tests {\n\t\tt.Run(test.key, func(t *testing.T) {\n\t\t\tms := NewMapSource(\"test\", test.m)\n\t\t\tm := NewMapValueSource(test.key, ms)\n\t\t\tval, b := m.Lookup()\n\t\t\tif !test.found {\n\t\t\t\tassert.False(t, b)\n\t\t\t} else {\n\t\t\t\tassert.True(t, b)\n\t\t\t\tassert.Equal(t, val, test.val)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestMapValueSourceStringer(t *testing.T) {\n\tm := map[any]any{\n\t\t\"foo\": map[any]any{\n\t\t\t\"bar\": 10,\n\t\t},\n\t}\n\tmvs := NewMapValueSource(\"bar\", NewMapSource(\"test\", m))\n\n\tassert.Equal(t, `&mapValueSource{key:\"bar\", src:&mapSource{name:\"test\"}}`, mvs.GoString())\n\tassert.Equal(t, `key \"bar\" from map source \"test\"`, mvs.String())\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: cobra\nSource File Path(s): ['cobra\\\\active_help_test.go\\\\source_files\\\\active_help.go']\nTest File Path: cobra\\active_help_test.go\\active_help_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: cobra\\active_help_test.go\\source_files\\active_help.go\n// Copyright 2013-2023 The Cobra Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage cobra\n\nimport (\n\t\"fmt\"\n\t\"os\"\n)\n\nconst (\n\tactiveHelpMarker = \"_activeHelp_ \"\n\t// The below values should not be changed: programs will be using them explicitly\n\t// in their user documentation, and users will be using them explicitly.\n\tactiveHelpEnvVarSuffix  = \"ACTIVE_HELP\"\n\tactiveHelpGlobalEnvVar  = configEnvVarGlobalPrefix + \"_\" + activeHelpEnvVarSuffix\n\tactiveHelpGlobalDisable = \"0\"\n)\n\n// AppendActiveHelp adds the specified string to the specified array to be used as ActiveHelp.\n// Such strings will be processed by the completion script and will be shown as ActiveHelp\n// to the user.\n// The array parameter should be the array that will contain the completions.\n// This function can be called multiple times before and/or after completions are added to\n// the array.  Each time this function is called with the same array, the new\n// ActiveHelp line will be shown below the previous ones when completion is triggered.\nfunc AppendActiveHelp(compArray []Completion, activeHelpStr string) []Completion {\n\treturn append(compArray, fmt.Sprintf(\"%s%s\", activeHelpMarker, activeHelpStr))\n}\n\n// GetActiveHelpConfig returns the value of the ActiveHelp environment variable\n// <PROGRAM>_ACTIVE_HELP where <PROGRAM> is the name of the root command in upper\n// case, with all non-ASCII-alphanumeric characters replaced by `_`.\n// It will always return \"0\" if the global environment variable COBRA_ACTIVE_HELP\n// is set to \"0\".\nfunc GetActiveHelpConfig(cmd *Command) string {\n\tactiveHelpCfg := os.Getenv(activeHelpGlobalEnvVar)\n\tif activeHelpCfg != activeHelpGlobalDisable {\n\t\tactiveHelpCfg = os.Getenv(activeHelpEnvVar(cmd.Root().Name()))\n\t}\n\treturn activeHelpCfg\n}\n\n// activeHelpEnvVar returns the name of the program-specific ActiveHelp environment\n// variable.  It has the format <PROGRAM>_ACTIVE_HELP where <PROGRAM> is the name of the\n// root command in upper case, with all non-ASCII-alphanumeric characters replaced by `_`.\nfunc activeHelpEnvVar(name string) string {\n\treturn configEnvVar(name, activeHelpEnvVarSuffix)\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:51:34"}, {"role": "assistant", "content": "```go\n// Copyright 2013-2023 The Cobra Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage cobra\n\nimport (\n\t\"fmt\"\n\t\"os\"\n\t\"strings\"\n\t\"testing\"\n)\n\nconst (\n\tactiveHelpMessage  = \"This is an activeHelp message\"\n\tactiveHelpMessage2 = \"This is the rest of the activeHelp message\"\n)\n\nfunc TestActiveHelpAlone(t *testing.T) {\n\trootCmd := &Command{\n\t\tUse: \"root\",\n\t\tRun: emptyRun,\n\t}\n\n\tactiveHelpFunc := func(cmd *Command, args []string, toComplete string) ([]string, ShellCompDirective) {\n\t\tcomps := AppendActiveHelp(nil, activeHelpMessage)\n\t\treturn comps, ShellCompDirectiveDefault\n\t}\n\n\t// Test that activeHelp can be added to a root command\n\trootCmd.ValidArgsFunction = activeHelpFunc\n\n\toutput, err := executeCommand(rootCmd, ShellCompNoDescRequestCmd, \"\")\n\tif err != nil {\n\t\tt.Errorf(\"Unexpected error: %v\", err)\n\t}\n\n\texpected := strings.Join([]string{\n\t\tfmt.Sprintf(\"%s%s\", activeHelpMarker, activeHelpMessage),\n\t\t\":0\",\n\t\t\"Completion ended with directive: ShellCompDirectiveDefault\", \"\"}, \"\\n\")\n\n\tif output != expected {\n\t\tt.Errorf(\"expected: %q, got: %q\", expected, output)\n\t}\n\n\trootCmd.ValidArgsFunction = nil\n\n\t// Test that activeHelp can be added to a child command\n\tchildCmd := &Command{\n\t\tUse:   \"thechild\",\n\t\tShort: \"The child command\",\n\t\tRun:   emptyRun,\n\t}\n\trootCmd.AddCommand(childCmd)\n\n\tchildCmd.ValidArgsFunction = activeHelpFunc\n\n\toutput, err = executeCommand(rootCmd, ShellCompNoDescRequestCmd, \"thechild\", \"\")\n\tif err != nil {\n\t\tt.Errorf(\"Unexpected error: %v\", err)\n\t}\n\n\texpected = strings.Join([]string{\n\t\tfmt.Sprintf(\"%s%s\", activeHelpMarker, activeHelpMessage),\n\t\t\":0\",\n\t\t\"Completion ended with directive: ShellCompDirectiveDefault\", \"\"}, \"\\n\")\n\n\tif output != expected {\n\t\tt.Errorf(\"expected: %q, got: %q\", expected, output)\n\t}\n}\n\nfunc TestActiveHelpWithComps(t *testing.T) {\n\trootCmd := &Command{\n\t\tUse: \"root\",\n\t\tRun: emptyRun,\n\t}\n\n\tchildCmd := &Command{\n\t\tUse:   \"thechild\",\n\t\tShort: \"The child command\",\n\t\tRun:   emptyRun,\n\t}\n\trootCmd.AddCommand(childCmd)\n\n\t// Test that activeHelp can be added following other completions\n\tchildCmd.ValidArgsFunction = func(cmd *Command, args []string, toComplete string) ([]string, ShellCompDirective) {\n\t\tcomps := []string{\"first\", \"second\"}\n\t\tcomps = AppendActiveHelp(comps, activeHelpMessage)\n\t\treturn comps, ShellCompDirectiveDefault\n\t}\n\n\toutput, err := executeCommand(rootCmd, ShellCompNoDescRequestCmd, \"thechild\", \"\")\n\tif err != nil {\n\t\tt.Errorf(\"Unexpected error: %v\", err)\n\t}\n\n\texpected := strings.Join([]string{\n\t\t\"first\",\n\t\t\"second\",\n\t\tfmt.Sprintf(\"%s%s\", activeHelpMarker, activeHelpMessage),\n\t\t\":0\",\n\t\t\"Completion ended with directive: ShellCompDirectiveDefault\", \"\"}, \"\\n\")\n\n\tif output != expected {\n\t\tt.Errorf(\"expected: %q, got: %q\", expected, output)\n\t}\n\n\t// Test that activeHelp can be added preceding other completions\n\tchildCmd.ValidArgsFunction = func(cmd *Command, args []string, toComplete string) ([]string, ShellCompDirective) {\n\t\tvar comps []string\n\t\tcomps = AppendActiveHelp(comps, activeHelpMessage)\n\t\tcomps = append(comps, []string{\"first\", \"second\"}...)\n\t\treturn comps, ShellCompDirectiveDefault\n\t}\n\n\toutput, err = executeCommand(rootCmd, ShellCompNoDescRequestCmd, \"thechild\", \"\")\n\tif err != nil {\n\t\tt.Errorf(\"Unexpected error: %v\", err)\n\t}\n\n\texpected = strings.Join([]string{\n\t\tfmt.Sprintf(\"%s%s\", activeHelpMarker, activeHelpMessage),\n\t\t\"first\",\n\t\t\"second\",\n\t\t\":0\",\n\t\t\"Completion ended with directive: ShellCompDirectiveDefault\", \"\"}, \"\\n\")\n\n\tif output != expected {\n\t\tt.Errorf(\"expected: %q, got: %q\", expected, output)\n\t}\n\n\t// Test that activeHelp can be added interleaved with other completions\n\tchildCmd.ValidArgsFunction = func(cmd *Command, args []string, toComplete string) ([]string, ShellCompDirective) {\n\t\tcomps := []string{\"first\"}\n\t\tcomps = AppendActiveHelp(comps, activeHelpMessage)\n\t\tcomps = append(comps, \"second\")\n\t\treturn comps, ShellCompDirectiveDefault\n\t}\n\n\toutput, err = executeCommand(rootCmd, ShellCompNoDescRequestCmd, \"thechild\", \"\")\n\tif err != nil {\n\t\tt.Errorf(\"Unexpected error: %v\", err)\n\t}\n\n\texpected = strings.Join([]string{\n\t\t\"first\",\n\t\tfmt.Sprintf(\"%s%s\", activeHelpMarker, activeHelpMessage),\n\t\t\"second\",\n\t\t\":0\",\n\t\t\"Completion ended with directive: ShellCompDirectiveDefault\", \"\"}, \"\\n\")\n\n\tif output != expected {\n\t\tt.Errorf(\"expected: %q, got: %q\", expected, output)\n\t}\n}\n\nfunc TestMultiActiveHelp(t *testing.T) {\n\trootCmd := &Command{\n\t\tUse: \"root\",\n\t\tRun: emptyRun,\n\t}\n\n\tchildCmd := &Command{\n\t\tUse:   \"thechild\",\n\t\tShort: \"The child command\",\n\t\tRun:   emptyRun,\n\t}\n\trootCmd.AddCommand(childCmd)\n\n\t// Test that multiple activeHelp message can be added\n\tchildCmd.ValidArgsFunction = func(cmd *Command, args []string, toComplete string) ([]string, ShellCompDirective) {\n\t\tcomps := AppendActiveHelp(nil, activeHelpMessage)\n\t\tcomps = AppendActiveHelp(comps, activeHelpMessage2)\n\t\treturn comps, ShellCompDirectiveNoFileComp\n\t}\n\n\toutput, err := executeCommand(rootCmd, ShellCompNoDescRequestCmd, \"thechild\", \"\")\n\tif err != nil {\n\t\tt.Errorf(\"Unexpected error: %v\", err)\n\t}\n\n\texpected := strings.Join([]string{\n\t\tfmt.Sprintf(\"%s%s\", activeHelpMarker, activeHelpMessage),\n\t\tfmt.Sprintf(\"%s%s\", activeHelpMarker, activeHelpMessage2),\n\t\t\":4\",\n\t\t\"Completion ended with directive: ShellCompDirectiveNoFileComp\", \"\"}, \"\\n\")\n\n\tif output != expected {\n\t\tt.Errorf(\"expected: %q, got: %q\", expected, output)\n\t}\n\n\t// Test that multiple activeHelp messages can be used along with completions\n\tchildCmd.ValidArgsFunction = func(cmd *Command, args []string, toComplete string) ([]string, ShellCompDirective) {\n\t\tcomps := []string{\"first\"}\n\t\tcomps = AppendActiveHelp(comps, activeHelpMessage)\n\t\tcomps = append(comps, \"second\")\n\t\tcomps = AppendActiveHelp(comps, activeHelpMessage2)\n\t\treturn comps, ShellCompDirectiveNoFileComp\n\t}\n\n\toutput, err = executeCommand(rootCmd, ShellCompNoDescRequestCmd, \"thechild\", \"\")\n\tif err != nil {\n\t\tt.Errorf(\"Unexpected error: %v\", err)\n\t}\n\n\texpected = strings.Join([]string{\n\t\t\"first\",\n\t\tfmt.Sprintf(\"%s%s\", activeHelpMarker, activeHelpMessage),\n\t\t\"second\",\n\t\tfmt.Sprintf(\"%s%s\", activeHelpMarker, activeHelpMessage2),\n\t\t\":4\",\n\t\t\"Completion ended with directive: ShellCompDirectiveNoFileComp\", \"\"}, \"\\n\")\n\n\tif output != expected {\n\t\tt.Errorf(\"expected: %q, got: %q\", expected, output)\n\t}\n}\n\nfunc TestActiveHelpForFlag(t *testing.T) {\n\trootCmd := &Command{\n\t\tUse: \"root\",\n\t\tRun: emptyRun,\n\t}\n\tflagname := \"flag\"\n\trootCmd.Flags().String(flagname, \"\", \"A flag\")\n\n\t// Test that multiple activeHelp message can be added\n\t_ = rootCmd.RegisterFlagCompletionFunc(flagname, func(cmd *Command, args []string, toComplete string) ([]string, ShellCompDirective) {\n\t\tcomps := []string{\"first\"}\n\t\tcomps = AppendActiveHelp(comps, activeHelpMessage)\n\t\tcomps = append(comps, \"second\")\n\t\tcomps = AppendActiveHelp(comps, activeHelpMessage2)\n\t\treturn comps, ShellCompDirectiveNoFileComp\n\t})\n\n\toutput, err := executeCommand(rootCmd, ShellCompNoDescRequestCmd, \"--flag\", \"\")\n\tif err != nil {\n\t\tt.Errorf(\"Unexpected error: %v\", err)\n\t}\n\n\texpected := strings.Join([]string{\n\t\t\"first\",\n\t\tfmt.Sprintf(\"%s%s\", activeHelpMarker, activeHelpMessage),\n\t\t\"second\",\n\t\tfmt.Sprintf(\"%s%s\", activeHelpMarker, activeHelpMessage2),\n\t\t\":4\",\n\t\t\"Completion ended with directive: ShellCompDirectiveNoFileComp\", \"\"}, \"\\n\")\n\n\tif output != expected {\n\t\tt.Errorf(\"expected: %q, got: %q\", expected, output)\n\t}\n}\n\nfunc TestConfigActiveHelp(t *testing.T) {\n\trootCmd := &Command{\n\t\tUse: \"root\",\n\t\tRun: emptyRun,\n\t}\n\n\tchildCmd := &Command{\n\t\tUse:   \"thechild\",\n\t\tShort: \"The child command\",\n\t\tRun:   emptyRun,\n\t}\n\trootCmd.AddCommand(childCmd)\n\n\tactiveHelpCfg := \"someconfig,anotherconfig\"\n\t// Set the variable that the user would be setting\n\tos.Setenv(activeHelpEnvVar(rootCmd.Name()), activeHelpCfg)\n\n\tchildCmd.ValidArgsFunction = func(cmd *Command, args []string, toComplete string) ([]string, ShellCompDirective) {\n\t\treceivedActiveHelpCfg := GetActiveHelpConfig(cmd)\n\t\tif receivedActiveHelpCfg != activeHelpCfg {\n\t\t\tt.Errorf(\"expected activeHelpConfig: %q, but got: %q\", activeHelpCfg, receivedActiveHelpCfg)\n\t\t}\n\t\treturn nil, ShellCompDirectiveDefault\n\t}\n\n\t_, err := executeCommand(rootCmd, ShellCompNoDescRequestCmd, \"thechild\", \"\")\n\tif err != nil {\n\t\tt.Errorf(\"Unexpected error: %v\", err)\n\t}\n\n\t// Test active help config for a flag\n\tactiveHelpCfg = \"a config for a flag\"\n\t// Set the variable that the completions scripts will be setting\n\tos.Setenv(activeHelpEnvVar(rootCmd.Name()), activeHelpCfg)\n\n\tflagname := \"flag\"\n\tchildCmd.Flags().String(flagname, \"\", \"A flag\")\n\n\t// Test that multiple activeHelp message can be added\n\t_ = childCmd.RegisterFlagCompletionFunc(flagname, func(cmd *Command, args []string, toComplete string) ([]string, ShellCompDirective) {\n\t\treceivedActiveHelpCfg := GetActiveHelpConfig(cmd)\n\t\tif receivedActiveHelpCfg != activeHelpCfg {\n\t\t\tt.Errorf(\"expected activeHelpConfig: %q, but got: %q\", activeHelpCfg, receivedActiveHelpCfg)\n\t\t}\n\t\treturn nil, ShellCompDirectiveDefault\n\t})\n\n\t_, err = executeCommand(rootCmd, ShellCompNoDescRequestCmd, \"thechild\", \"--flag\", \"\")\n\tif err != nil {\n\t\tt.Errorf(\"Unexpected error: %v\", err)\n\t}\n}\n\nfunc TestDisableActiveHelp(t *testing.T) {\n\trootCmd := &Command{\n\t\tUse: \"root\",\n\t\tRun: emptyRun,\n\t}\n\n\tchildCmd := &Command{\n\t\tUse:   \"thechild\",\n\t\tShort: \"The child command\",\n\t\tRun:   emptyRun,\n\t}\n\trootCmd.AddCommand(childCmd)\n\n\t// Test the disabling of activeHelp using the specific program\n\t// environment variable that the completions scripts will be setting.\n\t// Make sure the disabling value is \"0\" by hard-coding it in the tests;\n\t// this is for backwards-compatibility as programs will be using this value.\n\tos.Setenv(activeHelpEnvVar(rootCmd.Name()), \"0\")\n\n\tchildCmd.ValidArgsFunction = func(cmd *Command, args []string, toComplete string) ([]string, ShellCompDirective) {\n\t\tcomps := []string{\"first\"}\n\t\tcomps = AppendActiveHelp(comps, activeHelpMessage)\n\t\treturn comps, ShellCompDirectiveDefault\n\t}\n\n\toutput, err := executeCommand(rootCmd, ShellCompNoDescRequestCmd, \"thechild\", \"\")\n\tif err != nil {\n\t\tt.Errorf(\"Unexpected error: %v\", err)\n\t}\n\tos.Unsetenv(activeHelpEnvVar(rootCmd.Name()))\n\n\t// Make sure there is no ActiveHelp in the output\n\texpected := strings.Join([]string{\n\t\t\"first\",\n\t\t\":0\",\n\t\t\"Completion ended with directive: ShellCompDirectiveDefault\", \"\"}, \"\\n\")\n\n\tif output != expected {\n\t\tt.Errorf(\"expected: %q, got: %q\", expected, output)\n\t}\n\n\t// Now test the global disabling of ActiveHelp\n\tos.Setenv(activeHelpGlobalEnvVar, \"0\")\n\t// Set the specific variable, to make sure it is ignored when the global env\n\t// var is set properly\n\tos.Setenv(activeHelpEnvVar(rootCmd.Name()), \"1\")\n\n\toutput, err = executeCommand(rootCmd, ShellCompNoDescRequestCmd, \"thechild\", \"\")\n\tif err != nil {\n\t\tt.Errorf(\"Unexpected error: %v\", err)\n\t}\n\n\t// Make sure there is no ActiveHelp in the output\n\texpected = strings.Join([]string{\n\t\t\"first\",\n\t\t\":0\",\n\t\t\"Completion ended with directive: ShellCompDirectiveDefault\", \"\"}, \"\\n\")\n\n\tif output != expected {\n\t\tt.Errorf(\"expected: %q, got: %q\", expected, output)\n\t}\n\n\t// Make sure that if the global env variable is set to anything else than\n\t// the disable value it is ignored\n\tos.Setenv(activeHelpGlobalEnvVar, \"on\")\n\t// Set the specific variable, to make sure it is used (while ignoring the global env var)\n\tactiveHelpCfg := \"1\"\n\tos.Setenv(activeHelpEnvVar(rootCmd.Name()), activeHelpCfg)\n\n\tchildCmd.ValidArgsFunction = func(cmd *Command, args []string, toComplete string) ([]string, ShellCompDirective) {\n\t\treceivedActiveHelpCfg := GetActiveHelpConfig(cmd)\n\t\tif receivedActiveHelpCfg != activeHelpCfg {\n\t\t\tt.Errorf(\"expected activeHelpConfig: %q, but got: %q\", activeHelpCfg, receivedActiveHelpCfg)\n\t\t}\n\t\treturn nil, ShellCompDirectiveDefault\n\t}\n\n\t_, err = executeCommand(rootCmd, ShellCompNoDescRequestCmd, \"thechild\", \"\")\n\tif err != nil {\n\t\tt.Errorf(\"Unexpected error: %v\", err)\n\t}\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: cobra\nSource File Path(s): ['cobra\\\\args_test.go\\\\source_files\\\\args.go']\nTest File Path: cobra\\args_test.go\\args_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: cobra\\args_test.go\\source_files\\args.go\n// Copyright 2013-2023 The Cobra Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage cobra\n\nimport (\n\t\"fmt\"\n\t\"strings\"\n)\n\ntype PositionalArgs func(cmd *Command, args []string) error\n\n// legacyArgs validation has the following behaviour:\n// - root commands with no subcommands can take arbitrary arguments\n// - root commands with subcommands will do subcommand validity checking\n// - subcommands will always accept arbitrary arguments\nfunc legacyArgs(cmd *Command, args []string) error {\n\t// no subcommand, always take args\n\tif !cmd.HasSubCommands() {\n\t\treturn nil\n\t}\n\n\t// root command with subcommands, do subcommand checking.\n\tif !cmd.HasParent() && len(args) > 0 {\n\t\treturn fmt.Errorf(\"unknown command %q for %q%s\", args[0], cmd.CommandPath(), cmd.findSuggestions(args[0]))\n\t}\n\treturn nil\n}\n\n// NoArgs returns an error if any args are included.\nfunc NoArgs(cmd *Command, args []string) error {\n\tif len(args) > 0 {\n\t\treturn fmt.Errorf(\"unknown command %q for %q\", args[0], cmd.CommandPath())\n\t}\n\treturn nil\n}\n\n// OnlyValidArgs returns an error if there are any positional args that are not in\n// the `ValidArgs` field of `Command`\nfunc OnlyValidArgs(cmd *Command, args []string) error {\n\tif len(cmd.ValidArgs) > 0 {\n\t\t// Remove any description that may be included in ValidArgs.\n\t\t// A description is following a tab character.\n\t\tvalidArgs := make([]string, 0, len(cmd.ValidArgs))\n\t\tfor _, v := range cmd.ValidArgs {\n\t\t\tvalidArgs = append(validArgs, strings.SplitN(v, \"\\t\", 2)[0])\n\t\t}\n\t\tfor _, v := range args {\n\t\t\tif !stringInSlice(v, validArgs) {\n\t\t\t\treturn fmt.Errorf(\"invalid argument %q for %q%s\", v, cmd.CommandPath(), cmd.findSuggestions(args[0]))\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\n// ArbitraryArgs never returns an error.\nfunc ArbitraryArgs(cmd *Command, args []string) error {\n\treturn nil\n}\n\n// MinimumNArgs returns an error if there is not at least N args.\nfunc MinimumNArgs(n int) PositionalArgs {\n\treturn func(cmd *Command, args []string) error {\n\t\tif len(args) < n {\n\t\t\treturn fmt.Errorf(\"requires at least %d arg(s), only received %d\", n, len(args))\n\t\t}\n\t\treturn nil\n\t}\n}\n\n// MaximumNArgs returns an error if there are more than N args.\nfunc MaximumNArgs(n int) PositionalArgs {\n\treturn func(cmd *Command, args []string) error {\n\t\tif len(args) > n {\n\t\t\treturn fmt.Errorf(\"accepts at most %d arg(s), received %d\", n, len(args))\n\t\t}\n\t\treturn nil\n\t}\n}\n\n// ExactArgs returns an error if there are not exactly n args.\nfunc ExactArgs(n int) PositionalArgs {\n\treturn func(cmd *Command, args []string) error {\n\t\tif len(args) != n {\n\t\t\treturn fmt.Errorf(\"accepts %d arg(s), received %d\", n, len(args))\n\t\t}\n\t\treturn nil\n\t}\n}\n\n// RangeArgs returns an error if the number of args is not within the expected range.\nfunc RangeArgs(min int, max int) PositionalArgs {\n\treturn func(cmd *Command, args []string) error {\n\t\tif len(args) < min || len(args) > max {\n\t\t\treturn fmt.Errorf(\"accepts between %d and %d arg(s), received %d\", min, max, len(args))\n\t\t}\n\t\treturn nil\n\t}\n}\n\n// MatchAll allows combining several PositionalArgs to work in concert.\nfunc MatchAll(pargs ...PositionalArgs) PositionalArgs {\n\treturn func(cmd *Command, args []string) error {\n\t\tfor _, parg := range pargs {\n\t\t\tif err := parg(cmd, args); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t}\n}\n\n// ExactValidArgs returns an error if there are not exactly N positional args OR\n// there are any positional args that are not in the `ValidArgs` field of `Command`\n//\n// Deprecated: use MatchAll(ExactArgs(n), OnlyValidArgs) instead\nfunc ExactValidArgs(n int) PositionalArgs {\n\treturn MatchAll(ExactArgs(n), OnlyValidArgs)\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:51:34"}, {"role": "assistant", "content": "```go\n// Copyright 2013-2023 The Cobra Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage cobra\n\nimport (\n\t\"fmt\"\n\t\"strings\"\n\t\"testing\"\n)\n\nfunc getCommand(args PositionalArgs, withValid bool) *Command {\n\tc := &Command{\n\t\tUse:  \"c\",\n\t\tArgs: args,\n\t\tRun:  emptyRun,\n\t}\n\tif withValid {\n\t\tc.ValidArgs = []string{\"one\", \"two\", \"three\"}\n\t}\n\treturn c\n}\n\nfunc expectSuccess(output string, err error, t *testing.T) {\n\tif output != \"\" {\n\t\tt.Errorf(\"Unexpected output: %v\", output)\n\t}\n\tif err != nil {\n\t\tt.Fatalf(\"Unexpected error: %v\", err)\n\t}\n}\n\nfunc validOnlyWithInvalidArgs(err error, t *testing.T) {\n\tif err == nil {\n\t\tt.Fatal(\"Expected an error\")\n\t}\n\tgot := err.Error()\n\texpected := `invalid argument \"a\" for \"c\"`\n\tif got != expected {\n\t\tt.Errorf(\"Expected: %q, got: %q\", expected, got)\n\t}\n}\n\nfunc noArgsWithArgs(err error, t *testing.T, arg string) {\n\tif err == nil {\n\t\tt.Fatal(\"Expected an error\")\n\t}\n\tgot := err.Error()\n\texpected := `unknown command \"` + arg + `\" for \"c\"`\n\tif got != expected {\n\t\tt.Errorf(\"Expected: %q, got: %q\", expected, got)\n\t}\n}\n\nfunc minimumNArgsWithLessArgs(err error, t *testing.T) {\n\tif err == nil {\n\t\tt.Fatal(\"Expected an error\")\n\t}\n\tgot := err.Error()\n\texpected := \"requires at least 2 arg(s), only received 1\"\n\tif got != expected {\n\t\tt.Fatalf(\"Expected %q, got %q\", expected, got)\n\t}\n}\n\nfunc maximumNArgsWithMoreArgs(err error, t *testing.T) {\n\tif err == nil {\n\t\tt.Fatal(\"Expected an error\")\n\t}\n\tgot := err.Error()\n\texpected := \"accepts at most 2 arg(s), received 3\"\n\tif got != expected {\n\t\tt.Fatalf(\"Expected %q, got %q\", expected, got)\n\t}\n}\n\nfunc exactArgsWithInvalidCount(err error, t *testing.T) {\n\tif err == nil {\n\t\tt.Fatal(\"Expected an error\")\n\t}\n\tgot := err.Error()\n\texpected := \"accepts 2 arg(s), received 3\"\n\tif got != expected {\n\t\tt.Fatalf(\"Expected %q, got %q\", expected, got)\n\t}\n}\n\nfunc rangeArgsWithInvalidCount(err error, t *testing.T) {\n\tif err == nil {\n\t\tt.Fatal(\"Expected an error\")\n\t}\n\tgot := err.Error()\n\texpected := \"accepts between 2 and 4 arg(s), received 1\"\n\tif got != expected {\n\t\tt.Fatalf(\"Expected %q, got %q\", expected, got)\n\t}\n}\n\n// NoArgs\n\nfunc TestNoArgs(t *testing.T) {\n\tc := getCommand(NoArgs, false)\n\toutput, err := executeCommand(c)\n\texpectSuccess(output, err, t)\n}\n\nfunc TestNoArgs_WithArgs(t *testing.T) {\n\tc := getCommand(NoArgs, false)\n\t_, err := executeCommand(c, \"one\")\n\tnoArgsWithArgs(err, t, \"one\")\n}\n\nfunc TestNoArgs_WithValid_WithArgs(t *testing.T) {\n\tc := getCommand(NoArgs, true)\n\t_, err := executeCommand(c, \"one\")\n\tnoArgsWithArgs(err, t, \"one\")\n}\n\nfunc TestNoArgs_WithValid_WithInvalidArgs(t *testing.T) {\n\tc := getCommand(NoArgs, true)\n\t_, err := executeCommand(c, \"a\")\n\tnoArgsWithArgs(err, t, \"a\")\n}\n\nfunc TestNoArgs_WithValidOnly_WithInvalidArgs(t *testing.T) {\n\tc := getCommand(MatchAll(OnlyValidArgs, NoArgs), true)\n\t_, err := executeCommand(c, \"a\")\n\tvalidOnlyWithInvalidArgs(err, t)\n}\n\n// OnlyValidArgs\n\nfunc TestOnlyValidArgs(t *testing.T) {\n\tc := getCommand(OnlyValidArgs, true)\n\toutput, err := executeCommand(c, \"one\", \"two\")\n\texpectSuccess(output, err, t)\n}\n\nfunc TestOnlyValidArgs_WithInvalidArgs(t *testing.T) {\n\tc := getCommand(OnlyValidArgs, true)\n\t_, err := executeCommand(c, \"a\")\n\tvalidOnlyWithInvalidArgs(err, t)\n}\n\n// ArbitraryArgs\n\nfunc TestArbitraryArgs(t *testing.T) {\n\tc := getCommand(ArbitraryArgs, false)\n\toutput, err := executeCommand(c, \"a\", \"b\")\n\texpectSuccess(output, err, t)\n}\n\nfunc TestArbitraryArgs_WithValid(t *testing.T) {\n\tc := getCommand(ArbitraryArgs, true)\n\toutput, err := executeCommand(c, \"one\", \"two\")\n\texpectSuccess(output, err, t)\n}\n\nfunc TestArbitraryArgs_WithValid_WithInvalidArgs(t *testing.T) {\n\tc := getCommand(ArbitraryArgs, true)\n\toutput, err := executeCommand(c, \"a\")\n\texpectSuccess(output, err, t)\n}\n\nfunc TestArbitraryArgs_WithValidOnly_WithInvalidArgs(t *testing.T) {\n\tc := getCommand(MatchAll(OnlyValidArgs, ArbitraryArgs), true)\n\t_, err := executeCommand(c, \"a\")\n\tvalidOnlyWithInvalidArgs(err, t)\n}\n\n// MinimumNArgs\n\nfunc TestMinimumNArgs(t *testing.T) {\n\tc := getCommand(MinimumNArgs(2), false)\n\toutput, err := executeCommand(c, \"a\", \"b\", \"c\")\n\texpectSuccess(output, err, t)\n}\n\nfunc TestMinimumNArgs_WithValid(t *testing.T) {\n\tc := getCommand(MinimumNArgs(2), true)\n\toutput, err := executeCommand(c, \"one\", \"three\")\n\texpectSuccess(output, err, t)\n}\n\nfunc TestMinimumNArgs_WithValid__WithInvalidArgs(t *testing.T) {\n\tc := getCommand(MinimumNArgs(2), true)\n\toutput, err := executeCommand(c, \"a\", \"b\")\n\texpectSuccess(output, err, t)\n}\n\nfunc TestMinimumNArgs_WithValidOnly_WithInvalidArgs(t *testing.T) {\n\tc := getCommand(MatchAll(OnlyValidArgs, MinimumNArgs(2)), true)\n\t_, err := executeCommand(c, \"a\", \"b\")\n\tvalidOnlyWithInvalidArgs(err, t)\n}\n\nfunc TestMinimumNArgs_WithLessArgs(t *testing.T) {\n\tc := getCommand(MinimumNArgs(2), false)\n\t_, err := executeCommand(c, \"a\")\n\tminimumNArgsWithLessArgs(err, t)\n}\n\nfunc TestMinimumNArgs_WithLessArgs_WithValid(t *testing.T) {\n\tc := getCommand(MinimumNArgs(2), true)\n\t_, err := executeCommand(c, \"one\")\n\tminimumNArgsWithLessArgs(err, t)\n}\n\nfunc TestMinimumNArgs_WithLessArgs_WithValid_WithInvalidArgs(t *testing.T) {\n\tc := getCommand(MinimumNArgs(2), true)\n\t_, err := executeCommand(c, \"a\")\n\tminimumNArgsWithLessArgs(err, t)\n}\n\nfunc TestMinimumNArgs_WithLessArgs_WithValidOnly_WithInvalidArgs(t *testing.T) {\n\tc := getCommand(MatchAll(OnlyValidArgs, MinimumNArgs(2)), true)\n\t_, err := executeCommand(c, \"a\")\n\tvalidOnlyWithInvalidArgs(err, t)\n}\n\n// MaximumNArgs\n\nfunc TestMaximumNArgs(t *testing.T) {\n\tc := getCommand(MaximumNArgs(3), false)\n\toutput, err := executeCommand(c, \"a\", \"b\")\n\texpectSuccess(output, err, t)\n}\n\nfunc TestMaximumNArgs_WithValid(t *testing.T) {\n\tc := getCommand(MaximumNArgs(2), true)\n\toutput, err := executeCommand(c, \"one\", \"three\")\n\texpectSuccess(output, err, t)\n}\n\nfunc TestMaximumNArgs_WithValid_WithInvalidArgs(t *testing.T) {\n\tc := getCommand(MaximumNArgs(2), true)\n\toutput, err := executeCommand(c, \"a\", \"b\")\n\texpectSuccess(output, err, t)\n}\n\nfunc TestMaximumNArgs_WithValidOnly_WithInvalidArgs(t *testing.T) {\n\tc := getCommand(MatchAll(OnlyValidArgs, MaximumNArgs(2)), true)\n\t_, err := executeCommand(c, \"a\", \"b\")\n\tvalidOnlyWithInvalidArgs(err, t)\n}\n\nfunc TestMaximumNArgs_WithMoreArgs(t *testing.T) {\n\tc := getCommand(MaximumNArgs(2), false)\n\t_, err := executeCommand(c, \"a\", \"b\", \"c\")\n\tmaximumNArgsWithMoreArgs(err, t)\n}\n\nfunc TestMaximumNArgs_WithMoreArgs_WithValid(t *testing.T) {\n\tc := getCommand(MaximumNArgs(2), true)\n\t_, err := executeCommand(c, \"one\", \"three\", \"two\")\n\tmaximumNArgsWithMoreArgs(err, t)\n}\n\nfunc TestMaximumNArgs_WithMoreArgs_WithValid_WithInvalidArgs(t *testing.T) {\n\tc := getCommand(MaximumNArgs(2), true)\n\t_, err := executeCommand(c, \"a\", \"b\", \"c\")\n\tmaximumNArgsWithMoreArgs(err, t)\n}\n\nfunc TestMaximumNArgs_WithMoreArgs_WithValidOnly_WithInvalidArgs(t *testing.T) {\n\tc := getCommand(MatchAll(OnlyValidArgs, MaximumNArgs(2)), true)\n\t_, err := executeCommand(c, \"a\", \"b\", \"c\")\n\tvalidOnlyWithInvalidArgs(err, t)\n}\n\n// ExactArgs\n\nfunc TestExactArgs(t *testing.T) {\n\tc := getCommand(ExactArgs(3), false)\n\toutput, err := executeCommand(c, \"a\", \"b\", \"c\")\n\texpectSuccess(output, err, t)\n}\n\nfunc TestExactArgs_WithValid(t *testing.T) {\n\tc := getCommand(ExactArgs(3), true)\n\toutput, err := executeCommand(c, \"three\", \"one\", \"two\")\n\texpectSuccess(output, err, t)\n}\n\nfunc TestExactArgs_WithValid_WithInvalidArgs(t *testing.T) {\n\tc := getCommand(ExactArgs(3), true)\n\toutput, err := executeCommand(c, \"three\", \"a\", \"two\")\n\texpectSuccess(output, err, t)\n}\n\nfunc TestExactArgs_WithValidOnly_WithInvalidArgs(t *testing.T) {\n\tc := getCommand(MatchAll(OnlyValidArgs, ExactArgs(3)), true)\n\t_, err := executeCommand(c, \"three\", \"a\", \"two\")\n\tvalidOnlyWithInvalidArgs(err, t)\n}\n\nfunc TestExactArgs_WithInvalidCount(t *testing.T) {\n\tc := getCommand(ExactArgs(2), false)\n\t_, err := executeCommand(c, \"a\", \"b\", \"c\")\n\texactArgsWithInvalidCount(err, t)\n}\n\nfunc TestExactArgs_WithInvalidCount_WithValid(t *testing.T) {\n\tc := getCommand(ExactArgs(2), true)\n\t_, err := executeCommand(c, \"three\", \"one\", \"two\")\n\texactArgsWithInvalidCount(err, t)\n}\n\nfunc TestExactArgs_WithInvalidCount_WithValid_WithInvalidArgs(t *testing.T) {\n\tc := getCommand(ExactArgs(2), true)\n\t_, err := executeCommand(c, \"three\", \"a\", \"two\")\n\texactArgsWithInvalidCount(err, t)\n}\n\nfunc TestExactArgs_WithInvalidCount_WithValidOnly_WithInvalidArgs(t *testing.T) {\n\tc := getCommand(MatchAll(OnlyValidArgs, ExactArgs(2)), true)\n\t_, err := executeCommand(c, \"three\", \"a\", \"two\")\n\tvalidOnlyWithInvalidArgs(err, t)\n}\n\n// RangeArgs\n\nfunc TestRangeArgs(t *testing.T) {\n\tc := getCommand(RangeArgs(2, 4), false)\n\toutput, err := executeCommand(c, \"a\", \"b\", \"c\")\n\texpectSuccess(output, err, t)\n}\n\nfunc TestRangeArgs_WithValid(t *testing.T) {\n\tc := getCommand(RangeArgs(2, 4), true)\n\toutput, err := executeCommand(c, \"three\", \"one\", \"two\")\n\texpectSuccess(output, err, t)\n}\n\nfunc TestRangeArgs_WithValid_WithInvalidArgs(t *testing.T) {\n\tc := getCommand(RangeArgs(2, 4), true)\n\toutput, err := executeCommand(c, \"three\", \"a\", \"two\")\n\texpectSuccess(output, err, t)\n}\n\nfunc TestRangeArgs_WithValidOnly_WithInvalidArgs(t *testing.T) {\n\tc := getCommand(MatchAll(OnlyValidArgs, RangeArgs(2, 4)), true)\n\t_, err := executeCommand(c, \"three\", \"a\", \"two\")\n\tvalidOnlyWithInvalidArgs(err, t)\n}\n\nfunc TestRangeArgs_WithInvalidCount(t *testing.T) {\n\tc := getCommand(RangeArgs(2, 4), false)\n\t_, err := executeCommand(c, \"a\")\n\trangeArgsWithInvalidCount(err, t)\n}\n\nfunc TestRangeArgs_WithInvalidCount_WithValid(t *testing.T) {\n\tc := getCommand(RangeArgs(2, 4), true)\n\t_, err := executeCommand(c, \"two\")\n\trangeArgsWithInvalidCount(err, t)\n}\n\nfunc TestRangeArgs_WithInvalidCount_WithValid_WithInvalidArgs(t *testing.T) {\n\tc := getCommand(RangeArgs(2, 4), true)\n\t_, err := executeCommand(c, \"a\")\n\trangeArgsWithInvalidCount(err, t)\n}\n\nfunc TestRangeArgs_WithInvalidCount_WithValidOnly_WithInvalidArgs(t *testing.T) {\n\tc := getCommand(MatchAll(OnlyValidArgs, RangeArgs(2, 4)), true)\n\t_, err := executeCommand(c, \"a\")\n\tvalidOnlyWithInvalidArgs(err, t)\n}\n\n// Takes(No)Args\n\nfunc TestRootTakesNoArgs(t *testing.T) {\n\trootCmd := &Command{Use: \"root\", Run: emptyRun}\n\tchildCmd := &Command{Use: \"child\", Run: emptyRun}\n\trootCmd.AddCommand(childCmd)\n\n\t_, err := executeCommand(rootCmd, \"illegal\", \"args\")\n\tif err == nil {\n\t\tt.Fatal(\"Expected an error\")\n\t}\n\n\tgot := err.Error()\n\texpected := `unknown command \"illegal\" for \"root\"`\n\tif !strings.Contains(got, expected) {\n\t\tt.Errorf(\"expected %q, got %q\", expected, got)\n\t}\n}\n\nfunc TestRootTakesArgs(t *testing.T) {\n\trootCmd := &Command{Use: \"root\", Args: ArbitraryArgs, Run: emptyRun}\n\tchildCmd := &Command{Use: \"child\", Run: emptyRun}\n\trootCmd.AddCommand(childCmd)\n\n\t_, err := executeCommand(rootCmd, \"legal\", \"args\")\n\tif err != nil {\n\t\tt.Errorf(\"Unexpected error: %v\", err)\n\t}\n}\n\nfunc TestChildTakesNoArgs(t *testing.T) {\n\trootCmd := &Command{Use: \"root\", Run: emptyRun}\n\tchildCmd := &Command{Use: \"child\", Args: NoArgs, Run: emptyRun}\n\trootCmd.AddCommand(childCmd)\n\n\t_, err := executeCommand(rootCmd, \"child\", \"illegal\", \"args\")\n\tif err == nil {\n\t\tt.Fatal(\"Expected an error\")\n\t}\n\n\tgot := err.Error()\n\texpected := `unknown command \"illegal\" for \"root child\"`\n\tif !strings.Contains(got, expected) {\n\t\tt.Errorf(\"expected %q, got %q\", expected, got)\n\t}\n}\n\nfunc TestChildTakesArgs(t *testing.T) {\n\trootCmd := &Command{Use: \"root\", Run: emptyRun}\n\tchildCmd := &Command{Use: \"child\", Args: ArbitraryArgs, Run: emptyRun}\n\trootCmd.AddCommand(childCmd)\n\n\t_, err := executeCommand(rootCmd, \"child\", \"legal\", \"args\")\n\tif err != nil {\n\t\tt.Fatalf(\"Unexpected error: %v\", err)\n\t}\n}\n\nfunc TestMatchAll(t *testing.T) {\n\t// Somewhat contrived example check that ensures there are exactly 3\n\t// arguments, and each argument is exactly 2 bytes long.\n\tpargs := MatchAll(\n\t\tExactArgs(3),\n\t\tfunc(cmd *Command, args []string) error {\n\t\t\tfor _, arg := range args {\n\t\t\t\tif len([]byte(arg)) != 2 {\n\t\t\t\t\treturn fmt.Errorf(\"expected to be exactly 2 bytes long\")\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn nil\n\t\t},\n\t)\n\n\ttestCases := map[string]struct {\n\t\targs []string\n\t\tfail bool\n\t}{\n\t\t\"happy path\": {\n\t\t\t[]string{\"aa\", \"bb\", \"cc\"},\n\t\t\tfalse,\n\t\t},\n\t\t\"incorrect number of args\": {\n\t\t\t[]string{\"aa\", \"bb\", \"cc\", \"dd\"},\n\t\t\ttrue,\n\t\t},\n\t\t\"incorrect number of bytes in one arg\": {\n\t\t\t[]string{\"aa\", \"bb\", \"abc\"},\n\t\t\ttrue,\n\t\t},\n\t}\n\n\trootCmd := &Command{Use: \"root\", Args: pargs, Run: emptyRun}\n\n\tfor name, tc := range testCases {\n\t\tt.Run(name, func(t *testing.T) {\n\t\t\t_, err := executeCommand(rootCmd, tc.args...)\n\t\t\tif err != nil && !tc.fail {\n\t\t\t\tt.Errorf(\"unexpected: %v\\n\", err)\n\t\t\t}\n\t\t\tif err == nil && tc.fail {\n\t\t\t\tt.Errorf(\"expected error\")\n\t\t\t}\n\t\t})\n\t}\n}\n\n// DEPRECATED\n\nfunc TestExactValidArgs(t *testing.T) {\n\tc := getCommand(ExactValidArgs(3), true)\n\toutput, err := executeCommand(c, \"three\", \"one\", \"two\")\n\texpectSuccess(output, err, t)\n}\n\nfunc TestExactValidArgs_WithInvalidCount(t *testing.T) {\n\tc := getCommand(ExactValidArgs(2), false)\n\t_, err := executeCommand(c, \"three\", \"one\", \"two\")\n\texactArgsWithInvalidCount(err, t)\n}\n\nfunc TestExactValidArgs_WithInvalidCount_WithInvalidArgs(t *testing.T) {\n\tc := getCommand(ExactValidArgs(2), true)\n\t_, err := executeCommand(c, \"three\", \"a\", \"two\")\n\texactArgsWithInvalidCount(err, t)\n}\n\nfunc TestExactValidArgs_WithInvalidArgs(t *testing.T) {\n\tc := getCommand(ExactValidArgs(2), true)\n\t_, err := executeCommand(c, \"three\", \"a\")\n\tvalidOnlyWithInvalidArgs(err, t)\n}\n\n// This test make sure we keep backwards-compatibility with respect\n// to the legacyArgs() function.\n// It makes sure the root command accepts arguments if it does not have\n// sub-commands.\nfunc TestLegacyArgsRootAcceptsArgs(t *testing.T) {\n\trootCmd := &Command{Use: \"root\", Args: nil, Run: emptyRun}\n\n\t_, err := executeCommand(rootCmd, \"somearg\")\n\tif err != nil {\n\t\tt.Fatalf(\"Unexpected error: %v\", err)\n\t}\n}\n\n// This test make sure we keep backwards-compatibility with respect\n// to the legacyArgs() function.\n// It makes sure a sub-command accepts arguments and further sub-commands\nfunc TestLegacyArgsSubcmdAcceptsArgs(t *testing.T) {\n\trootCmd := &Command{Use: \"root\", Args: nil, Run: emptyRun}\n\tchildCmd := &Command{Use: \"child\", Args: nil, Run: emptyRun}\n\tgrandchildCmd := &Command{Use: \"grandchild\", Args: nil, Run: emptyRun}\n\trootCmd.AddCommand(childCmd)\n\tchildCmd.AddCommand(grandchildCmd)\n\n\t_, err := executeCommand(rootCmd, \"child\", \"somearg\")\n\tif err != nil {\n\t\tt.Fatalf(\"Unexpected error: %v\", err)\n\t}\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: cobra\nSource File Path(s): ['cobra\\\\bash_completions_test.go\\\\source_files\\\\bash_completions.go']\nTest File Path: cobra\\bash_completions_test.go\\bash_completions_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: cobra\\bash_completions_test.go\\source_files\\bash_completions.go\n// Copyright 2013-2023 The Cobra Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage cobra\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"io\"\n\t\"os\"\n\t\"sort\"\n\t\"strings\"\n\n\t\"github.com/spf13/pflag\"\n)\n\n// Annotations for Bash completion.\nconst (\n\tBashCompFilenameExt     = \"cobra_annotation_bash_completion_filename_extensions\"\n\tBashCompCustom          = \"cobra_annotation_bash_completion_custom\"\n\tBashCompOneRequiredFlag = \"cobra_annotation_bash_completion_one_required_flag\"\n\tBashCompSubdirsInDir    = \"cobra_annotation_bash_completion_subdirs_in_dir\"\n)\n\nfunc writePreamble(buf io.StringWriter, name string) {\n\tWriteStringAndCheck(buf, fmt.Sprintf(\"# bash completion for %-36s -*- shell-script -*-\\n\", name))\n\tWriteStringAndCheck(buf, fmt.Sprintf(`\n__%[1]s_debug()\n{\n    if [[ -n ${BASH_COMP_DEBUG_FILE:-} ]]; then\n        echo \"$*\" >> \"${BASH_COMP_DEBUG_FILE}\"\n    fi\n}\n\n# Homebrew on Macs have version 1.3 of bash-completion which doesn't include\n# _init_completion. This is a very minimal version of that function.\n__%[1]s_init_completion()\n{\n    COMPREPLY=()\n    _get_comp_words_by_ref \"$@\" cur prev words cword\n}\n\n__%[1]s_index_of_word()\n{\n    local w word=$1\n    shift\n    index=0\n    for w in \"$@\"; do\n        [[ $w = \"$word\" ]] && return\n        index=$((index+1))\n    done\n    index=-1\n}\n\n__%[1]s_contains_word()\n{\n    local w word=$1; shift\n    for w in \"$@\"; do\n        [[ $w = \"$word\" ]] && return\n    done\n    return 1\n}\n\n__%[1]s_handle_go_custom_completion()\n{\n    __%[1]s_debug \"${FUNCNAME[0]}: cur is ${cur}, words[*] is ${words[*]}, #words[@] is ${#words[@]}\"\n\n    local shellCompDirectiveError=%[3]d\n    local shellCompDirectiveNoSpace=%[4]d\n    local shellCompDirectiveNoFileComp=%[5]d\n    local shellCompDirectiveFilterFileExt=%[6]d\n    local shellCompDirectiveFilterDirs=%[7]d\n\n    local out requestComp lastParam lastChar comp directive args\n\n    # Prepare the command to request completions for the program.\n    # Calling ${words[0]} instead of directly %[1]s allows handling aliases\n    args=(\"${words[@]:1}\")\n    # Disable ActiveHelp which is not supported for bash completion v1\n    requestComp=\"%[8]s=0 ${words[0]} %[2]s ${args[*]}\"\n\n    lastParam=${words[$((${#words[@]}-1))]}\n    lastChar=${lastParam:$((${#lastParam}-1)):1}\n    __%[1]s_debug \"${FUNCNAME[0]}: lastParam ${lastParam}, lastChar ${lastChar}\"\n\n    if [ -z \"${cur}\" ] && [ \"${lastChar}\" != \"=\" ]; then\n        # If the last parameter is complete (there is a space following it)\n        # We add an extra empty parameter so we can indicate this to the go method.\n        __%[1]s_debug \"${FUNCNAME[0]}: Adding extra empty parameter\"\n        requestComp=\"${requestComp} \\\"\\\"\"\n    fi\n\n    __%[1]s_debug \"${FUNCNAME[0]}: calling ${requestComp}\"\n    # Use eval to handle any environment variables and such\n    out=$(eval \"${requestComp}\" 2>/dev/null)\n\n    # Extract the directive integer at the very end of the output following a colon (:)\n    directive=${out##*:}\n    # Remove the directive\n    out=${out%%:*}\n    if [ \"${directive}\" = \"${out}\" ]; then\n        # There is not directive specified\n        directive=0\n    fi\n    __%[1]s_debug \"${FUNCNAME[0]}: the completion directive is: ${directive}\"\n    __%[1]s_debug \"${FUNCNAME[0]}: the completions are: ${out}\"\n\n    if [ $((directive & shellCompDirectiveError)) -ne 0 ]; then\n        # Error code.  No completion.\n        __%[1]s_debug \"${FUNCNAME[0]}: received error from custom completion go code\"\n        return\n    else\n        if [ $((directive & shellCompDirectiveNoSpace)) -ne 0 ]; then\n            if [[ $(type -t compopt) = \"builtin\" ]]; then\n                __%[1]s_debug \"${FUNCNAME[0]}: activating no space\"\n                compopt -o nospace\n            fi\n        fi\n        if [ $((directive & shellCompDirectiveNoFileComp)) -ne 0 ]; then\n            if [[ $(type -t compopt) = \"builtin\" ]]; then\n                __%[1]s_debug \"${FUNCNAME[0]}: activating no file completion\"\n                compopt +o default\n            fi\n        fi\n    fi\n\n    if [ $((directive & shellCompDirectiveFilterFileExt)) -ne 0 ]; then\n        # File extension filtering\n        local fullFilter filter filteringCmd\n        # Do not use quotes around the $out variable or else newline\n        # characters will be kept.\n        for filter in ${out}; do\n            fullFilter+=\"$filter|\"\n        done\n\n        filteringCmd=\"_filedir $fullFilter\"\n        __%[1]s_debug \"File filtering command: $filteringCmd\"\n        $filteringCmd\n    elif [ $((directive & shellCompDirectiveFilterDirs)) -ne 0 ]; then\n        # File completion for directories only\n        local subdir\n        # Use printf to strip any trailing newline\n        subdir=$(printf \"%%s\" \"${out}\")\n        if [ -n \"$subdir\" ]; then\n            __%[1]s_debug \"Listing directories in $subdir\"\n            __%[1]s_handle_subdirs_in_dir_flag \"$subdir\"\n        else\n            __%[1]s_debug \"Listing directories in .\"\n            _filedir -d\n        fi\n    else\n        while IFS='' read -r comp; do\n            COMPREPLY+=(\"$comp\")\n        done < <(compgen -W \"${out}\" -- \"$cur\")\n    fi\n}\n\n__%[1]s_handle_reply()\n{\n    __%[1]s_debug \"${FUNCNAME[0]}\"\n    local comp\n    case $cur in\n        -*)\n            if [[ $(type -t compopt) = \"builtin\" ]]; then\n                compopt -o nospace\n            fi\n            local allflags\n            if [ ${#must_have_one_flag[@]} -ne 0 ]; then\n                allflags=(\"${must_have_one_flag[@]}\")\n            else\n                allflags=(\"${flags[*]} ${two_word_flags[*]}\")\n            fi\n            while IFS='' read -r comp; do\n                COMPREPLY+=(\"$comp\")\n            done < <(compgen -W \"${allflags[*]}\" -- \"$cur\")\n            if [[ $(type -t compopt) = \"builtin\" ]]; then\n                [[ \"${COMPREPLY[0]}\" == *= ]] || compopt +o nospace\n            fi\n\n            # complete after --flag=abc\n            if [[ $cur == *=* ]]; then\n                if [[ $(type -t compopt) = \"builtin\" ]]; then\n                    compopt +o nospace\n                fi\n\n                local index flag\n                flag=\"${cur%%=*}\"\n                __%[1]s_index_of_word \"${flag}\" \"${flags_with_completion[@]}\"\n                COMPREPLY=()\n                if [[ ${index} -ge 0 ]]; then\n                    PREFIX=\"\"\n                    cur=\"${cur#*=}\"\n                    ${flags_completion[${index}]}\n                    if [ -n \"${ZSH_VERSION:-}\" ]; then\n                        # zsh completion needs --flag= prefix\n                        eval \"COMPREPLY=( \\\"\\${COMPREPLY[@]/#/${flag}=}\\\" )\"\n                    fi\n                fi\n            fi\n\n            if [[ -z \"${flag_parsing_disabled}\" ]]; then\n                # If flag parsing is enabled, we have completed the flags and can return.\n                # If flag parsing is disabled, we may not know all (or any) of the flags, so we fallthrough\n                # to possibly call handle_go_custom_completion.\n                return 0;\n            fi\n            ;;\n    esac\n\n    # check if we are handling a flag with special work handling\n    local index\n    __%[1]s_index_of_word \"${prev}\" \"${flags_with_completion[@]}\"\n    if [[ ${index} -ge 0 ]]; then\n        ${flags_completion[${index}]}\n        return\n    fi\n\n    # we are parsing a flag and don't have a special handler, no completion\n    if [[ ${cur} != \"${words[cword]}\" ]]; then\n        return\n    fi\n\n    local completions\n    completions=(\"${commands[@]}\")\n    if [[ ${#must_have_one_noun[@]} -ne 0 ]]; then\n        completions+=(\"${must_have_one_noun[@]}\")\n    elif [[ -n \"${has_completion_function}\" ]]; then\n        # if a go completion function is provided, defer to that function\n        __%[1]s_handle_go_custom_completion\n    fi\n    if [[ ${#must_have_one_flag[@]} -ne 0 ]]; then\n        completions+=(\"${must_have_one_flag[@]}\")\n    fi\n    while IFS='' read -r comp; do\n        COMPREPLY+=(\"$comp\")\n    done < <(compgen -W \"${completions[*]}\" -- \"$cur\")\n\n    if [[ ${#COMPREPLY[@]} -eq 0 && ${#noun_aliases[@]} -gt 0 && ${#must_have_one_noun[@]} -ne 0 ]]; then\n        while IFS='' read -r comp; do\n            COMPREPLY+=(\"$comp\")\n        done < <(compgen -W \"${noun_aliases[*]}\" -- \"$cur\")\n    fi\n\n    if [[ ${#COMPREPLY[@]} -eq 0 ]]; then\n        if declare -F __%[1]s_custom_func >/dev/null; then\n            # try command name qualified custom func\n            __%[1]s_custom_func\n        else\n            # otherwise fall back to unqualified for compatibility\n            declare -F __custom_func >/dev/null && __custom_func\n        fi\n    fi\n\n    # available in bash-completion >= 2, not always present on macOS\n    if declare -F __ltrim_colon_completions >/dev/null; then\n        __ltrim_colon_completions \"$cur\"\n    fi\n\n    # If there is only 1 completion and it is a flag with an = it will be completed\n    # but we don't want a space after the =\n    if [[ \"${#COMPREPLY[@]}\" -eq \"1\" ]] && [[ $(type -t compopt) = \"builtin\" ]] && [[ \"${COMPREPLY[0]}\" == --*= ]]; then\n       compopt -o nospace\n    fi\n}\n\n# The arguments should be in the form \"ext1|ext2|extn\"\n__%[1]s_handle_filename_extension_flag()\n{\n    local ext=\"$1\"\n    _filedir \"@(${ext})\"\n}\n\n__%[1]s_handle_subdirs_in_dir_flag()\n{\n    local dir=\"$1\"\n    pushd \"${dir}\" >/dev/null 2>&1 && _filedir -d && popd >/dev/null 2>&1 || return\n}\n\n__%[1]s_handle_flag()\n{\n    __%[1]s_debug \"${FUNCNAME[0]}: c is $c words[c] is ${words[c]}\"\n\n    # if a command required a flag, and we found it, unset must_have_one_flag()\n    local flagname=${words[c]}\n    local flagvalue=\"\"\n    # if the word contained an =\n    if [[ ${words[c]} == *\"=\"* ]]; then\n        flagvalue=${flagname#*=} # take in as flagvalue after the =\n        flagname=${flagname%%=*} # strip everything after the =\n        flagname=\"${flagname}=\" # but put the = back\n    fi\n    __%[1]s_debug \"${FUNCNAME[0]}: looking for ${flagname}\"\n    if __%[1]s_contains_word \"${flagname}\" \"${must_have_one_flag[@]}\"; then\n        must_have_one_flag=()\n    fi\n\n    # if you set a flag which only applies to this command, don't show subcommands\n    if __%[1]s_contains_word \"${flagname}\" \"${local_nonpersistent_flags[@]}\"; then\n      commands=()\n    fi\n\n    # keep flag value with flagname as flaghash\n    # flaghash variable is an associative array which is only supported in bash > 3.\n    if [[ -z \"${BASH_VERSION:-}\" || \"${BASH_VERSINFO[0]:-}\" -gt 3 ]]; then\n        if [ -n \"${flagvalue}\" ] ; then\n            flaghash[${flagname}]=${flagvalue}\n        elif [ -n \"${words[ $((c+1)) ]}\" ] ; then\n            flaghash[${flagname}]=${words[ $((c+1)) ]}\n        else\n            flaghash[${flagname}]=\"true\" # pad \"true\" for bool flag\n        fi\n    fi\n\n    # skip the argument to a two word flag\n    if [[ ${words[c]} != *\"=\"* ]] && __%[1]s_contains_word \"${words[c]}\" \"${two_word_flags[@]}\"; then\n        __%[1]s_debug \"${FUNCNAME[0]}: found a flag ${words[c]}, skip the next argument\"\n        c=$((c+1))\n        # if we are looking for a flags value, don't show commands\n        if [[ $c -eq $cword ]]; then\n            commands=()\n        fi\n    fi\n\n    c=$((c+1))\n\n}\n\n__%[1]s_handle_noun()\n{\n    __%[1]s_debug \"${FUNCNAME[0]}: c is $c words[c] is ${words[c]}\"\n\n    if __%[1]s_contains_word \"${words[c]}\" \"${must_have_one_noun[@]}\"; then\n        must_have_one_noun=()\n    elif __%[1]s_contains_word \"${words[c]}\" \"${noun_aliases[@]}\"; then\n        must_have_one_noun=()\n    fi\n\n    nouns+=(\"${words[c]}\")\n    c=$((c+1))\n}\n\n__%[1]s_handle_command()\n{\n    __%[1]s_debug \"${FUNCNAME[0]}: c is $c words[c] is ${words[c]}\"\n\n    local next_command\n    if [[ -n ${last_command} ]]; then\n        next_command=\"_${last_command}_${words[c]//:/__}\"\n    else\n        if [[ $c -eq 0 ]]; then\n            next_command=\"_%[1]s_root_command\"\n        else\n            next_command=\"_${words[c]//:/__}\"\n        fi\n    fi\n    c=$((c+1))\n    __%[1]s_debug \"${FUNCNAME[0]}: looking for ${next_command}\"\n    declare -F \"$next_command\" >/dev/null && $next_command\n}\n\n__%[1]s_handle_word()\n{\n    if [[ $c -ge $cword ]]; then\n        __%[1]s_handle_reply\n        return\n    fi\n    __%[1]s_debug \"${FUNCNAME[0]}: c is $c words[c] is ${words[c]}\"\n    if [[ \"${words[c]}\" == -* ]]; then\n        __%[1]s_handle_flag\n    elif __%[1]s_contains_word \"${words[c]}\" \"${commands[@]}\"; then\n        __%[1]s_handle_command\n    elif [[ $c -eq 0 ]]; then\n        __%[1]s_handle_command\n    elif __%[1]s_contains_word \"${words[c]}\" \"${command_aliases[@]}\"; then\n        # aliashash variable is an associative array which is only supported in bash > 3.\n        if [[ -z \"${BASH_VERSION:-}\" || \"${BASH_VERSINFO[0]:-}\" -gt 3 ]]; then\n            words[c]=${aliashash[${words[c]}]}\n            __%[1]s_handle_command\n        else\n            __%[1]s_handle_noun\n        fi\n    else\n        __%[1]s_handle_noun\n    fi\n    __%[1]s_handle_word\n}\n\n`, name, ShellCompNoDescRequestCmd,\n\t\tShellCompDirectiveError, ShellCompDirectiveNoSpace, ShellCompDirectiveNoFileComp,\n\t\tShellCompDirectiveFilterFileExt, ShellCompDirectiveFilterDirs, activeHelpEnvVar(name)))\n}\n\nfunc writePostscript(buf io.StringWriter, name string) {\n\tname = strings.ReplaceAll(name, \":\", \"__\")\n\tWriteStringAndCheck(buf, fmt.Sprintf(\"__start_%s()\\n\", name))\n\tWriteStringAndCheck(buf, fmt.Sprintf(`{\n    local cur prev words cword split\n    declare -A flaghash 2>/dev/null || :\n    declare -A aliashash 2>/dev/null || :\n    if declare -F _init_completion >/dev/null 2>&1; then\n        _init_completion -s || return\n    else\n        __%[1]s_init_completion -n \"=\" || return\n    fi\n\n    local c=0\n    local flag_parsing_disabled=\n    local flags=()\n    local two_word_flags=()\n    local local_nonpersistent_flags=()\n    local flags_with_completion=()\n    local flags_completion=()\n    local commands=(\"%[1]s\")\n    local command_aliases=()\n    local must_have_one_flag=()\n    local must_have_one_noun=()\n    local has_completion_function=\"\"\n    local last_command=\"\"\n    local nouns=()\n    local noun_aliases=()\n\n    __%[1]s_handle_word\n}\n\n`, name))\n\tWriteStringAndCheck(buf, fmt.Sprintf(`if [[ $(type -t compopt) = \"builtin\" ]]; then\n    complete -o default -F __start_%s %s\nelse\n    complete -o default -o nospace -F __start_%s %s\nfi\n\n`, name, name, name, name))\n\tWriteStringAndCheck(buf, \"# ex: ts=4 sw=4 et filetype=sh\\n\")\n}\n\nfunc writeCommands(buf io.StringWriter, cmd *Command) {\n\tWriteStringAndCheck(buf, \"    commands=()\\n\")\n\tfor _, c := range cmd.Commands() {\n\t\tif !c.IsAvailableCommand() && c != cmd.helpCommand {\n\t\t\tcontinue\n\t\t}\n\t\tWriteStringAndCheck(buf, fmt.Sprintf(\"    commands+=(%q)\\n\", c.Name()))\n\t\twriteCmdAliases(buf, c)\n\t}\n\tWriteStringAndCheck(buf, \"\\n\")\n}\n\nfunc writeFlagHandler(buf io.StringWriter, name string, annotations map[string][]string, cmd *Command) {\n\tfor key, value := range annotations {\n\t\tswitch key {\n\t\tcase BashCompFilenameExt:\n\t\t\tWriteStringAndCheck(buf, fmt.Sprintf(\"    flags_with_completion+=(%q)\\n\", name))\n\n\t\t\tvar ext string\n\t\t\tif len(value) > 0 {\n\t\t\t\text = fmt.Sprintf(\"__%s_handle_filename_extension_flag \", cmd.Root().Name()) + strings.Join(value, \"|\")\n\t\t\t} else {\n\t\t\t\text = \"_filedir\"\n\t\t\t}\n\t\t\tWriteStringAndCheck(buf, fmt.Sprintf(\"    flags_completion+=(%q)\\n\", ext))\n\t\tcase BashCompCustom:\n\t\t\tWriteStringAndCheck(buf, fmt.Sprintf(\"    flags_with_completion+=(%q)\\n\", name))\n\n\t\t\tif len(value) > 0 {\n\t\t\t\thandlers := strings.Join(value, \"; \")\n\t\t\t\tWriteStringAndCheck(buf, fmt.Sprintf(\"    flags_completion+=(%q)\\n\", handlers))\n\t\t\t} else {\n\t\t\t\tWriteStringAndCheck(buf, \"    flags_completion+=(:)\\n\")\n\t\t\t}\n\t\tcase BashCompSubdirsInDir:\n\t\t\tWriteStringAndCheck(buf, fmt.Sprintf(\"    flags_with_completion+=(%q)\\n\", name))\n\n\t\t\tvar ext string\n\t\t\tif len(value) == 1 {\n\t\t\t\text = fmt.Sprintf(\"__%s_handle_subdirs_in_dir_flag \", cmd.Root().Name()) + value[0]\n\t\t\t} else {\n\t\t\t\text = \"_filedir -d\"\n\t\t\t}\n\t\t\tWriteStringAndCheck(buf, fmt.Sprintf(\"    flags_completion+=(%q)\\n\", ext))\n\t\t}\n\t}\n}\n\nconst cbn = \"\\\")\\n\"\n\nfunc writeShortFlag(buf io.StringWriter, flag *pflag.Flag, cmd *Command) {\n\tname := flag.Shorthand\n\tformat := \"    \"\n\tif len(flag.NoOptDefVal) == 0 {\n\t\tformat += \"two_word_\"\n\t}\n\tformat += \"flags+=(\\\"-%s\" + cbn\n\tWriteStringAndCheck(buf, fmt.Sprintf(format, name))\n\twriteFlagHandler(buf, \"-\"+name, flag.Annotations, cmd)\n}\n\nfunc writeFlag(buf io.StringWriter, flag *pflag.Flag, cmd *Command) {\n\tname := flag.Name\n\tformat := \"    flags+=(\\\"--%s\"\n\tif len(flag.NoOptDefVal) == 0 {\n\t\tformat += \"=\"\n\t}\n\tformat += cbn\n\tWriteStringAndCheck(buf, fmt.Sprintf(format, name))\n\tif len(flag.NoOptDefVal) == 0 {\n\t\tformat = \"    two_word_flags+=(\\\"--%s\" + cbn\n\t\tWriteStringAndCheck(buf, fmt.Sprintf(format, name))\n\t}\n\twriteFlagHandler(buf, \"--\"+name, flag.Annotations, cmd)\n}\n\nfunc writeLocalNonPersistentFlag(buf io.StringWriter, flag *pflag.Flag) {\n\tname := flag.Name\n\tformat := \"    local_nonpersistent_flags+=(\\\"--%[1]s\" + cbn\n\tif len(flag.NoOptDefVal) == 0 {\n\t\tformat += \"    local_nonpersistent_flags+=(\\\"--%[1]s=\" + cbn\n\t}\n\tWriteStringAndCheck(buf, fmt.Sprintf(format, name))\n\tif len(flag.Shorthand) > 0 {\n\t\tWriteStringAndCheck(buf, fmt.Sprintf(\"    local_nonpersistent_flags+=(\\\"-%s\\\")\\n\", flag.Shorthand))\n\t}\n}\n\n// prepareCustomAnnotationsForFlags setup annotations for go completions for registered flags\nfunc prepareCustomAnnotationsForFlags(cmd *Command) {\n\tflagCompletionMutex.RLock()\n\tdefer flagCompletionMutex.RUnlock()\n\tfor flag := range flagCompletionFunctions {\n\t\t// Make sure the completion script calls the __*_go_custom_completion function for\n\t\t// every registered flag.  We need to do this here (and not when the flag was registered\n\t\t// for completion) so that we can know the root command name for the prefix\n\t\t// of __<prefix>_go_custom_completion\n\t\tif flag.Annotations == nil {\n\t\t\tflag.Annotations = map[string][]string{}\n\t\t}\n\t\tflag.Annotations[BashCompCustom] = []string{fmt.Sprintf(\"__%[1]s_handle_go_custom_completion\", cmd.Root().Name())}\n\t}\n}\n\nfunc writeFlags(buf io.StringWriter, cmd *Command) {\n\tprepareCustomAnnotationsForFlags(cmd)\n\tWriteStringAndCheck(buf, `    flags=()\n    two_word_flags=()\n    local_nonpersistent_flags=()\n    flags_with_completion=()\n    flags_completion=()\n\n`)\n\n\tif cmd.DisableFlagParsing {\n\t\tWriteStringAndCheck(buf, \"    flag_parsing_disabled=1\\n\")\n\t}\n\n\tlocalNonPersistentFlags := cmd.LocalNonPersistentFlags()\n\tcmd.NonInheritedFlags().VisitAll(func(flag *pflag.Flag) {\n\t\tif nonCompletableFlag(flag) {\n\t\t\treturn\n\t\t}\n\t\twriteFlag(buf, flag, cmd)\n\t\tif len(flag.Shorthand) > 0 {\n\t\t\twriteShortFlag(buf, flag, cmd)\n\t\t}\n\t\t// localNonPersistentFlags are used to stop the completion of subcommands when one is set\n\t\t// if TraverseChildren is true we should allow to complete subcommands\n\t\tif localNonPersistentFlags.Lookup(flag.Name) != nil && !cmd.Root().TraverseChildren {\n\t\t\twriteLocalNonPersistentFlag(buf, flag)\n\t\t}\n\t})\n\tcmd.InheritedFlags().VisitAll(func(flag *pflag.Flag) {\n\t\tif nonCompletableFlag(flag) {\n\t\t\treturn\n\t\t}\n\t\twriteFlag(buf, flag, cmd)\n\t\tif len(flag.Shorthand) > 0 {\n\t\t\twriteShortFlag(buf, flag, cmd)\n\t\t}\n\t})\n\n\tWriteStringAndCheck(buf, \"\\n\")\n}\n\nfunc writeRequiredFlag(buf io.StringWriter, cmd *Command) {\n\tWriteStringAndCheck(buf, \"    must_have_one_flag=()\\n\")\n\tflags := cmd.NonInheritedFlags()\n\tflags.VisitAll(func(flag *pflag.Flag) {\n\t\tif nonCompletableFlag(flag) {\n\t\t\treturn\n\t\t}\n\t\tif _, ok := flag.Annotations[BashCompOneRequiredFlag]; ok {\n\t\t\tformat := \"    must_have_one_flag+=(\\\"--%s\"\n\t\t\tif flag.Value.Type() != \"bool\" {\n\t\t\t\tformat += \"=\"\n\t\t\t}\n\t\t\tformat += cbn\n\t\t\tWriteStringAndCheck(buf, fmt.Sprintf(format, flag.Name))\n\n\t\t\tif len(flag.Shorthand) > 0 {\n\t\t\t\tWriteStringAndCheck(buf, fmt.Sprintf(\"    must_have_one_flag+=(\\\"-%s\"+cbn, flag.Shorthand))\n\t\t\t}\n\t\t}\n\t})\n}\n\nfunc writeRequiredNouns(buf io.StringWriter, cmd *Command) {\n\tWriteStringAndCheck(buf, \"    must_have_one_noun=()\\n\")\n\tsort.Strings(cmd.ValidArgs)\n\tfor _, value := range cmd.ValidArgs {\n\t\t// Remove any description that may be included following a tab character.\n\t\t// Descriptions are not supported by bash completion.\n\t\tvalue = strings.SplitN(value, \"\\t\", 2)[0]\n\t\tWriteStringAndCheck(buf, fmt.Sprintf(\"    must_have_one_noun+=(%q)\\n\", value))\n\t}\n\tif cmd.ValidArgsFunction != nil {\n\t\tWriteStringAndCheck(buf, \"    has_completion_function=1\\n\")\n\t}\n}\n\nfunc writeCmdAliases(buf io.StringWriter, cmd *Command) {\n\tif len(cmd.Aliases) == 0 {\n\t\treturn\n\t}\n\n\tsort.Strings(cmd.Aliases)\n\n\tWriteStringAndCheck(buf, fmt.Sprint(`    if [[ -z \"${BASH_VERSION:-}\" || \"${BASH_VERSINFO[0]:-}\" -gt 3 ]]; then`, \"\\n\"))\n\tfor _, value := range cmd.Aliases {\n\t\tWriteStringAndCheck(buf, fmt.Sprintf(\"        command_aliases+=(%q)\\n\", value))\n\t\tWriteStringAndCheck(buf, fmt.Sprintf(\"        aliashash[%q]=%q\\n\", value, cmd.Name()))\n\t}\n\tWriteStringAndCheck(buf, `    fi`)\n\tWriteStringAndCheck(buf, \"\\n\")\n}\nfunc writeArgAliases(buf io.StringWriter, cmd *Command) {\n\tWriteStringAndCheck(buf, \"    noun_aliases=()\\n\")\n\tsort.Strings(cmd.ArgAliases)\n\tfor _, value := range cmd.ArgAliases {\n\t\tWriteStringAndCheck(buf, fmt.Sprintf(\"    noun_aliases+=(%q)\\n\", value))\n\t}\n}\n\nfunc gen(buf io.StringWriter, cmd *Command) {\n\tfor _, c := range cmd.Commands() {\n\t\tif !c.IsAvailableCommand() && c != cmd.helpCommand {\n\t\t\tcontinue\n\t\t}\n\t\tgen(buf, c)\n\t}\n\tcommandName := cmd.CommandPath()\n\tcommandName = strings.ReplaceAll(commandName, \" \", \"_\")\n\tcommandName = strings.ReplaceAll(commandName, \":\", \"__\")\n\n\tif cmd.Root() == cmd {\n\t\tWriteStringAndCheck(buf, fmt.Sprintf(\"_%s_root_command()\\n{\\n\", commandName))\n\t} else {\n\t\tWriteStringAndCheck(buf, fmt.Sprintf(\"_%s()\\n{\\n\", commandName))\n\t}\n\n\tWriteStringAndCheck(buf, fmt.Sprintf(\"    last_command=%q\\n\", commandName))\n\tWriteStringAndCheck(buf, \"\\n\")\n\tWriteStringAndCheck(buf, \"    command_aliases=()\\n\")\n\tWriteStringAndCheck(buf, \"\\n\")\n\n\twriteCommands(buf, cmd)\n\twriteFlags(buf, cmd)\n\twriteRequiredFlag(buf, cmd)\n\twriteRequiredNouns(buf, cmd)\n\twriteArgAliases(buf, cmd)\n\tWriteStringAndCheck(buf, \"}\\n\\n\")\n}\n\n// GenBashCompletion generates bash completion file and writes to the passed writer.\nfunc (c *Command) GenBashCompletion(w io.Writer) error {\n\tbuf := new(bytes.Buffer)\n\twritePreamble(buf, c.Name())\n\tif len(c.BashCompletionFunction) > 0 {\n\t\tbuf.WriteString(c.BashCompletionFunction + \"\\n\")\n\t}\n\tgen(buf, c)\n\twritePostscript(buf, c.Name())\n\n\t_, err := buf.WriteTo(w)\n\treturn err\n}\n\nfunc nonCompletableFlag(flag *pflag.Flag) bool {\n\treturn flag.Hidden || len(flag.Deprecated) > 0\n}\n\n// GenBashCompletionFile generates bash completion file.\nfunc (c *Command) GenBashCompletionFile(filename string) error {\n\toutFile, err := os.Create(filename)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer outFile.Close()\n\n\treturn c.GenBashCompletion(outFile)\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:51:34"}, {"role": "assistant", "content": "```go\n// Copyright 2013-2023 The Cobra Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage cobra\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"os\"\n\t\"os/exec\"\n\t\"regexp\"\n\t\"strings\"\n\t\"testing\"\n)\n\nfunc checkOmit(t *testing.T, found, unexpected string) {\n\tif strings.Contains(found, unexpected) {\n\t\tt.Errorf(\"Got: %q\\nBut should not have!\\n\", unexpected)\n\t}\n}\n\nfunc check(t *testing.T, found, expected string) {\n\tif !strings.Contains(found, expected) {\n\t\tt.Errorf(\"Expecting to contain: \\n %q\\nGot:\\n %q\\n\", expected, found)\n\t}\n}\n\nfunc checkNumOccurrences(t *testing.T, found, expected string, expectedOccurrences int) {\n\tnumOccurrences := strings.Count(found, expected)\n\tif numOccurrences != expectedOccurrences {\n\t\tt.Errorf(\"Expecting to contain %d occurrences of: \\n %q\\nGot %d:\\n %q\\n\", expectedOccurrences, expected, numOccurrences, found)\n\t}\n}\n\nfunc checkRegex(t *testing.T, found, pattern string) {\n\tmatched, err := regexp.MatchString(pattern, found)\n\tif err != nil {\n\t\tt.Errorf(\"Error thrown performing MatchString: \\n %s\\n\", err)\n\t}\n\tif !matched {\n\t\tt.Errorf(\"Expecting to match: \\n %q\\nGot:\\n %q\\n\", pattern, found)\n\t}\n}\n\nfunc runShellCheck(s string) error {\n\tcmd := exec.Command(\"shellcheck\", \"-s\", \"bash\", \"-\", \"-e\",\n\t\t\"SC2034\", // PREFIX appears unused. Verify it or export it.\n\t)\n\tcmd.Stderr = os.Stderr\n\tcmd.Stdout = os.Stdout\n\n\tstdin, err := cmd.StdinPipe()\n\tif err != nil {\n\t\treturn err\n\t}\n\tgo func() {\n\t\t_, err := stdin.Write([]byte(s))\n\t\tCheckErr(err)\n\n\t\tstdin.Close()\n\t}()\n\n\treturn cmd.Run()\n}\n\n// World worst custom function, just keep telling you to enter hello!\nconst bashCompletionFunc = `__root_custom_func() {\n\tCOMPREPLY=( \"hello\" )\n}\n`\n\nfunc TestBashCompletions(t *testing.T) {\n\trootCmd := &Command{\n\t\tUse:                    \"root\",\n\t\tArgAliases:             []string{\"pods\", \"nodes\", \"services\", \"replicationcontrollers\", \"po\", \"no\", \"svc\", \"rc\"},\n\t\tValidArgs:              []string{\"pod\", \"node\", \"service\", \"replicationcontroller\"},\n\t\tBashCompletionFunction: bashCompletionFunc,\n\t\tRun:                    emptyRun,\n\t}\n\trootCmd.Flags().IntP(\"introot\", \"i\", -1, \"help message for flag introot\")\n\tassertNoErr(t, rootCmd.MarkFlagRequired(\"introot\"))\n\n\t// Filename.\n\trootCmd.Flags().String(\"filename\", \"\", \"Enter a filename\")\n\tassertNoErr(t, rootCmd.MarkFlagFilename(\"filename\", \"json\", \"yaml\", \"yml\"))\n\n\t// Persistent filename.\n\trootCmd.PersistentFlags().String(\"persistent-filename\", \"\", \"Enter a filename\")\n\tassertNoErr(t, rootCmd.MarkPersistentFlagFilename(\"persistent-filename\"))\n\tassertNoErr(t, rootCmd.MarkPersistentFlagRequired(\"persistent-filename\"))\n\n\t// Filename extensions.\n\trootCmd.Flags().String(\"filename-ext\", \"\", \"Enter a filename (extension limited)\")\n\tassertNoErr(t, rootCmd.MarkFlagFilename(\"filename-ext\"))\n\trootCmd.Flags().String(\"custom\", \"\", \"Enter a filename (extension limited)\")\n\tassertNoErr(t, rootCmd.MarkFlagCustom(\"custom\", \"__complete_custom\"))\n\n\t// Subdirectories in a given directory.\n\trootCmd.Flags().String(\"theme\", \"\", \"theme to use (located in /themes/THEMENAME/)\")\n\tassertNoErr(t, rootCmd.Flags().SetAnnotation(\"theme\", BashCompSubdirsInDir, []string{\"themes\"}))\n\n\t// For two word flags check\n\trootCmd.Flags().StringP(\"two\", \"t\", \"\", \"this is two word flags\")\n\trootCmd.Flags().BoolP(\"two-w-default\", \"T\", false, \"this is not two word flags\")\n\n\techoCmd := &Command{\n\t\tUse:     \"echo [string to echo]\",\n\t\tAliases: []string{\"say\"},\n\t\tShort:   \"Echo anything to the screen\",\n\t\tLong:    \"an utterly useless command for testing.\",\n\t\tExample: \"Just run cobra-test echo\",\n\t\tRun:     emptyRun,\n\t}\n\n\techoCmd.Flags().String(\"filename\", \"\", \"Enter a filename\")\n\tassertNoErr(t, echoCmd.MarkFlagFilename(\"filename\", \"json\", \"yaml\", \"yml\"))\n\techoCmd.Flags().String(\"config\", \"\", \"config to use (located in /config/PROFILE/)\")\n\tassertNoErr(t, echoCmd.Flags().SetAnnotation(\"config\", BashCompSubdirsInDir, []string{\"config\"}))\n\n\tprintCmd := &Command{\n\t\tUse:   \"print [string to print]\",\n\t\tArgs:  MinimumNArgs(1),\n\t\tShort: \"Print anything to the screen\",\n\t\tLong:  \"an absolutely utterly useless command for testing.\",\n\t\tRun:   emptyRun,\n\t}\n\n\tdeprecatedCmd := &Command{\n\t\tUse:        \"deprecated [can't do anything here]\",\n\t\tArgs:       NoArgs,\n\t\tShort:      \"A command which is deprecated\",\n\t\tLong:       \"an absolutely utterly useless command for testing deprecation!.\",\n\t\tDeprecated: \"Please use echo instead\",\n\t\tRun:        emptyRun,\n\t}\n\n\tcolonCmd := &Command{\n\t\tUse: \"cmd:colon\",\n\t\tRun: emptyRun,\n\t}\n\n\ttimesCmd := &Command{\n\t\tUse:        \"times [# times] [string to echo]\",\n\t\tSuggestFor: []string{\"counts\"},\n\t\tArgs:       OnlyValidArgs,\n\t\tValidArgs:  []string{\"one\", \"two\", \"three\", \"four\"},\n\t\tShort:      \"Echo anything to the screen more times\",\n\t\tLong:       \"a slightly useless command for testing.\",\n\t\tRun:        emptyRun,\n\t}\n\n\techoCmd.AddCommand(timesCmd)\n\trootCmd.AddCommand(echoCmd, printCmd, deprecatedCmd, colonCmd)\n\n\tbuf := new(bytes.Buffer)\n\tassertNoErr(t, rootCmd.GenBashCompletion(buf))\n\toutput := buf.String()\n\n\tcheck(t, output, \"_root\")\n\tcheck(t, output, \"_root_echo\")\n\tcheck(t, output, \"_root_echo_times\")\n\tcheck(t, output, \"_root_print\")\n\tcheck(t, output, \"_root_cmd__colon\")\n\n\t// check for required flags\n\tcheck(t, output, `must_have_one_flag+=(\"--introot=\")`)\n\tcheck(t, output, `must_have_one_flag+=(\"--persistent-filename=\")`)\n\t// check for custom completion function with both qualified and unqualified name\n\tcheckNumOccurrences(t, output, `__custom_func`, 2)      // 1. check existence, 2. invoke\n\tcheckNumOccurrences(t, output, `__root_custom_func`, 3) // 1. check existence, 2. invoke, 3. actual definition\n\t// check for custom completion function body\n\tcheck(t, output, `COMPREPLY=( \"hello\" )`)\n\t// check for required nouns\n\tcheck(t, output, `must_have_one_noun+=(\"pod\")`)\n\t// check for noun aliases\n\tcheck(t, output, `noun_aliases+=(\"pods\")`)\n\tcheck(t, output, `noun_aliases+=(\"rc\")`)\n\tcheckOmit(t, output, `must_have_one_noun+=(\"pods\")`)\n\t// check for filename extension flags\n\tcheck(t, output, `flags_completion+=(\"_filedir\")`)\n\t// check for filename extension flags\n\tcheck(t, output, `must_have_one_noun+=(\"three\")`)\n\t// check for filename extension flags\n\tcheck(t, output, fmt.Sprintf(`flags_completion+=(\"__%s_handle_filename_extension_flag json|yaml|yml\")`, rootCmd.Name()))\n\t// check for filename extension flags in a subcommand\n\tcheckRegex(t, output, fmt.Sprintf(`_root_echo\\(\\)\\n{[^}]*flags_completion\\+=\\(\"__%s_handle_filename_extension_flag json\\|yaml\\|yml\"\\)`, rootCmd.Name()))\n\t// check for custom flags\n\tcheck(t, output, `flags_completion+=(\"__complete_custom\")`)\n\t// check for subdirs_in_dir flags\n\tcheck(t, output, fmt.Sprintf(`flags_completion+=(\"__%s_handle_subdirs_in_dir_flag themes\")`, rootCmd.Name()))\n\t// check for subdirs_in_dir flags in a subcommand\n\tcheckRegex(t, output, fmt.Sprintf(`_root_echo\\(\\)\\n{[^}]*flags_completion\\+=\\(\"__%s_handle_subdirs_in_dir_flag config\"\\)`, rootCmd.Name()))\n\n\t// check two word flags\n\tcheck(t, output, `two_word_flags+=(\"--two\")`)\n\tcheck(t, output, `two_word_flags+=(\"-t\")`)\n\tcheckOmit(t, output, `two_word_flags+=(\"--two-w-default\")`)\n\tcheckOmit(t, output, `two_word_flags+=(\"-T\")`)\n\n\t// check local nonpersistent flag\n\tcheck(t, output, `local_nonpersistent_flags+=(\"--two\")`)\n\tcheck(t, output, `local_nonpersistent_flags+=(\"--two=\")`)\n\tcheck(t, output, `local_nonpersistent_flags+=(\"-t\")`)\n\tcheck(t, output, `local_nonpersistent_flags+=(\"--two-w-default\")`)\n\tcheck(t, output, `local_nonpersistent_flags+=(\"-T\")`)\n\n\tcheckOmit(t, output, deprecatedCmd.Name())\n\n\t// If available, run shellcheck against the script.\n\tif err := exec.Command(\"which\", \"shellcheck\").Run(); err != nil {\n\t\treturn\n\t}\n\tif err := runShellCheck(output); err != nil {\n\t\tt.Fatalf(\"shellcheck failed: %v\", err)\n\t}\n}\n\nfunc TestBashCompletionHiddenFlag(t *testing.T) {\n\tc := &Command{Use: \"c\", Run: emptyRun}\n\n\tconst flagName = \"hiddenFlag\"\n\tc.Flags().Bool(flagName, false, \"\")\n\tassertNoErr(t, c.Flags().MarkHidden(flagName))\n\n\tbuf := new(bytes.Buffer)\n\tassertNoErr(t, c.GenBashCompletion(buf))\n\toutput := buf.String()\n\n\tif strings.Contains(output, flagName) {\n\t\tt.Errorf(\"Expected completion to not include %q flag: Got %v\", flagName, output)\n\t}\n}\n\nfunc TestBashCompletionDeprecatedFlag(t *testing.T) {\n\tc := &Command{Use: \"c\", Run: emptyRun}\n\n\tconst flagName = \"deprecated-flag\"\n\tc.Flags().Bool(flagName, false, \"\")\n\tassertNoErr(t, c.Flags().MarkDeprecated(flagName, \"use --not-deprecated instead\"))\n\n\tbuf := new(bytes.Buffer)\n\tassertNoErr(t, c.GenBashCompletion(buf))\n\toutput := buf.String()\n\n\tif strings.Contains(output, flagName) {\n\t\tt.Errorf(\"expected completion to not include %q flag: Got %v\", flagName, output)\n\t}\n}\n\nfunc TestBashCompletionTraverseChildren(t *testing.T) {\n\tc := &Command{Use: \"c\", Run: emptyRun, TraverseChildren: true}\n\n\tc.Flags().StringP(\"string-flag\", \"s\", \"\", \"string flag\")\n\tc.Flags().BoolP(\"bool-flag\", \"b\", false, \"bool flag\")\n\n\tbuf := new(bytes.Buffer)\n\tassertNoErr(t, c.GenBashCompletion(buf))\n\toutput := buf.String()\n\n\t// check that local nonpersistent flag are not set since we have TraverseChildren set to true\n\tcheckOmit(t, output, `local_nonpersistent_flags+=(\"--string-flag\")`)\n\tcheckOmit(t, output, `local_nonpersistent_flags+=(\"--string-flag=\")`)\n\tcheckOmit(t, output, `local_nonpersistent_flags+=(\"-s\")`)\n\tcheckOmit(t, output, `local_nonpersistent_flags+=(\"--bool-flag\")`)\n\tcheckOmit(t, output, `local_nonpersistent_flags+=(\"-b\")`)\n}\n\nfunc TestBashCompletionNoActiveHelp(t *testing.T) {\n\tc := &Command{Use: \"c\", Run: emptyRun}\n\n\tbuf := new(bytes.Buffer)\n\tassertNoErr(t, c.GenBashCompletion(buf))\n\toutput := buf.String()\n\n\t// check that active help is being disabled\n\tactiveHelpVar := activeHelpEnvVar(c.Name())\n\tcheck(t, output, fmt.Sprintf(\"%s=0\", activeHelpVar))\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: cobra\nSource File Path(s): ['cobra\\\\cobra_test.go\\\\source_files\\\\cobra.go']\nTest File Path: cobra\\cobra_test.go\\cobra_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: cobra\\cobra_test.go\\source_files\\cobra.go\n// Copyright 2013-2023 The Cobra Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Commands similar to git, go tools and other modern CLI tools\n// inspired by go, go-Commander, gh and subcommand\n\npackage cobra\n\nimport (\n\t\"fmt\"\n\t\"io\"\n\t\"os\"\n\t\"reflect\"\n\t\"strconv\"\n\t\"strings\"\n\t\"text/template\"\n\t\"time\"\n\t\"unicode\"\n)\n\nvar templateFuncs = template.FuncMap{\n\t\"trim\":                    strings.TrimSpace,\n\t\"trimRightSpace\":          trimRightSpace,\n\t\"trimTrailingWhitespaces\": trimRightSpace,\n\t\"appendIfNotPresent\":      appendIfNotPresent,\n\t\"rpad\":                    rpad,\n\t\"gt\":                      Gt,\n\t\"eq\":                      Eq,\n}\n\nvar initializers []func()\nvar finalizers []func()\n\nconst (\n\tdefaultPrefixMatching   = false\n\tdefaultCommandSorting   = true\n\tdefaultCaseInsensitive  = false\n\tdefaultTraverseRunHooks = false\n)\n\n// EnablePrefixMatching allows setting automatic prefix matching. Automatic prefix matching can be a dangerous thing\n// to automatically enable in CLI tools.\n// Set this to true to enable it.\nvar EnablePrefixMatching = defaultPrefixMatching\n\n// EnableCommandSorting controls sorting of the slice of commands, which is turned on by default.\n// To disable sorting, set it to false.\nvar EnableCommandSorting = defaultCommandSorting\n\n// EnableCaseInsensitive allows case-insensitive commands names. (case sensitive by default)\nvar EnableCaseInsensitive = defaultCaseInsensitive\n\n// EnableTraverseRunHooks executes persistent pre-run and post-run hooks from all parents.\n// By default this is disabled, which means only the first run hook to be found is executed.\nvar EnableTraverseRunHooks = defaultTraverseRunHooks\n\n// MousetrapHelpText enables an information splash screen on Windows\n// if the CLI is started from explorer.exe.\n// To disable the mousetrap, just set this variable to blank string (\"\").\n// Works only on Microsoft Windows.\nvar MousetrapHelpText = `This is a command line tool.\n\nYou need to open cmd.exe and run it from there.\n`\n\n// MousetrapDisplayDuration controls how long the MousetrapHelpText message is displayed on Windows\n// if the CLI is started from explorer.exe. Set to 0 to wait for the return key to be pressed.\n// To disable the mousetrap, just set MousetrapHelpText to blank string (\"\").\n// Works only on Microsoft Windows.\nvar MousetrapDisplayDuration = 5 * time.Second\n\n// AddTemplateFunc adds a template function that's available to Usage and Help\n// template generation.\nfunc AddTemplateFunc(name string, tmplFunc interface{}) {\n\ttemplateFuncs[name] = tmplFunc\n}\n\n// AddTemplateFuncs adds multiple template functions that are available to Usage and\n// Help template generation.\nfunc AddTemplateFuncs(tmplFuncs template.FuncMap) {\n\tfor k, v := range tmplFuncs {\n\t\ttemplateFuncs[k] = v\n\t}\n}\n\n// OnInitialize sets the passed functions to be run when each command's\n// Execute method is called.\nfunc OnInitialize(y ...func()) {\n\tinitializers = append(initializers, y...)\n}\n\n// OnFinalize sets the passed functions to be run when each command's\n// Execute method is terminated.\nfunc OnFinalize(y ...func()) {\n\tfinalizers = append(finalizers, y...)\n}\n\n// FIXME Gt is unused by cobra and should be removed in a version 2. It exists only for compatibility with users of cobra.\n\n// Gt takes two types and checks whether the first type is greater than the second. In case of types Arrays, Chans,\n// Maps and Slices, Gt will compare their lengths. Ints are compared directly while strings are first parsed as\n// ints and then compared.\nfunc Gt(a interface{}, b interface{}) bool {\n\tvar left, right int64\n\tav := reflect.ValueOf(a)\n\n\tswitch av.Kind() {\n\tcase reflect.Array, reflect.Chan, reflect.Map, reflect.Slice:\n\t\tleft = int64(av.Len())\n\tcase reflect.Int, reflect.Int8, reflect.Int16, reflect.Int32, reflect.Int64:\n\t\tleft = av.Int()\n\tcase reflect.String:\n\t\tleft, _ = strconv.ParseInt(av.String(), 10, 64)\n\t}\n\n\tbv := reflect.ValueOf(b)\n\n\tswitch bv.Kind() {\n\tcase reflect.Array, reflect.Chan, reflect.Map, reflect.Slice:\n\t\tright = int64(bv.Len())\n\tcase reflect.Int, reflect.Int8, reflect.Int16, reflect.Int32, reflect.Int64:\n\t\tright = bv.Int()\n\tcase reflect.String:\n\t\tright, _ = strconv.ParseInt(bv.String(), 10, 64)\n\t}\n\n\treturn left > right\n}\n\n// FIXME Eq is unused by cobra and should be removed in a version 2. It exists only for compatibility with users of cobra.\n\n// Eq takes two types and checks whether they are equal. Supported types are int and string. Unsupported types will panic.\nfunc Eq(a interface{}, b interface{}) bool {\n\tav := reflect.ValueOf(a)\n\tbv := reflect.ValueOf(b)\n\n\tswitch av.Kind() {\n\tcase reflect.Array, reflect.Chan, reflect.Map, reflect.Slice:\n\t\tpanic(\"Eq called on unsupported type\")\n\tcase reflect.Int, reflect.Int8, reflect.Int16, reflect.Int32, reflect.Int64:\n\t\treturn av.Int() == bv.Int()\n\tcase reflect.String:\n\t\treturn av.String() == bv.String()\n\t}\n\treturn false\n}\n\nfunc trimRightSpace(s string) string {\n\treturn strings.TrimRightFunc(s, unicode.IsSpace)\n}\n\n// FIXME appendIfNotPresent is unused by cobra and should be removed in a version 2. It exists only for compatibility with users of cobra.\n\n// appendIfNotPresent will append stringToAppend to the end of s, but only if it's not yet present in s.\nfunc appendIfNotPresent(s, stringToAppend string) string {\n\tif strings.Contains(s, stringToAppend) {\n\t\treturn s\n\t}\n\treturn s + \" \" + stringToAppend\n}\n\n// rpad adds padding to the right of a string.\nfunc rpad(s string, padding int) string {\n\tformattedString := fmt.Sprintf(\"%%-%ds\", padding)\n\treturn fmt.Sprintf(formattedString, s)\n}\n\nfunc tmpl(text string) *tmplFunc {\n\treturn &tmplFunc{\n\t\ttmpl: text,\n\t\tfn: func(w io.Writer, data interface{}) error {\n\t\t\tt := template.New(\"top\")\n\t\t\tt.Funcs(templateFuncs)\n\t\t\ttemplate.Must(t.Parse(text))\n\t\t\treturn t.Execute(w, data)\n\t\t},\n\t}\n}\n\n// ld compares two strings and returns the levenshtein distance between them.\nfunc ld(s, t string, ignoreCase bool) int {\n\tif ignoreCase {\n\t\ts = strings.ToLower(s)\n\t\tt = strings.ToLower(t)\n\t}\n\td := make([][]int, len(s)+1)\n\tfor i := range d {\n\t\td[i] = make([]int, len(t)+1)\n\t\td[i][0] = i\n\t}\n\tfor j := range d[0] {\n\t\td[0][j] = j\n\t}\n\tfor j := 1; j <= len(t); j++ {\n\t\tfor i := 1; i <= len(s); i++ {\n\t\t\tif s[i-1] == t[j-1] {\n\t\t\t\td[i][j] = d[i-1][j-1]\n\t\t\t} else {\n\t\t\t\tmin := d[i-1][j]\n\t\t\t\tif d[i][j-1] < min {\n\t\t\t\t\tmin = d[i][j-1]\n\t\t\t\t}\n\t\t\t\tif d[i-1][j-1] < min {\n\t\t\t\t\tmin = d[i-1][j-1]\n\t\t\t\t}\n\t\t\t\td[i][j] = min + 1\n\t\t\t}\n\t\t}\n\n\t}\n\treturn d[len(s)][len(t)]\n}\n\nfunc stringInSlice(a string, list []string) bool {\n\tfor _, b := range list {\n\t\tif b == a {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// CheckErr prints the msg with the prefix 'Error:' and exits with error code 1. If the msg is nil, it does nothing.\nfunc CheckErr(msg interface{}) {\n\tif msg != nil {\n\t\tfmt.Fprintln(os.Stderr, \"Error:\", msg)\n\t\tos.Exit(1)\n\t}\n}\n\n// WriteStringAndCheck writes a string into a buffer, and checks if the error is not nil.\nfunc WriteStringAndCheck(b io.StringWriter, s string) {\n\t_, err := b.WriteString(s)\n\tCheckErr(err)\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:51:34"}, {"role": "assistant", "content": "```go\n// Copyright 2013-2023 The Cobra Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage cobra\n\nimport (\n\t\"os\"\n\t\"os/exec\"\n\t\"path/filepath\"\n\t\"runtime\"\n\t\"strings\"\n\t\"testing\"\n\t\"text/template\"\n)\n\nfunc assertNoErr(t *testing.T, e error) {\n\tif e != nil {\n\t\tt.Error(e)\n\t}\n}\n\nfunc TestAddTemplateFunctions(t *testing.T) {\n\tAddTemplateFunc(\"t\", func() bool { return true })\n\tAddTemplateFuncs(template.FuncMap{\n\t\t\"f\": func() bool { return false },\n\t\t\"h\": func() string { return \"Hello,\" },\n\t\t\"w\": func() string { return \"world.\" }})\n\n\tc := &Command{}\n\tc.SetUsageTemplate(`{{if t}}{{h}}{{end}}{{if f}}{{h}}{{end}} {{w}}`)\n\n\tconst expected = \"Hello, world.\"\n\tif got := c.UsageString(); got != expected {\n\t\tt.Errorf(\"Expected UsageString: %v\\nGot: %v\", expected, got)\n\t}\n}\n\nfunc TestLevenshteinDistance(t *testing.T) {\n\ttests := []struct {\n\t\tname       string\n\t\ts          string\n\t\tt          string\n\t\tignoreCase bool\n\t\texpected   int\n\t}{\n\t\t{\n\t\t\tname:       \"Equal strings (case-sensitive)\",\n\t\t\ts:          \"hello\",\n\t\t\tt:          \"hello\",\n\t\t\tignoreCase: false,\n\t\t\texpected:   0,\n\t\t},\n\t\t{\n\t\t\tname:       \"Equal strings (case-insensitive)\",\n\t\t\ts:          \"Hello\",\n\t\t\tt:          \"hello\",\n\t\t\tignoreCase: true,\n\t\t\texpected:   0,\n\t\t},\n\t\t{\n\t\t\tname:       \"Different strings (case-sensitive)\",\n\t\t\ts:          \"kitten\",\n\t\t\tt:          \"sitting\",\n\t\t\tignoreCase: false,\n\t\t\texpected:   3,\n\t\t},\n\t\t{\n\t\t\tname:       \"Different strings (case-insensitive)\",\n\t\t\ts:          \"Kitten\",\n\t\t\tt:          \"Sitting\",\n\t\t\tignoreCase: true,\n\t\t\texpected:   3,\n\t\t},\n\t\t{\n\t\t\tname:       \"Empty strings\",\n\t\t\ts:          \"\",\n\t\t\tt:          \"\",\n\t\t\tignoreCase: false,\n\t\t\texpected:   0,\n\t\t},\n\t\t{\n\t\t\tname:       \"One empty string\",\n\t\t\ts:          \"abc\",\n\t\t\tt:          \"\",\n\t\t\tignoreCase: false,\n\t\t\texpected:   3,\n\t\t},\n\t\t{\n\t\t\tname:       \"Both empty strings\",\n\t\t\ts:          \"\",\n\t\t\tt:          \"\",\n\t\t\tignoreCase: true,\n\t\t\texpected:   0,\n\t\t},\n\t}\n\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\t// Act\n\t\t\tgot := ld(tt.s, tt.t, tt.ignoreCase)\n\n\t\t\t// Assert\n\t\t\tif got != tt.expected {\n\t\t\t\tt.Errorf(\"Expected ld: %v\\nGot: %v\", tt.expected, got)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestStringInSlice(t *testing.T) {\n\ttests := []struct {\n\t\tname     string\n\t\ta        string\n\t\tlist     []string\n\t\texpected bool\n\t}{\n\t\t{\n\t\t\tname:     \"String in slice (case-sensitive)\",\n\t\t\ta:        \"apple\",\n\t\t\tlist:     []string{\"orange\", \"banana\", \"apple\", \"grape\"},\n\t\t\texpected: true,\n\t\t},\n\t\t{\n\t\t\tname:     \"String not in slice (case-sensitive)\",\n\t\t\ta:        \"pear\",\n\t\t\tlist:     []string{\"orange\", \"banana\", \"apple\", \"grape\"},\n\t\t\texpected: false,\n\t\t},\n\t\t{\n\t\t\tname:     \"String in slice (case-insensitive)\",\n\t\t\ta:        \"APPLE\",\n\t\t\tlist:     []string{\"orange\", \"banana\", \"apple\", \"grape\"},\n\t\t\texpected: false,\n\t\t},\n\t\t{\n\t\t\tname:     \"Empty slice\",\n\t\t\ta:        \"apple\",\n\t\t\tlist:     []string{},\n\t\t\texpected: false,\n\t\t},\n\t\t{\n\t\t\tname:     \"Empty string\",\n\t\t\ta:        \"\",\n\t\t\tlist:     []string{\"orange\", \"banana\", \"apple\", \"grape\"},\n\t\t\texpected: false,\n\t\t},\n\t\t{\n\t\t\tname:     \"Empty strings match\",\n\t\t\ta:        \"\",\n\t\t\tlist:     []string{\"orange\", \"\"},\n\t\t\texpected: true,\n\t\t},\n\t\t{\n\t\t\tname:     \"Empty string in empty slice\",\n\t\t\ta:        \"\",\n\t\t\tlist:     []string{},\n\t\t\texpected: false,\n\t\t},\n\t}\n\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\t// Act\n\t\t\tgot := stringInSlice(tt.a, tt.list)\n\n\t\t\t// Assert\n\t\t\tif got != tt.expected {\n\t\t\t\tt.Errorf(\"Expected stringInSlice: %v\\nGot: %v\", tt.expected, got)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestRpad(t *testing.T) {\n\ttests := []struct {\n\t\tname        string\n\t\tinputString string\n\t\tpadding     int\n\t\texpected    string\n\t}{\n\t\t{\n\t\t\tname:        \"Padding required\",\n\t\t\tinputString: \"Hello\",\n\t\t\tpadding:     10,\n\t\t\texpected:    \"Hello     \",\n\t\t},\n\t\t{\n\t\t\tname:        \"No padding required\",\n\t\t\tinputString: \"World\",\n\t\t\tpadding:     5,\n\t\t\texpected:    \"World\",\n\t\t},\n\t\t{\n\t\t\tname:        \"Empty string\",\n\t\t\tinputString: \"\",\n\t\t\tpadding:     8,\n\t\t\texpected:    \"        \",\n\t\t},\n\t\t{\n\t\t\tname:        \"Zero padding\",\n\t\t\tinputString: \"cobra\",\n\t\t\tpadding:     0,\n\t\t\texpected:    \"cobra\",\n\t\t},\n\t}\n\n\tfor _, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\t// Act\n\t\t\tgot := rpad(tt.inputString, tt.padding)\n\n\t\t\t// Assert\n\t\t\tif got != tt.expected {\n\t\t\t\tt.Errorf(\"Expected rpad: %v\\nGot: %v\", tt.expected, got)\n\t\t\t}\n\t\t})\n\t}\n}\n\n// TestDeadcodeElimination checks that a simple program using cobra in its\n// default configuration is linked taking full advantage of the linker's\n// deadcode elimination step.\n//\n// If reflect.Value.MethodByName/reflect.Value.Method are reachable the\n// linker will not always be able to prove that exported methods are\n// unreachable, making deadcode elimination less effective. Using\n// text/template and html/template makes reflect.Value.MethodByName\n// reachable.\n// Since cobra can use text/template templates this test checks that in its\n// default configuration that code path can be proven to be unreachable by\n// the linker.\n//\n// See also: https://github.com/spf13/cobra/pull/1956\nfunc TestDeadcodeElimination(t *testing.T) {\n\tif runtime.GOOS == \"windows\" {\n\t\tt.Skip(\"go tool nm fails on windows\")\n\t}\n\n\t// check that a simple program using cobra in its default configuration is\n\t// linked with deadcode elimination enabled.\n\tconst (\n\t\tdirname  = \"test_deadcode\"\n\t\tprogname = \"test_deadcode_elimination\"\n\t)\n\t_ = os.Mkdir(dirname, 0770)\n\tdefer os.RemoveAll(dirname)\n\tfilename := filepath.Join(dirname, progname+\".go\")\n\terr := os.WriteFile(filename, []byte(`package main\n\nimport (\n\t\"fmt\"\n\t\"os\"\n\n\t\"github.com/spf13/cobra\"\n)\n\nvar rootCmd = &cobra.Command{\n\tVersion: \"1.0\",\n\tUse:     \"example_program\",\n\tShort:   \"example_program - test fixture to check that deadcode elimination is allowed\",\n\tRun: func(cmd *cobra.Command, args []string) {\n\t\tfmt.Println(\"hello world\")\n\t},\n\tAliases: []string{\"alias1\", \"alias2\"},\n\tExample: \"stringer --help\",\n}\n\nfunc main() {\n\tif err := rootCmd.Execute(); err != nil {\n\t\tfmt.Fprintf(os.Stderr, \"Whoops. There was an error while executing your CLI '%s'\", err)\n\t\tos.Exit(1)\n\t}\n}\n`), 0600)\n\tif err != nil {\n\t\tt.Fatalf(\"could not write test program: %v\", err)\n\t}\n\tbuf, err := exec.Command(\"go\", \"build\", filename).CombinedOutput()\n\tif err != nil {\n\t\tt.Fatalf(\"could not compile test program: %s\", string(buf))\n\t}\n\tdefer os.Remove(progname)\n\tbuf, err = exec.Command(\"go\", \"tool\", \"nm\", progname).CombinedOutput()\n\tif err != nil {\n\t\tt.Fatalf(\"could not run go tool nm: %v\", err)\n\t}\n\tif strings.Contains(string(buf), \"MethodByName\") {\n\t\tt.Error(\"compiled programs contains MethodByName symbol\")\n\t}\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: cobra\nSource File Path(s): ['cobra\\\\fish_completions_test.go\\\\source_files\\\\fish_completions.go']\nTest File Path: cobra\\fish_completions_test.go\\fish_completions_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: cobra\\fish_completions_test.go\\source_files\\fish_completions.go\n// Copyright 2013-2023 The Cobra Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage cobra\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"io\"\n\t\"os\"\n\t\"strings\"\n)\n\nfunc genFishComp(buf io.StringWriter, name string, includeDesc bool) {\n\t// Variables should not contain a '-' or ':' character\n\tnameForVar := name\n\tnameForVar = strings.ReplaceAll(nameForVar, \"-\", \"_\")\n\tnameForVar = strings.ReplaceAll(nameForVar, \":\", \"_\")\n\n\tcompCmd := ShellCompRequestCmd\n\tif !includeDesc {\n\t\tcompCmd = ShellCompNoDescRequestCmd\n\t}\n\tWriteStringAndCheck(buf, fmt.Sprintf(\"# fish completion for %-36s -*- shell-script -*-\\n\", name))\n\tWriteStringAndCheck(buf, fmt.Sprintf(`\nfunction __%[1]s_debug\n    set -l file \"$BASH_COMP_DEBUG_FILE\"\n    if test -n \"$file\"\n        echo \"$argv\" >> $file\n    end\nend\n\nfunction __%[1]s_perform_completion\n    __%[1]s_debug \"Starting __%[1]s_perform_completion\"\n\n    # Extract all args except the last one\n    set -l args (commandline -opc)\n    # Extract the last arg and escape it in case it is a space\n    set -l lastArg (string escape -- (commandline -ct))\n\n    __%[1]s_debug \"args: $args\"\n    __%[1]s_debug \"last arg: $lastArg\"\n\n    # Disable ActiveHelp which is not supported for fish shell\n    set -l requestComp \"%[10]s=0 $args[1] %[3]s $args[2..-1] $lastArg\"\n\n    __%[1]s_debug \"Calling $requestComp\"\n    set -l results (eval $requestComp 2> /dev/null)\n\n    # Some programs may output extra empty lines after the directive.\n    # Let's ignore them or else it will break completion.\n    # Ref: https://github.com/spf13/cobra/issues/1279\n    for line in $results[-1..1]\n        if test (string trim -- $line) = \"\"\n            # Found an empty line, remove it\n            set results $results[1..-2]\n        else\n            # Found non-empty line, we have our proper output\n            break\n        end\n    end\n\n    set -l comps $results[1..-2]\n    set -l directiveLine $results[-1]\n\n    # For Fish, when completing a flag with an = (e.g., <program> -n=<TAB>)\n    # completions must be prefixed with the flag\n    set -l flagPrefix (string match -r -- '-.*=' \"$lastArg\")\n\n    __%[1]s_debug \"Comps: $comps\"\n    __%[1]s_debug \"DirectiveLine: $directiveLine\"\n    __%[1]s_debug \"flagPrefix: $flagPrefix\"\n\n    for comp in $comps\n        printf \"%%s%%s\\n\" \"$flagPrefix\" \"$comp\"\n    end\n\n    printf \"%%s\\n\" \"$directiveLine\"\nend\n\n# this function limits calls to __%[1]s_perform_completion, by caching the result behind $__%[1]s_perform_completion_once_result\nfunction __%[1]s_perform_completion_once\n    __%[1]s_debug \"Starting __%[1]s_perform_completion_once\"\n\n    if test -n \"$__%[1]s_perform_completion_once_result\"\n        __%[1]s_debug \"Seems like a valid result already exists, skipping __%[1]s_perform_completion\"\n        return 0\n    end\n\n    set --global __%[1]s_perform_completion_once_result (__%[1]s_perform_completion)\n    if test -z \"$__%[1]s_perform_completion_once_result\"\n        __%[1]s_debug \"No completions, probably due to a failure\"\n        return 1\n    end\n\n    __%[1]s_debug \"Performed completions and set __%[1]s_perform_completion_once_result\"\n    return 0\nend\n\n# this function is used to clear the $__%[1]s_perform_completion_once_result variable after completions are run\nfunction __%[1]s_clear_perform_completion_once_result\n    __%[1]s_debug \"\"\n    __%[1]s_debug \"========= clearing previously set __%[1]s_perform_completion_once_result variable ==========\"\n    set --erase __%[1]s_perform_completion_once_result\n    __%[1]s_debug \"Successfully erased the variable __%[1]s_perform_completion_once_result\"\nend\n\nfunction __%[1]s_requires_order_preservation\n    __%[1]s_debug \"\"\n    __%[1]s_debug \"========= checking if order preservation is required ==========\"\n\n    __%[1]s_perform_completion_once\n    if test -z \"$__%[1]s_perform_completion_once_result\"\n        __%[1]s_debug \"Error determining if order preservation is required\"\n        return 1\n    end\n\n    set -l directive (string sub --start 2 $__%[1]s_perform_completion_once_result[-1])\n    __%[1]s_debug \"Directive is: $directive\"\n\n    set -l shellCompDirectiveKeepOrder %[9]d\n    set -l keeporder (math (math --scale 0 $directive / $shellCompDirectiveKeepOrder) %% 2)\n    __%[1]s_debug \"Keeporder is: $keeporder\"\n\n    if test $keeporder -ne 0\n        __%[1]s_debug \"This does require order preservation\"\n        return 0\n    end\n\n    __%[1]s_debug \"This doesn't require order preservation\"\n    return 1\nend\n\n\n# This function does two things:\n# - Obtain the completions and store them in the global __%[1]s_comp_results\n# - Return false if file completion should be performed\nfunction __%[1]s_prepare_completions\n    __%[1]s_debug \"\"\n    __%[1]s_debug \"========= starting completion logic ==========\"\n\n    # Start fresh\n    set --erase __%[1]s_comp_results\n\n    __%[1]s_perform_completion_once\n    __%[1]s_debug \"Completion results: $__%[1]s_perform_completion_once_result\"\n\n    if test -z \"$__%[1]s_perform_completion_once_result\"\n        __%[1]s_debug \"No completion, probably due to a failure\"\n        # Might as well do file completion, in case it helps\n        return 1\n    end\n\n    set -l directive (string sub --start 2 $__%[1]s_perform_completion_once_result[-1])\n    set --global __%[1]s_comp_results $__%[1]s_perform_completion_once_result[1..-2]\n\n    __%[1]s_debug \"Completions are: $__%[1]s_comp_results\"\n    __%[1]s_debug \"Directive is: $directive\"\n\n    set -l shellCompDirectiveError %[4]d\n    set -l shellCompDirectiveNoSpace %[5]d\n    set -l shellCompDirectiveNoFileComp %[6]d\n    set -l shellCompDirectiveFilterFileExt %[7]d\n    set -l shellCompDirectiveFilterDirs %[8]d\n\n    if test -z \"$directive\"\n        set directive 0\n    end\n\n    set -l compErr (math (math --scale 0 $directive / $shellCompDirectiveError) %% 2)\n    if test $compErr -eq 1\n        __%[1]s_debug \"Received error directive: aborting.\"\n        # Might as well do file completion, in case it helps\n        return 1\n    end\n\n    set -l filefilter (math (math --scale 0 $directive / $shellCompDirectiveFilterFileExt) %% 2)\n    set -l dirfilter (math (math --scale 0 $directive / $shellCompDirectiveFilterDirs) %% 2)\n    if test $filefilter -eq 1; or test $dirfilter -eq 1\n        __%[1]s_debug \"File extension filtering or directory filtering not supported\"\n        # Do full file completion instead\n        return 1\n    end\n\n    set -l nospace (math (math --scale 0 $directive / $shellCompDirectiveNoSpace) %% 2)\n    set -l nofiles (math (math --scale 0 $directive / $shellCompDirectiveNoFileComp) %% 2)\n\n    __%[1]s_debug \"nospace: $nospace, nofiles: $nofiles\"\n\n    # If we want to prevent a space, or if file completion is NOT disabled,\n    # we need to count the number of valid completions.\n    # To do so, we will filter on prefix as the completions we have received\n    # may not already be filtered so as to allow fish to match on different\n    # criteria than the prefix.\n    if test $nospace -ne 0; or test $nofiles -eq 0\n        set -l prefix (commandline -t | string escape --style=regex)\n        __%[1]s_debug \"prefix: $prefix\"\n\n        set -l completions (string match -r -- \"^$prefix.*\" $__%[1]s_comp_results)\n        set --global __%[1]s_comp_results $completions\n        __%[1]s_debug \"Filtered completions are: $__%[1]s_comp_results\"\n\n        # Important not to quote the variable for count to work\n        set -l numComps (count $__%[1]s_comp_results)\n        __%[1]s_debug \"numComps: $numComps\"\n\n        if test $numComps -eq 1; and test $nospace -ne 0\n            # We must first split on \\t to get rid of the descriptions to be\n            # able to check what the actual completion will be.\n            # We don't need descriptions anyway since there is only a single\n            # real completion which the shell will expand immediately.\n            set -l split (string split --max 1 \\t $__%[1]s_comp_results[1])\n\n            # Fish won't add a space if the completion ends with any\n            # of the following characters: @=/:.,\n            set -l lastChar (string sub -s -1 -- $split)\n            if not string match -r -q \"[@=/:.,]\" -- \"$lastChar\"\n                # In other cases, to support the \"nospace\" directive we trick the shell\n                # by outputting an extra, longer completion.\n                __%[1]s_debug \"Adding second completion to perform nospace directive\"\n                set --global __%[1]s_comp_results $split[1] $split[1].\n                __%[1]s_debug \"Completions are now: $__%[1]s_comp_results\"\n            end\n        end\n\n        if test $numComps -eq 0; and test $nofiles -eq 0\n            # To be consistent with bash and zsh, we only trigger file\n            # completion when there are no other completions\n            __%[1]s_debug \"Requesting file completion\"\n            return 1\n        end\n    end\n\n    return 0\nend\n\n# Since Fish completions are only loaded once the user triggers them, we trigger them ourselves\n# so we can properly delete any completions provided by another script.\n# Only do this if the program can be found, or else fish may print some errors; besides,\n# the existing completions will only be loaded if the program can be found.\nif type -q \"%[2]s\"\n    # The space after the program name is essential to trigger completion for the program\n    # and not completion of the program name itself.\n    # Also, we use '> /dev/null 2>&1' since '&>' is not supported in older versions of fish.\n    complete --do-complete \"%[2]s \" > /dev/null 2>&1\nend\n\n# Remove any pre-existing completions for the program since we will be handling all of them.\ncomplete -c %[2]s -e\n\n# this will get called after the two calls below and clear the $__%[1]s_perform_completion_once_result global\ncomplete -c %[2]s -n '__%[1]s_clear_perform_completion_once_result'\n# The call to __%[1]s_prepare_completions will setup __%[1]s_comp_results\n# which provides the program's completion choices.\n# If this doesn't require order preservation, we don't use the -k flag\ncomplete -c %[2]s -n 'not __%[1]s_requires_order_preservation && __%[1]s_prepare_completions' -f -a '$__%[1]s_comp_results'\n# otherwise we use the -k flag\ncomplete -k -c %[2]s -n '__%[1]s_requires_order_preservation && __%[1]s_prepare_completions' -f -a '$__%[1]s_comp_results'\n`, nameForVar, name, compCmd,\n\t\tShellCompDirectiveError, ShellCompDirectiveNoSpace, ShellCompDirectiveNoFileComp,\n\t\tShellCompDirectiveFilterFileExt, ShellCompDirectiveFilterDirs, ShellCompDirectiveKeepOrder, activeHelpEnvVar(name)))\n}\n\n// GenFishCompletion generates fish completion file and writes to the passed writer.\nfunc (c *Command) GenFishCompletion(w io.Writer, includeDesc bool) error {\n\tbuf := new(bytes.Buffer)\n\tgenFishComp(buf, c.Name(), includeDesc)\n\t_, err := buf.WriteTo(w)\n\treturn err\n}\n\n// GenFishCompletionFile generates fish completion file.\nfunc (c *Command) GenFishCompletionFile(filename string, includeDesc bool) error {\n\toutFile, err := os.Create(filename)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer outFile.Close()\n\n\treturn c.GenFishCompletion(outFile, includeDesc)\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:51:34"}, {"role": "assistant", "content": "```go\n// Copyright 2013-2023 The Cobra Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage cobra\n\nimport (\n\t\"bytes\"\n\t\"errors\"\n\t\"fmt\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"testing\"\n)\n\nfunc TestCompleteNoDesCmdInFishScript(t *testing.T) {\n\trootCmd := &Command{Use: \"root\", Args: NoArgs, Run: emptyRun}\n\tchild := &Command{\n\t\tUse:               \"child\",\n\t\tValidArgsFunction: validArgsFunc,\n\t\tRun:               emptyRun,\n\t}\n\trootCmd.AddCommand(child)\n\n\tbuf := new(bytes.Buffer)\n\tassertNoErr(t, rootCmd.GenFishCompletion(buf, false))\n\toutput := buf.String()\n\n\tcheck(t, output, ShellCompNoDescRequestCmd)\n}\n\nfunc TestCompleteCmdInFishScript(t *testing.T) {\n\trootCmd := &Command{Use: \"root\", Args: NoArgs, Run: emptyRun}\n\tchild := &Command{\n\t\tUse:               \"child\",\n\t\tValidArgsFunction: validArgsFunc,\n\t\tRun:               emptyRun,\n\t}\n\trootCmd.AddCommand(child)\n\n\tbuf := new(bytes.Buffer)\n\tassertNoErr(t, rootCmd.GenFishCompletion(buf, true))\n\toutput := buf.String()\n\n\tcheck(t, output, ShellCompRequestCmd)\n\tcheckOmit(t, output, ShellCompNoDescRequestCmd)\n}\n\nfunc TestProgWithDash(t *testing.T) {\n\trootCmd := &Command{Use: \"root-dash\", Args: NoArgs, Run: emptyRun}\n\tbuf := new(bytes.Buffer)\n\tassertNoErr(t, rootCmd.GenFishCompletion(buf, false))\n\toutput := buf.String()\n\n\t// Functions name should have replace the '-'\n\tcheck(t, output, \"__root_dash_perform_completion\")\n\tcheckOmit(t, output, \"__root-dash_perform_completion\")\n\n\t// The command name should not have replaced the '-'\n\tcheck(t, output, \"-c root-dash\")\n\tcheckOmit(t, output, \"-c root_dash\")\n}\n\nfunc TestProgWithColon(t *testing.T) {\n\trootCmd := &Command{Use: \"root:colon\", Args: NoArgs, Run: emptyRun}\n\tbuf := new(bytes.Buffer)\n\tassertNoErr(t, rootCmd.GenFishCompletion(buf, false))\n\toutput := buf.String()\n\n\t// Functions name should have replace the ':'\n\tcheck(t, output, \"__root_colon_perform_completion\")\n\tcheckOmit(t, output, \"__root:colon_perform_completion\")\n\n\t// The command name should not have replaced the ':'\n\tcheck(t, output, \"-c root:colon\")\n\tcheckOmit(t, output, \"-c root_colon\")\n}\n\nfunc TestFishCompletionNoActiveHelp(t *testing.T) {\n\tc := &Command{Use: \"c\", Run: emptyRun}\n\n\tbuf := new(bytes.Buffer)\n\tassertNoErr(t, c.GenFishCompletion(buf, true))\n\toutput := buf.String()\n\n\t// check that active help is being disabled\n\tactiveHelpVar := activeHelpEnvVar(c.Name())\n\tcheck(t, output, fmt.Sprintf(\"%s=0\", activeHelpVar))\n}\n\nfunc TestGenFishCompletionFile(t *testing.T) {\n\ttmpFile, err := os.CreateTemp(\"\", \"cobra-test\")\n\tif err != nil {\n\t\tt.Fatal(err.Error())\n\t}\n\n\tdefer os.Remove(tmpFile.Name())\n\n\trootCmd := &Command{Use: \"root\", Args: NoArgs, Run: emptyRun}\n\tchild := &Command{\n\t\tUse:               \"child\",\n\t\tValidArgsFunction: validArgsFunc,\n\t\tRun:               emptyRun,\n\t}\n\trootCmd.AddCommand(child)\n\n\tassertNoErr(t, rootCmd.GenFishCompletionFile(tmpFile.Name(), false))\n}\n\nfunc TestFailGenFishCompletionFile(t *testing.T) {\n\ttmpDir, err := os.MkdirTemp(\"\", \"cobra-test\")\n\tif err != nil {\n\t\tt.Fatal(err.Error())\n\t}\n\n\tdefer os.RemoveAll(tmpDir)\n\n\tf, _ := os.OpenFile(filepath.Join(tmpDir, \"test\"), os.O_CREATE, 0400)\n\tdefer f.Close()\n\n\trootCmd := &Command{Use: \"root\", Args: NoArgs, Run: emptyRun}\n\tchild := &Command{\n\t\tUse:               \"child\",\n\t\tValidArgsFunction: validArgsFunc,\n\t\tRun:               emptyRun,\n\t}\n\trootCmd.AddCommand(child)\n\n\tgot := rootCmd.GenFishCompletionFile(f.Name(), false)\n\tif !errors.Is(got, os.ErrPermission) {\n\t\tt.Errorf(\"got: %s, want: %s\", got.Error(), os.ErrPermission.Error())\n\t}\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: cobra\nSource File Path(s): ['cobra\\\\powershell_completions_test.go\\\\source_files\\\\powershell_completions.go']\nTest File Path: cobra\\powershell_completions_test.go\\powershell_completions_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: cobra\\powershell_completions_test.go\\source_files\\powershell_completions.go\n// Copyright 2013-2023 The Cobra Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// The generated scripts require PowerShell v5.0+ (which comes Windows 10, but\n// can be downloaded separately for windows 7 or 8.1).\n\npackage cobra\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"io\"\n\t\"os\"\n\t\"strings\"\n)\n\nfunc genPowerShellComp(buf io.StringWriter, name string, includeDesc bool) {\n\t// Variables should not contain a '-' or ':' character\n\tnameForVar := name\n\tnameForVar = strings.ReplaceAll(nameForVar, \"-\", \"_\")\n\tnameForVar = strings.ReplaceAll(nameForVar, \":\", \"_\")\n\n\tcompCmd := ShellCompRequestCmd\n\tif !includeDesc {\n\t\tcompCmd = ShellCompNoDescRequestCmd\n\t}\n\tWriteStringAndCheck(buf, fmt.Sprintf(`# powershell completion for %-36[1]s -*- shell-script -*-\n\nfunction __%[1]s_debug {\n    if ($env:BASH_COMP_DEBUG_FILE) {\n        \"$args\" | Out-File -Append -FilePath \"$env:BASH_COMP_DEBUG_FILE\"\n    }\n}\n\nfilter __%[1]s_escapeStringWithSpecialChars {\n`+\"    $_ -replace '\\\\s|#|@|\\\\$|;|,|''|\\\\{|\\\\}|\\\\(|\\\\)|\\\"|`|\\\\||<|>|&','`$&'\"+`\n}\n\n[scriptblock]${__%[2]sCompleterBlock} = {\n    param(\n            $WordToComplete,\n            $CommandAst,\n            $CursorPosition\n        )\n\n    # Get the current command line and convert into a string\n    $Command = $CommandAst.CommandElements\n    $Command = \"$Command\"\n\n    __%[1]s_debug \"\"\n    __%[1]s_debug \"========= starting completion logic ==========\"\n    __%[1]s_debug \"WordToComplete: $WordToComplete Command: $Command CursorPosition: $CursorPosition\"\n\n    # The user could have moved the cursor backwards on the command-line.\n    # We need to trigger completion from the $CursorPosition location, so we need\n    # to truncate the command-line ($Command) up to the $CursorPosition location.\n    # Make sure the $Command is longer then the $CursorPosition before we truncate.\n    # This happens because the $Command does not include the last space.\n    if ($Command.Length -gt $CursorPosition) {\n        $Command=$Command.Substring(0,$CursorPosition)\n    }\n    __%[1]s_debug \"Truncated command: $Command\"\n\n    $ShellCompDirectiveError=%[4]d\n    $ShellCompDirectiveNoSpace=%[5]d\n    $ShellCompDirectiveNoFileComp=%[6]d\n    $ShellCompDirectiveFilterFileExt=%[7]d\n    $ShellCompDirectiveFilterDirs=%[8]d\n    $ShellCompDirectiveKeepOrder=%[9]d\n\n    # Prepare the command to request completions for the program.\n    # Split the command at the first space to separate the program and arguments.\n    $Program,$Arguments = $Command.Split(\" \",2)\n\n    $RequestComp=\"$Program %[3]s $Arguments\"\n    __%[1]s_debug \"RequestComp: $RequestComp\"\n\n    # we cannot use $WordToComplete because it\n    # has the wrong values if the cursor was moved\n    # so use the last argument\n    if ($WordToComplete -ne \"\" ) {\n        $WordToComplete = $Arguments.Split(\" \")[-1]\n    }\n    __%[1]s_debug \"New WordToComplete: $WordToComplete\"\n\n\n    # Check for flag with equal sign\n    $IsEqualFlag = ($WordToComplete -Like \"--*=*\" )\n    if ( $IsEqualFlag ) {\n        __%[1]s_debug \"Completing equal sign flag\"\n        # Remove the flag part\n        $Flag,$WordToComplete = $WordToComplete.Split(\"=\",2)\n    }\n\n    if ( $WordToComplete -eq \"\" -And ( -Not $IsEqualFlag )) {\n        # If the last parameter is complete (there is a space following it)\n        # We add an extra empty parameter so we can indicate this to the go method.\n        __%[1]s_debug \"Adding extra empty parameter\"\n        # PowerShell 7.2+ changed the way how the arguments are passed to executables,\n        # so for pre-7.2 or when Legacy argument passing is enabled we need to use\n`+\"        # `\\\"`\\\" to pass an empty argument, a \\\"\\\" or '' does not work!!!\"+`\n        if ($PSVersionTable.PsVersion -lt [version]'7.2.0' -or\n            ($PSVersionTable.PsVersion -lt [version]'7.3.0' -and -not [ExperimentalFeature]::IsEnabled(\"PSNativeCommandArgumentPassing\")) -or\n            (($PSVersionTable.PsVersion -ge [version]'7.3.0' -or [ExperimentalFeature]::IsEnabled(\"PSNativeCommandArgumentPassing\")) -and\n              $PSNativeCommandArgumentPassing -eq 'Legacy')) {\n`+\"             $RequestComp=\\\"$RequestComp\\\" + ' `\\\"`\\\"'\"+`\n        } else {\n             $RequestComp=\"$RequestComp\" + ' \"\"'\n        }\n    }\n\n    __%[1]s_debug \"Calling $RequestComp\"\n    # First disable ActiveHelp which is not supported for Powershell\n    ${env:%[10]s}=0\n\n    #call the command store the output in $out and redirect stderr and stdout to null\n    # $Out is an array contains each line per element\n    Invoke-Expression -OutVariable out \"$RequestComp\" 2>&1 | Out-Null\n\n    # get directive from last line\n    [int]$Directive = $Out[-1].TrimStart(':')\n    if ($Directive -eq \"\") {\n        # There is no directive specified\n        $Directive = 0\n    }\n    __%[1]s_debug \"The completion directive is: $Directive\"\n\n    # remove directive (last element) from out\n    $Out = $Out | Where-Object { $_ -ne $Out[-1] }\n    __%[1]s_debug \"The completions are: $Out\"\n\n    if (($Directive -band $ShellCompDirectiveError) -ne 0 ) {\n        # Error code.  No completion.\n        __%[1]s_debug \"Received error from custom completion go code\"\n        return\n    }\n\n    $Longest = 0\n    [Array]$Values = $Out | ForEach-Object {\n        #Split the output in name and description\n`+\"        $Name, $Description = $_.Split(\\\"`t\\\",2)\"+`\n        __%[1]s_debug \"Name: $Name Description: $Description\"\n\n        # Look for the longest completion so that we can format things nicely\n        if ($Longest -lt $Name.Length) {\n            $Longest = $Name.Length\n        }\n\n        # Set the description to a one space string if there is none set.\n        # This is needed because the CompletionResult does not accept an empty string as argument\n        if (-Not $Description) {\n            $Description = \" \"\n        }\n        New-Object -TypeName PSCustomObject -Property @{\n            Name = \"$Name\"\n            Description = \"$Description\"\n        }\n    }\n\n\n    $Space = \" \"\n    if (($Directive -band $ShellCompDirectiveNoSpace) -ne 0 ) {\n        # remove the space here\n        __%[1]s_debug \"ShellCompDirectiveNoSpace is called\"\n        $Space = \"\"\n    }\n\n    if ((($Directive -band $ShellCompDirectiveFilterFileExt) -ne 0 ) -or\n       (($Directive -band $ShellCompDirectiveFilterDirs) -ne 0 ))  {\n        __%[1]s_debug \"ShellCompDirectiveFilterFileExt ShellCompDirectiveFilterDirs are not supported\"\n\n        # return here to prevent the completion of the extensions\n        return\n    }\n\n    $Values = $Values | Where-Object {\n        # filter the result\n        $_.Name -like \"$WordToComplete*\"\n\n        # Join the flag back if we have an equal sign flag\n        if ( $IsEqualFlag ) {\n            __%[1]s_debug \"Join the equal sign flag back to the completion value\"\n            $_.Name = $Flag + \"=\" + $_.Name\n        }\n    }\n\n    # we sort the values in ascending order by name if keep order isn't passed\n    if (($Directive -band $ShellCompDirectiveKeepOrder) -eq 0 ) {\n        $Values = $Values | Sort-Object -Property Name\n    }\n\n    if (($Directive -band $ShellCompDirectiveNoFileComp) -ne 0 ) {\n        __%[1]s_debug \"ShellCompDirectiveNoFileComp is called\"\n\n        if ($Values.Length -eq 0) {\n            # Just print an empty string here so the\n            # shell does not start to complete paths.\n            # We cannot use CompletionResult here because\n            # it does not accept an empty string as argument.\n            \"\"\n            return\n        }\n    }\n\n    # Get the current mode\n    $Mode = (Get-PSReadLineKeyHandler | Where-Object {$_.Key -eq \"Tab\" }).Function\n    __%[1]s_debug \"Mode: $Mode\"\n\n    $Values | ForEach-Object {\n\n        # store temporary because switch will overwrite $_\n        $comp = $_\n\n        # PowerShell supports three different completion modes\n        # - TabCompleteNext (default windows style - on each key press the next option is displayed)\n        # - Complete (works like bash)\n        # - MenuComplete (works like zsh)\n        # You set the mode with Set-PSReadLineKeyHandler -Key Tab -Function <mode>\n\n        # CompletionResult Arguments:\n        # 1) CompletionText text to be used as the auto completion result\n        # 2) ListItemText   text to be displayed in the suggestion list\n        # 3) ResultType     type of completion result\n        # 4) ToolTip        text for the tooltip with details about the object\n\n        switch ($Mode) {\n\n            # bash like\n            \"Complete\" {\n\n                if ($Values.Length -eq 1) {\n                    __%[1]s_debug \"Only one completion left\"\n\n                    # insert space after value\n                    $CompletionText = $($comp.Name | __%[1]s_escapeStringWithSpecialChars) + $Space\n                    if ($ExecutionContext.SessionState.LanguageMode -eq \"FullLanguage\"){\n                        [System.Management.Automation.CompletionResult]::new($CompletionText, \"$($comp.Name)\", 'ParameterValue', \"$($comp.Description)\")\n                    } else {\n                        $CompletionText\n                    }\n\n                } else {\n                    # Add the proper number of spaces to align the descriptions\n                    while($comp.Name.Length -lt $Longest) {\n                        $comp.Name = $comp.Name + \" \"\n                    }\n\n                    # Check for empty description and only add parentheses if needed\n                    if ($($comp.Description) -eq \" \" ) {\n                        $Description = \"\"\n                    } else {\n                        $Description = \"  ($($comp.Description))\"\n                    }\n\n                    $CompletionText = \"$($comp.Name)$Description\"\n                    if ($ExecutionContext.SessionState.LanguageMode -eq \"FullLanguage\"){\n                        [System.Management.Automation.CompletionResult]::new($CompletionText, \"$($comp.Name)$Description\", 'ParameterValue', \"$($comp.Description)\")\n                    } else {\n                        $CompletionText\n                    }\n                }\n             }\n\n            # zsh like\n            \"MenuComplete\" {\n                # insert space after value\n                # MenuComplete will automatically show the ToolTip of\n                # the highlighted value at the bottom of the suggestions.\n\n                $CompletionText = $($comp.Name | __%[1]s_escapeStringWithSpecialChars) + $Space\n                if ($ExecutionContext.SessionState.LanguageMode -eq \"FullLanguage\"){\n                    [System.Management.Automation.CompletionResult]::new($CompletionText, \"$($comp.Name)\", 'ParameterValue', \"$($comp.Description)\")\n                } else {\n                    $CompletionText\n                }\n            }\n\n            # TabCompleteNext and in case we get something unknown\n            Default {\n                # Like MenuComplete but we don't want to add a space here because\n                # the user need to press space anyway to get the completion.\n                # Description will not be shown because that's not possible with TabCompleteNext\n\n                $CompletionText = $($comp.Name | __%[1]s_escapeStringWithSpecialChars)\n                if ($ExecutionContext.SessionState.LanguageMode -eq \"FullLanguage\"){\n                    [System.Management.Automation.CompletionResult]::new($CompletionText, \"$($comp.Name)\", 'ParameterValue', \"$($comp.Description)\")\n                } else {\n                    $CompletionText\n                }\n            }\n        }\n\n    }\n}\n\nRegister-ArgumentCompleter -CommandName '%[1]s' -ScriptBlock ${__%[2]sCompleterBlock}\n`, name, nameForVar, compCmd,\n\t\tShellCompDirectiveError, ShellCompDirectiveNoSpace, ShellCompDirectiveNoFileComp,\n\t\tShellCompDirectiveFilterFileExt, ShellCompDirectiveFilterDirs, ShellCompDirectiveKeepOrder, activeHelpEnvVar(name)))\n}\n\nfunc (c *Command) genPowerShellCompletion(w io.Writer, includeDesc bool) error {\n\tbuf := new(bytes.Buffer)\n\tgenPowerShellComp(buf, c.Name(), includeDesc)\n\t_, err := buf.WriteTo(w)\n\treturn err\n}\n\nfunc (c *Command) genPowerShellCompletionFile(filename string, includeDesc bool) error {\n\toutFile, err := os.Create(filename)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer outFile.Close()\n\n\treturn c.genPowerShellCompletion(outFile, includeDesc)\n}\n\n// GenPowerShellCompletionFile generates powershell completion file without descriptions.\nfunc (c *Command) GenPowerShellCompletionFile(filename string) error {\n\treturn c.genPowerShellCompletionFile(filename, false)\n}\n\n// GenPowerShellCompletion generates powershell completion file without descriptions\n// and writes it to the passed writer.\nfunc (c *Command) GenPowerShellCompletion(w io.Writer) error {\n\treturn c.genPowerShellCompletion(w, false)\n}\n\n// GenPowerShellCompletionFileWithDesc generates powershell completion file with descriptions.\nfunc (c *Command) GenPowerShellCompletionFileWithDesc(filename string) error {\n\treturn c.genPowerShellCompletionFile(filename, true)\n}\n\n// GenPowerShellCompletionWithDesc generates powershell completion file with descriptions\n// and writes it to the passed writer.\nfunc (c *Command) GenPowerShellCompletionWithDesc(w io.Writer) error {\n\treturn c.genPowerShellCompletion(w, true)\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:51:34"}, {"role": "assistant", "content": "```go\n// Copyright 2013-2023 The Cobra Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage cobra\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestPwshCompletionNoActiveHelp(t *testing.T) {\n\tc := &Command{Use: \"c\", Run: emptyRun}\n\n\tbuf := new(bytes.Buffer)\n\tassertNoErr(t, c.GenPowerShellCompletion(buf))\n\toutput := buf.String()\n\n\t// check that active help is being disabled\n\tactiveHelpVar := activeHelpEnvVar(c.Name())\n\tcheck(t, output, fmt.Sprintf(\"${env:%s}=0\", activeHelpVar))\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: cobra\nSource File Path(s): ['cobra\\\\zsh_completions_test.go\\\\source_files\\\\zsh_completions.go']\nTest File Path: cobra\\zsh_completions_test.go\\zsh_completions_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: cobra\\zsh_completions_test.go\\source_files\\zsh_completions.go\n// Copyright 2013-2023 The Cobra Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage cobra\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"io\"\n\t\"os\"\n)\n\n// GenZshCompletionFile generates zsh completion file including descriptions.\nfunc (c *Command) GenZshCompletionFile(filename string) error {\n\treturn c.genZshCompletionFile(filename, true)\n}\n\n// GenZshCompletion generates zsh completion file including descriptions\n// and writes it to the passed writer.\nfunc (c *Command) GenZshCompletion(w io.Writer) error {\n\treturn c.genZshCompletion(w, true)\n}\n\n// GenZshCompletionFileNoDesc generates zsh completion file without descriptions.\nfunc (c *Command) GenZshCompletionFileNoDesc(filename string) error {\n\treturn c.genZshCompletionFile(filename, false)\n}\n\n// GenZshCompletionNoDesc generates zsh completion file without descriptions\n// and writes it to the passed writer.\nfunc (c *Command) GenZshCompletionNoDesc(w io.Writer) error {\n\treturn c.genZshCompletion(w, false)\n}\n\n// MarkZshCompPositionalArgumentFile only worked for zsh and its behavior was\n// not consistent with Bash completion. It has therefore been disabled.\n// Instead, when no other completion is specified, file completion is done by\n// default for every argument. One can disable file completion on a per-argument\n// basis by using ValidArgsFunction and ShellCompDirectiveNoFileComp.\n// To achieve file extension filtering, one can use ValidArgsFunction and\n// ShellCompDirectiveFilterFileExt.\n//\n// Deprecated\nfunc (c *Command) MarkZshCompPositionalArgumentFile(argPosition int, patterns ...string) error {\n\treturn nil\n}\n\n// MarkZshCompPositionalArgumentWords only worked for zsh. It has therefore\n// been disabled.\n// To achieve the same behavior across all shells, one can use\n// ValidArgs (for the first argument only) or ValidArgsFunction for\n// any argument (can include the first one also).\n//\n// Deprecated\nfunc (c *Command) MarkZshCompPositionalArgumentWords(argPosition int, words ...string) error {\n\treturn nil\n}\n\nfunc (c *Command) genZshCompletionFile(filename string, includeDesc bool) error {\n\toutFile, err := os.Create(filename)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer outFile.Close()\n\n\treturn c.genZshCompletion(outFile, includeDesc)\n}\n\nfunc (c *Command) genZshCompletion(w io.Writer, includeDesc bool) error {\n\tbuf := new(bytes.Buffer)\n\tgenZshComp(buf, c.Name(), includeDesc)\n\t_, err := buf.WriteTo(w)\n\treturn err\n}\n\nfunc genZshComp(buf io.StringWriter, name string, includeDesc bool) {\n\tcompCmd := ShellCompRequestCmd\n\tif !includeDesc {\n\t\tcompCmd = ShellCompNoDescRequestCmd\n\t}\n\tWriteStringAndCheck(buf, fmt.Sprintf(`#compdef %[1]s\ncompdef _%[1]s %[1]s\n\n# zsh completion for %-36[1]s -*- shell-script -*-\n\n__%[1]s_debug()\n{\n    local file=\"$BASH_COMP_DEBUG_FILE\"\n    if [[ -n ${file} ]]; then\n        echo \"$*\" >> \"${file}\"\n    fi\n}\n\n_%[1]s()\n{\n    local shellCompDirectiveError=%[3]d\n    local shellCompDirectiveNoSpace=%[4]d\n    local shellCompDirectiveNoFileComp=%[5]d\n    local shellCompDirectiveFilterFileExt=%[6]d\n    local shellCompDirectiveFilterDirs=%[7]d\n    local shellCompDirectiveKeepOrder=%[8]d\n\n    local lastParam lastChar flagPrefix requestComp out directive comp lastComp noSpace keepOrder\n    local -a completions\n\n    __%[1]s_debug \"\\n========= starting completion logic ==========\"\n    __%[1]s_debug \"CURRENT: ${CURRENT}, words[*]: ${words[*]}\"\n\n    # The user could have moved the cursor backwards on the command-line.\n    # We need to trigger completion from the $CURRENT location, so we need\n    # to truncate the command-line ($words) up to the $CURRENT location.\n    # (We cannot use $CURSOR as its value does not work when a command is an alias.)\n    words=(\"${=words[1,CURRENT]}\")\n    __%[1]s_debug \"Truncated words[*]: ${words[*]},\"\n\n    lastParam=${words[-1]}\n    lastChar=${lastParam[-1]}\n    __%[1]s_debug \"lastParam: ${lastParam}, lastChar: ${lastChar}\"\n\n    # For zsh, when completing a flag with an = (e.g., %[1]s -n=<TAB>)\n    # completions must be prefixed with the flag\n    setopt local_options BASH_REMATCH\n    if [[ \"${lastParam}\" =~ '-.*=' ]]; then\n        # We are dealing with a flag with an =\n        flagPrefix=\"-P ${BASH_REMATCH}\"\n    fi\n\n    # Prepare the command to obtain completions\n    requestComp=\"${words[1]} %[2]s ${words[2,-1]}\"\n    if [ \"${lastChar}\" = \"\" ]; then\n        # If the last parameter is complete (there is a space following it)\n        # We add an extra empty parameter so we can indicate this to the go completion code.\n        __%[1]s_debug \"Adding extra empty parameter\"\n        requestComp=\"${requestComp} \\\"\\\"\"\n    fi\n\n    __%[1]s_debug \"About to call: eval ${requestComp}\"\n\n    # Use eval to handle any environment variables and such\n    out=$(eval ${requestComp} 2>/dev/null)\n    __%[1]s_debug \"completion output: ${out}\"\n\n    # Extract the directive integer following a : from the last line\n    local lastLine\n    while IFS='\\n' read -r line; do\n        lastLine=${line}\n    done < <(printf \"%%s\\n\" \"${out[@]}\")\n    __%[1]s_debug \"last line: ${lastLine}\"\n\n    if [ \"${lastLine[1]}\" = : ]; then\n        directive=${lastLine[2,-1]}\n        # Remove the directive including the : and the newline\n        local suffix\n        (( suffix=${#lastLine}+2))\n        out=${out[1,-$suffix]}\n    else\n        # There is no directive specified.  Leave $out as is.\n        __%[1]s_debug \"No directive found.  Setting do default\"\n        directive=0\n    fi\n\n    __%[1]s_debug \"directive: ${directive}\"\n    __%[1]s_debug \"completions: ${out}\"\n    __%[1]s_debug \"flagPrefix: ${flagPrefix}\"\n\n    if [ $((directive & shellCompDirectiveError)) -ne 0 ]; then\n        __%[1]s_debug \"Completion received error. Ignoring completions.\"\n        return\n    fi\n\n    local activeHelpMarker=\"%[9]s\"\n    local endIndex=${#activeHelpMarker}\n    local startIndex=$((${#activeHelpMarker}+1))\n    local hasActiveHelp=0\n    while IFS='\\n' read -r comp; do\n        # Check if this is an activeHelp statement (i.e., prefixed with $activeHelpMarker)\n        if [ \"${comp[1,$endIndex]}\" = \"$activeHelpMarker\" ];then\n            __%[1]s_debug \"ActiveHelp found: $comp\"\n            comp=\"${comp[$startIndex,-1]}\"\n            if [ -n \"$comp\" ]; then\n                compadd -x \"${comp}\"\n                __%[1]s_debug \"ActiveHelp will need delimiter\"\n                hasActiveHelp=1\n            fi\n\n            continue\n        fi\n\n        if [ -n \"$comp\" ]; then\n            # If requested, completions are returned with a description.\n            # The description is preceded by a TAB character.\n            # For zsh's _describe, we need to use a : instead of a TAB.\n            # We first need to escape any : as part of the completion itself.\n            comp=${comp//:/\\\\:}\n\n            local tab=\"$(printf '\\t')\"\n            comp=${comp//$tab/:}\n\n            __%[1]s_debug \"Adding completion: ${comp}\"\n            completions+=${comp}\n            lastComp=$comp\n        fi\n    done < <(printf \"%%s\\n\" \"${out[@]}\")\n\n    # Add a delimiter after the activeHelp statements, but only if:\n    # - there are completions following the activeHelp statements, or\n    # - file completion will be performed (so there will be choices after the activeHelp)\n    if [ $hasActiveHelp -eq 1 ]; then\n        if [ ${#completions} -ne 0 ] || [ $((directive & shellCompDirectiveNoFileComp)) -eq 0 ]; then\n            __%[1]s_debug \"Adding activeHelp delimiter\"\n            compadd -x \"--\"\n            hasActiveHelp=0\n        fi\n    fi\n\n    if [ $((directive & shellCompDirectiveNoSpace)) -ne 0 ]; then\n        __%[1]s_debug \"Activating nospace.\"\n        noSpace=\"-S ''\"\n    fi\n\n    if [ $((directive & shellCompDirectiveKeepOrder)) -ne 0 ]; then\n        __%[1]s_debug \"Activating keep order.\"\n        keepOrder=\"-V\"\n    fi\n\n    if [ $((directive & shellCompDirectiveFilterFileExt)) -ne 0 ]; then\n        # File extension filtering\n        local filteringCmd\n        filteringCmd='_files'\n        for filter in ${completions[@]}; do\n            if [ ${filter[1]} != '*' ]; then\n                # zsh requires a glob pattern to do file filtering\n                filter=\"\\*.$filter\"\n            fi\n            filteringCmd+=\" -g $filter\"\n        done\n        filteringCmd+=\" ${flagPrefix}\"\n\n        __%[1]s_debug \"File filtering command: $filteringCmd\"\n        _arguments '*:filename:'\"$filteringCmd\"\n    elif [ $((directive & shellCompDirectiveFilterDirs)) -ne 0 ]; then\n        # File completion for directories only\n        local subdir\n        subdir=\"${completions[1]}\"\n        if [ -n \"$subdir\" ]; then\n            __%[1]s_debug \"Listing directories in $subdir\"\n            pushd \"${subdir}\" >/dev/null 2>&1\n        else\n            __%[1]s_debug \"Listing directories in .\"\n        fi\n\n        local result\n        _arguments '*:dirname:_files -/'\" ${flagPrefix}\"\n        result=$?\n        if [ -n \"$subdir\" ]; then\n            popd >/dev/null 2>&1\n        fi\n        return $result\n    else\n        __%[1]s_debug \"Calling _describe\"\n        if eval _describe $keepOrder \"completions\" completions $flagPrefix $noSpace; then\n            __%[1]s_debug \"_describe found some completions\"\n\n            # Return the success of having called _describe\n            return 0\n        else\n            __%[1]s_debug \"_describe did not find completions.\"\n            __%[1]s_debug \"Checking if we should do file completion.\"\n            if [ $((directive & shellCompDirectiveNoFileComp)) -ne 0 ]; then\n                __%[1]s_debug \"deactivating file completion\"\n\n                # We must return an error code here to let zsh know that there were no\n                # completions found by _describe; this is what will trigger other\n                # matching algorithms to attempt to find completions.\n                # For example zsh can match letters in the middle of words.\n                return 1\n            else\n                # Perform file completion\n                __%[1]s_debug \"Activating file completion\"\n\n                # We must return the result of this command, so it must be the\n                # last command, or else we must store its result to return it.\n                _arguments '*:filename:_files'\" ${flagPrefix}\"\n            fi\n        fi\n    fi\n}\n\n# don't run the completion function when being source-ed or eval-ed\nif [ \"$funcstack[1]\" = \"_%[1]s\" ]; then\n    _%[1]s\nfi\n`, name, compCmd,\n\t\tShellCompDirectiveError, ShellCompDirectiveNoSpace, ShellCompDirectiveNoFileComp,\n\t\tShellCompDirectiveFilterFileExt, ShellCompDirectiveFilterDirs, ShellCompDirectiveKeepOrder,\n\t\tactiveHelpMarker))\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:51:34"}, {"role": "assistant", "content": "```go\n// Copyright 2013-2023 The Cobra Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage cobra\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestZshCompletionWithActiveHelp(t *testing.T) {\n\tc := &Command{Use: \"c\", Run: emptyRun}\n\n\tbuf := new(bytes.Buffer)\n\tassertNoErr(t, c.GenZshCompletion(buf))\n\toutput := buf.String()\n\n\t// check that active help is not being disabled\n\tactiveHelpVar := activeHelpEnvVar(c.Name())\n\tcheckOmit(t, output, fmt.Sprintf(\"%s=0\", activeHelpVar))\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: moby\nSource File Path(s): ['moby\\\\authconfig_test.go\\\\source_files\\\\authconfig.go']\nTest File Path: moby\\authconfig_test.go\\authconfig_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: moby\\authconfig_test.go\\source_files\\authconfig.go\npackage registry // import \"github.com/docker/docker/api/types/registry\"\nimport (\n\t\"context\"\n\t\"encoding/base64\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"strings\"\n)\n\n// AuthHeader is the name of the header used to send encoded registry\n// authorization credentials for registry operations (push/pull).\nconst AuthHeader = \"X-Registry-Auth\"\n\n// RequestAuthConfig is a function interface that clients can supply\n// to retry operations after getting an authorization error.\n//\n// The function must return the [AuthHeader] value ([AuthConfig]), encoded\n// in base64url format ([RFC4648, section 5]), which can be decoded by\n// [DecodeAuthConfig].\n//\n// It must return an error if the privilege request fails.\n//\n// [RFC4648, section 5]: https://tools.ietf.org/html/rfc4648#section-5\ntype RequestAuthConfig func(context.Context) (string, error)\n\n// AuthConfig contains authorization information for connecting to a Registry.\ntype AuthConfig struct {\n\tUsername string `json:\"username,omitempty\"`\n\tPassword string `json:\"password,omitempty\"`\n\tAuth     string `json:\"auth,omitempty\"`\n\n\t// Email is an optional value associated with the username.\n\t// This field is deprecated and will be removed in a later\n\t// version of docker.\n\tEmail string `json:\"email,omitempty\"`\n\n\tServerAddress string `json:\"serveraddress,omitempty\"`\n\n\t// IdentityToken is used to authenticate the user and get\n\t// an access token for the registry.\n\tIdentityToken string `json:\"identitytoken,omitempty\"`\n\n\t// RegistryToken is a bearer token to be sent to a registry\n\tRegistryToken string `json:\"registrytoken,omitempty\"`\n}\n\n// EncodeAuthConfig serializes the auth configuration as a base64url encoded\n// ([RFC4648, section 5]) JSON string for sending through the X-Registry-Auth header.\n//\n// [RFC4648, section 5]: https://tools.ietf.org/html/rfc4648#section-5\nfunc EncodeAuthConfig(authConfig AuthConfig) (string, error) {\n\tbuf, err := json.Marshal(authConfig)\n\tif err != nil {\n\t\treturn \"\", errInvalidParameter{err}\n\t}\n\treturn base64.URLEncoding.EncodeToString(buf), nil\n}\n\n// DecodeAuthConfig decodes base64url encoded ([RFC4648, section 5]) JSON\n// authentication information as sent through the X-Registry-Auth header.\n//\n// This function always returns an [AuthConfig], even if an error occurs. It is up\n// to the caller to decide if authentication is required, and if the error can\n// be ignored.\n//\n// [RFC4648, section 5]: https://tools.ietf.org/html/rfc4648#section-5\nfunc DecodeAuthConfig(authEncoded string) (*AuthConfig, error) {\n\tif authEncoded == \"\" {\n\t\treturn &AuthConfig{}, nil\n\t}\n\n\tauthJSON := base64.NewDecoder(base64.URLEncoding, strings.NewReader(authEncoded))\n\treturn decodeAuthConfigFromReader(authJSON)\n}\n\n// DecodeAuthConfigBody decodes authentication information as sent as JSON in the\n// body of a request. This function is to provide backward compatibility with old\n// clients and API versions. Current clients and API versions expect authentication\n// to be provided through the X-Registry-Auth header.\n//\n// Like [DecodeAuthConfig], this function always returns an [AuthConfig], even if an\n// error occurs. It is up to the caller to decide if authentication is required,\n// and if the error can be ignored.\nfunc DecodeAuthConfigBody(rdr io.ReadCloser) (*AuthConfig, error) {\n\treturn decodeAuthConfigFromReader(rdr)\n}\n\nfunc decodeAuthConfigFromReader(rdr io.Reader) (*AuthConfig, error) {\n\tauthConfig := &AuthConfig{}\n\tif err := json.NewDecoder(rdr).Decode(authConfig); err != nil {\n\t\t// always return an (empty) AuthConfig to increase compatibility with\n\t\t// the existing API.\n\t\treturn &AuthConfig{}, invalid(err)\n\t}\n\treturn authConfig, nil\n}\n\nfunc invalid(err error) error {\n\treturn errInvalidParameter{fmt.Errorf(\"invalid X-Registry-Auth header: %w\", err)}\n}\n\ntype errInvalidParameter struct{ error }\n\nfunc (errInvalidParameter) InvalidParameter() {}\n\nfunc (e errInvalidParameter) Cause() error { return e.error }\n\nfunc (e errInvalidParameter) Unwrap() error { return e.error }\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:51:53"}, {"role": "assistant", "content": "```go\npackage registry // import \"github.com/docker/docker/api/types/registry\"\nimport (\n\t\"io\"\n\t\"strings\"\n\t\"testing\"\n\n\t\"gotest.tools/v3/assert\"\n)\n\nconst (\n\tunencoded        = `{\"username\":\"testuser\",\"password\":\"testpassword\",\"serveraddress\":\"example.com\"}`\n\tencoded          = `eyJ1c2VybmFtZSI6InRlc3R1c2VyIiwicGFzc3dvcmQiOiJ0ZXN0cGFzc3dvcmQiLCJzZXJ2ZXJhZGRyZXNzIjoiZXhhbXBsZS5jb20ifQ==`\n\tencodedNoPadding = `eyJ1c2VybmFtZSI6InRlc3R1c2VyIiwicGFzc3dvcmQiOiJ0ZXN0cGFzc3dvcmQiLCJzZXJ2ZXJhZGRyZXNzIjoiZXhhbXBsZS5jb20ifQ`\n)\n\nvar expected = AuthConfig{\n\tUsername:      \"testuser\",\n\tPassword:      \"testpassword\",\n\tServerAddress: \"example.com\",\n}\n\nfunc TestDecodeAuthConfig(t *testing.T) {\n\tt.Run(\"valid\", func(t *testing.T) {\n\t\ttoken, err := DecodeAuthConfig(encoded)\n\t\tassert.NilError(t, err)\n\t\tassert.Equal(t, *token, expected)\n\t})\n\n\tt.Run(\"empty\", func(t *testing.T) {\n\t\ttoken, err := DecodeAuthConfig(\"\")\n\t\tassert.NilError(t, err)\n\t\tassert.Equal(t, *token, AuthConfig{})\n\t})\n\n\t// We currently only support base64url encoding with padding, so\n\t// un-padded should produce an error.\n\t//\n\t// RFC4648, section 5: https://tools.ietf.org/html/rfc4648#section-5\n\t// RFC4648, section 3.2: https://tools.ietf.org/html/rfc4648#section-3.2\n\tt.Run(\"invalid encoding\", func(t *testing.T) {\n\t\ttoken, err := DecodeAuthConfig(encodedNoPadding)\n\n\t\tassert.ErrorType(t, err, errInvalidParameter{})\n\t\tassert.ErrorContains(t, err, \"invalid X-Registry-Auth header: unexpected EOF\")\n\t\tassert.Equal(t, *token, AuthConfig{})\n\t})\n}\n\nfunc TestDecodeAuthConfigBody(t *testing.T) {\n\ttoken, err := DecodeAuthConfigBody(io.NopCloser(strings.NewReader(unencoded)))\n\tassert.NilError(t, err)\n\tassert.Equal(t, *token, expected)\n}\n\nfunc TestEncodeAuthConfig(t *testing.T) {\n\ttoken, err := EncodeAuthConfig(expected)\n\tassert.NilError(t, err)\n\tassert.Equal(t, token, encoded)\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: moby\nSource File Path(s): ['moby\\\\checkpoint_create_test.go\\\\source_files\\\\checkpoint_create.go']\nTest File Path: moby\\checkpoint_create_test.go\\checkpoint_create_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: moby\\checkpoint_create_test.go\\source_files\\checkpoint_create.go\npackage client // import \"github.com/docker/docker/client\"\n\nimport (\n\t\"context\"\n\n\t\"github.com/docker/docker/api/types/checkpoint\"\n)\n\n// CheckpointCreate creates a checkpoint from the given container with the given name\nfunc (cli *Client) CheckpointCreate(ctx context.Context, containerID string, options checkpoint.CreateOptions) error {\n\tcontainerID, err := trimID(\"container\", containerID)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tresp, err := cli.post(ctx, \"/containers/\"+containerID+\"/checkpoints\", nil, options, nil)\n\tensureReaderClosed(resp)\n\treturn err\n}\n\n### Source File Dependency Files Content\n### Dependency File: moby\\checkpoint_create_test.go\\dependent_files\\list.go\npackage checkpoint\n\n// Summary represents the details of a checkpoint when listing endpoints.\ntype Summary struct {\n\t// Name is the name of the checkpoint.\n\tName string\n}\n\n\n### Dependency File: moby\\checkpoint_create_test.go\\dependent_files\\options.go\npackage checkpoint\n\n// CreateOptions holds parameters to create a checkpoint from a container.\ntype CreateOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n\tExit          bool\n}\n\n// ListOptions holds parameters to list checkpoints for a container.\ntype ListOptions struct {\n\tCheckpointDir string\n}\n\n// DeleteOptions holds parameters to delete a checkpoint from a container.\ntype DeleteOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:51:53"}, {"role": "assistant", "content": "```go\npackage client // import \"github.com/docker/docker/client\"\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"net/http\"\n\t\"strings\"\n\t\"testing\"\n\n\t\"github.com/docker/docker/api/types/checkpoint\"\n\t\"github.com/docker/docker/errdefs\"\n\t\"gotest.tools/v3/assert\"\n\tis \"gotest.tools/v3/assert/cmp\"\n)\n\nfunc TestCheckpointCreateError(t *testing.T) {\n\tclient := &Client{\n\t\tclient: newMockClient(errorMock(http.StatusInternalServerError, \"Server error\")),\n\t}\n\terr := client.CheckpointCreate(context.Background(), \"nothing\", checkpoint.CreateOptions{\n\t\tCheckpointID: \"noting\",\n\t\tExit:         true,\n\t})\n\n\tassert.Check(t, is.ErrorType(err, errdefs.IsSystem))\n\n\terr = client.CheckpointCreate(context.Background(), \"\", checkpoint.CreateOptions{})\n\tassert.Check(t, is.ErrorType(err, errdefs.IsInvalidParameter))\n\tassert.Check(t, is.ErrorContains(err, \"value is empty\"))\n\n\terr = client.CheckpointCreate(context.Background(), \"    \", checkpoint.CreateOptions{})\n\tassert.Check(t, is.ErrorType(err, errdefs.IsInvalidParameter))\n\tassert.Check(t, is.ErrorContains(err, \"value is empty\"))\n}\n\nfunc TestCheckpointCreate(t *testing.T) {\n\texpectedContainerID := \"container_id\"\n\texpectedCheckpointID := \"checkpoint_id\"\n\texpectedURL := \"/containers/container_id/checkpoints\"\n\n\tclient := &Client{\n\t\tclient: newMockClient(func(req *http.Request) (*http.Response, error) {\n\t\t\tif !strings.HasPrefix(req.URL.Path, expectedURL) {\n\t\t\t\treturn nil, fmt.Errorf(\"expected URL '%s', got '%s'\", expectedURL, req.URL)\n\t\t\t}\n\n\t\t\tif req.Method != http.MethodPost {\n\t\t\t\treturn nil, fmt.Errorf(\"expected POST method, got %s\", req.Method)\n\t\t\t}\n\n\t\t\tcreateOptions := &checkpoint.CreateOptions{}\n\t\t\tif err := json.NewDecoder(req.Body).Decode(createOptions); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\tif createOptions.CheckpointID != expectedCheckpointID {\n\t\t\t\treturn nil, fmt.Errorf(\"expected CheckpointID to be 'checkpoint_id', got %v\", createOptions.CheckpointID)\n\t\t\t}\n\n\t\t\tif !createOptions.Exit {\n\t\t\t\treturn nil, fmt.Errorf(\"expected Exit to be true\")\n\t\t\t}\n\n\t\t\treturn &http.Response{\n\t\t\t\tStatusCode: http.StatusOK,\n\t\t\t\tBody:       io.NopCloser(bytes.NewReader([]byte(\"\"))),\n\t\t\t}, nil\n\t\t}),\n\t}\n\n\terr := client.CheckpointCreate(context.Background(), expectedContainerID, checkpoint.CreateOptions{\n\t\tCheckpointID: expectedCheckpointID,\n\t\tExit:         true,\n\t})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: moby\nSource File Path(s): ['moby\\\\checkpoint_delete_test.go\\\\source_files\\\\checkpoint_delete.go']\nTest File Path: moby\\checkpoint_delete_test.go\\checkpoint_delete_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: moby\\checkpoint_delete_test.go\\source_files\\checkpoint_delete.go\npackage client // import \"github.com/docker/docker/client\"\n\nimport (\n\t\"context\"\n\t\"net/url\"\n\n\t\"github.com/docker/docker/api/types/checkpoint\"\n)\n\n// CheckpointDelete deletes the checkpoint with the given name from the given container\nfunc (cli *Client) CheckpointDelete(ctx context.Context, containerID string, options checkpoint.DeleteOptions) error {\n\tcontainerID, err := trimID(\"container\", containerID)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tquery := url.Values{}\n\tif options.CheckpointDir != \"\" {\n\t\tquery.Set(\"dir\", options.CheckpointDir)\n\t}\n\n\tresp, err := cli.delete(ctx, \"/containers/\"+containerID+\"/checkpoints/\"+options.CheckpointID, query, nil)\n\tensureReaderClosed(resp)\n\treturn err\n}\n\n### Source File Dependency Files Content\n### Dependency File: moby\\checkpoint_delete_test.go\\dependent_files\\list.go\npackage checkpoint\n\n// Summary represents the details of a checkpoint when listing endpoints.\ntype Summary struct {\n\t// Name is the name of the checkpoint.\n\tName string\n}\n\n\n### Dependency File: moby\\checkpoint_delete_test.go\\dependent_files\\options.go\npackage checkpoint\n\n// CreateOptions holds parameters to create a checkpoint from a container.\ntype CreateOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n\tExit          bool\n}\n\n// ListOptions holds parameters to list checkpoints for a container.\ntype ListOptions struct {\n\tCheckpointDir string\n}\n\n// DeleteOptions holds parameters to delete a checkpoint from a container.\ntype DeleteOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:51:53"}, {"role": "assistant", "content": "```go\npackage client // import \"github.com/docker/docker/client\"\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"net/http\"\n\t\"strings\"\n\t\"testing\"\n\n\t\"github.com/docker/docker/api/types/checkpoint\"\n\t\"github.com/docker/docker/errdefs\"\n\t\"gotest.tools/v3/assert\"\n\tis \"gotest.tools/v3/assert/cmp\"\n)\n\nfunc TestCheckpointDeleteError(t *testing.T) {\n\tclient := &Client{\n\t\tclient: newMockClient(errorMock(http.StatusInternalServerError, \"Server error\")),\n\t}\n\n\terr := client.CheckpointDelete(context.Background(), \"container_id\", checkpoint.DeleteOptions{\n\t\tCheckpointID: \"checkpoint_id\",\n\t})\n\n\tassert.Check(t, is.ErrorType(err, errdefs.IsSystem))\n\n\terr = client.CheckpointDelete(context.Background(), \"\", checkpoint.DeleteOptions{})\n\tassert.Check(t, is.ErrorType(err, errdefs.IsInvalidParameter))\n\tassert.Check(t, is.ErrorContains(err, \"value is empty\"))\n\n\terr = client.CheckpointDelete(context.Background(), \"    \", checkpoint.DeleteOptions{})\n\tassert.Check(t, is.ErrorType(err, errdefs.IsInvalidParameter))\n\tassert.Check(t, is.ErrorContains(err, \"value is empty\"))\n}\n\nfunc TestCheckpointDelete(t *testing.T) {\n\texpectedURL := \"/containers/container_id/checkpoints/checkpoint_id\"\n\n\tclient := &Client{\n\t\tclient: newMockClient(func(req *http.Request) (*http.Response, error) {\n\t\t\tif !strings.HasPrefix(req.URL.Path, expectedURL) {\n\t\t\t\treturn nil, fmt.Errorf(\"Expected URL '%s', got '%s'\", expectedURL, req.URL)\n\t\t\t}\n\t\t\tif req.Method != http.MethodDelete {\n\t\t\t\treturn nil, fmt.Errorf(\"expected DELETE method, got %s\", req.Method)\n\t\t\t}\n\t\t\treturn &http.Response{\n\t\t\t\tStatusCode: http.StatusOK,\n\t\t\t\tBody:       io.NopCloser(bytes.NewReader([]byte(\"\"))),\n\t\t\t}, nil\n\t\t}),\n\t}\n\n\terr := client.CheckpointDelete(context.Background(), \"container_id\", checkpoint.DeleteOptions{\n\t\tCheckpointID: \"checkpoint_id\",\n\t})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: moby\nSource File Path(s): ['moby\\\\checkpoint_list_test.go\\\\source_files\\\\checkpoint_list.go']\nTest File Path: moby\\checkpoint_list_test.go\\checkpoint_list_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: moby\\checkpoint_list_test.go\\source_files\\checkpoint_list.go\npackage client // import \"github.com/docker/docker/client\"\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"net/url\"\n\n\t\"github.com/docker/docker/api/types/checkpoint\"\n)\n\n// CheckpointList returns the checkpoints of the given container in the docker host\nfunc (cli *Client) CheckpointList(ctx context.Context, container string, options checkpoint.ListOptions) ([]checkpoint.Summary, error) {\n\tvar checkpoints []checkpoint.Summary\n\n\tquery := url.Values{}\n\tif options.CheckpointDir != \"\" {\n\t\tquery.Set(\"dir\", options.CheckpointDir)\n\t}\n\n\tresp, err := cli.get(ctx, \"/containers/\"+container+\"/checkpoints\", query, nil)\n\tdefer ensureReaderClosed(resp)\n\tif err != nil {\n\t\treturn checkpoints, err\n\t}\n\n\terr = json.NewDecoder(resp.body).Decode(&checkpoints)\n\treturn checkpoints, err\n}\n\n### Source File Dependency Files Content\n### Dependency File: moby\\checkpoint_list_test.go\\dependent_files\\list.go\npackage checkpoint\n\n// Summary represents the details of a checkpoint when listing endpoints.\ntype Summary struct {\n\t// Name is the name of the checkpoint.\n\tName string\n}\n\n\n### Dependency File: moby\\checkpoint_list_test.go\\dependent_files\\options.go\npackage checkpoint\n\n// CreateOptions holds parameters to create a checkpoint from a container.\ntype CreateOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n\tExit          bool\n}\n\n// ListOptions holds parameters to list checkpoints for a container.\ntype ListOptions struct {\n\tCheckpointDir string\n}\n\n// DeleteOptions holds parameters to delete a checkpoint from a container.\ntype DeleteOptions struct {\n\tCheckpointID  string\n\tCheckpointDir string\n}\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:51:53"}, {"role": "assistant", "content": "```go\npackage client // import \"github.com/docker/docker/client\"\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"net/http\"\n\t\"strings\"\n\t\"testing\"\n\n\t\"github.com/docker/docker/api/types/checkpoint\"\n\t\"github.com/docker/docker/errdefs\"\n\t\"gotest.tools/v3/assert\"\n\tis \"gotest.tools/v3/assert/cmp\"\n)\n\nfunc TestCheckpointListError(t *testing.T) {\n\tclient := &Client{\n\t\tclient: newMockClient(errorMock(http.StatusInternalServerError, \"Server error\")),\n\t}\n\n\t_, err := client.CheckpointList(context.Background(), \"container_id\", checkpoint.ListOptions{})\n\tassert.Check(t, is.ErrorType(err, errdefs.IsSystem))\n}\n\nfunc TestCheckpointList(t *testing.T) {\n\texpectedURL := \"/containers/container_id/checkpoints\"\n\n\tclient := &Client{\n\t\tclient: newMockClient(func(req *http.Request) (*http.Response, error) {\n\t\t\tif !strings.HasPrefix(req.URL.Path, expectedURL) {\n\t\t\t\treturn nil, fmt.Errorf(\"Expected URL '%s', got '%s'\", expectedURL, req.URL)\n\t\t\t}\n\t\t\tcontent, err := json.Marshal([]checkpoint.Summary{\n\t\t\t\t{\n\t\t\t\t\tName: \"checkpoint\",\n\t\t\t\t},\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\treturn &http.Response{\n\t\t\t\tStatusCode: http.StatusOK,\n\t\t\t\tBody:       io.NopCloser(bytes.NewReader(content)),\n\t\t\t}, nil\n\t\t}),\n\t}\n\n\tcheckpoints, err := client.CheckpointList(context.Background(), \"container_id\", checkpoint.ListOptions{})\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif len(checkpoints) != 1 {\n\t\tt.Fatalf(\"expected 1 checkpoint, got %v\", checkpoints)\n\t}\n}\n\nfunc TestCheckpointListContainerNotFound(t *testing.T) {\n\tclient := &Client{\n\t\tclient: newMockClient(errorMock(http.StatusNotFound, \"Server error\")),\n\t}\n\n\t_, err := client.CheckpointList(context.Background(), \"unknown\", checkpoint.ListOptions{})\n\tassert.Check(t, is.ErrorType(err, errdefs.IsNotFound))\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: moby\nSource File Path(s): ['moby\\\\endpoint_test.go\\\\source_files\\\\endpoint.go']\nTest File Path: moby\\endpoint_test.go\\endpoint_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: moby\\endpoint_test.go\\source_files\\endpoint.go\npackage network\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"net\"\n\n\t\"github.com/docker/docker/internal/multierror\"\n)\n\n// EndpointSettings stores the network endpoint details\ntype EndpointSettings struct {\n\t// Configurations\n\tIPAMConfig *EndpointIPAMConfig\n\tLinks      []string\n\tAliases    []string // Aliases holds the list of extra, user-specified DNS names for this endpoint.\n\t// MacAddress may be used to specify a MAC address when the container is created.\n\t// Once the container is running, it becomes operational data (it may contain a\n\t// generated address).\n\tMacAddress string\n\tDriverOpts map[string]string\n\n\t// GwPriority determines which endpoint will provide the default gateway\n\t// for the container. The endpoint with the highest priority will be used.\n\t// If multiple endpoints have the same priority, they are lexicographically\n\t// sorted based on their network name, and the one that sorts first is picked.\n\tGwPriority int\n\t// Operational data\n\tNetworkID           string\n\tEndpointID          string\n\tGateway             string\n\tIPAddress           string\n\tIPPrefixLen         int\n\tIPv6Gateway         string\n\tGlobalIPv6Address   string\n\tGlobalIPv6PrefixLen int\n\t// DNSNames holds all the (non fully qualified) DNS names associated to this endpoint. First entry is used to\n\t// generate PTR records.\n\tDNSNames []string\n}\n\n// Copy makes a deep copy of `EndpointSettings`\nfunc (es *EndpointSettings) Copy() *EndpointSettings {\n\tepCopy := *es\n\tif es.IPAMConfig != nil {\n\t\tepCopy.IPAMConfig = es.IPAMConfig.Copy()\n\t}\n\n\tif es.Links != nil {\n\t\tlinks := make([]string, 0, len(es.Links))\n\t\tepCopy.Links = append(links, es.Links...)\n\t}\n\n\tif es.Aliases != nil {\n\t\taliases := make([]string, 0, len(es.Aliases))\n\t\tepCopy.Aliases = append(aliases, es.Aliases...)\n\t}\n\n\tif len(es.DNSNames) > 0 {\n\t\tepCopy.DNSNames = make([]string, len(es.DNSNames))\n\t\tcopy(epCopy.DNSNames, es.DNSNames)\n\t}\n\n\treturn &epCopy\n}\n\n// EndpointIPAMConfig represents IPAM configurations for the endpoint\ntype EndpointIPAMConfig struct {\n\tIPv4Address  string   `json:\",omitempty\"`\n\tIPv6Address  string   `json:\",omitempty\"`\n\tLinkLocalIPs []string `json:\",omitempty\"`\n}\n\n// Copy makes a copy of the endpoint ipam config\nfunc (cfg *EndpointIPAMConfig) Copy() *EndpointIPAMConfig {\n\tcfgCopy := *cfg\n\tcfgCopy.LinkLocalIPs = make([]string, 0, len(cfg.LinkLocalIPs))\n\tcfgCopy.LinkLocalIPs = append(cfgCopy.LinkLocalIPs, cfg.LinkLocalIPs...)\n\treturn &cfgCopy\n}\n\n// NetworkSubnet describes a user-defined subnet for a specific network. It's only used to validate if an\n// EndpointIPAMConfig is valid for a specific network.\ntype NetworkSubnet interface {\n\t// Contains checks whether the NetworkSubnet contains [addr].\n\tContains(addr net.IP) bool\n\t// IsStatic checks whether the subnet was statically allocated (ie. user-defined).\n\tIsStatic() bool\n}\n\n// IsInRange checks whether static IP addresses are valid in a specific network.\nfunc (cfg *EndpointIPAMConfig) IsInRange(v4Subnets []NetworkSubnet, v6Subnets []NetworkSubnet) error {\n\tvar errs []error\n\n\tif err := validateEndpointIPAddress(cfg.IPv4Address, v4Subnets); err != nil {\n\t\terrs = append(errs, err)\n\t}\n\tif err := validateEndpointIPAddress(cfg.IPv6Address, v6Subnets); err != nil {\n\t\terrs = append(errs, err)\n\t}\n\n\treturn multierror.Join(errs...)\n}\n\nfunc validateEndpointIPAddress(epAddr string, ipamSubnets []NetworkSubnet) error {\n\tif epAddr == \"\" {\n\t\treturn nil\n\t}\n\n\tvar staticSubnet bool\n\tparsedAddr := net.ParseIP(epAddr)\n\tfor _, subnet := range ipamSubnets {\n\t\tif subnet.IsStatic() {\n\t\t\tstaticSubnet = true\n\t\t\tif subnet.Contains(parsedAddr) {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\t}\n\n\tif staticSubnet {\n\t\treturn fmt.Errorf(\"no configured subnet or ip-range contain the IP address %s\", epAddr)\n\t}\n\n\treturn errors.New(\"user specified IP address is supported only when connecting to networks with user configured subnets\")\n}\n\n// Validate checks whether cfg is valid.\nfunc (cfg *EndpointIPAMConfig) Validate() error {\n\tif cfg == nil {\n\t\treturn nil\n\t}\n\n\tvar errs []error\n\n\tif cfg.IPv4Address != \"\" {\n\t\tif addr := net.ParseIP(cfg.IPv4Address); addr == nil || addr.To4() == nil || addr.IsUnspecified() {\n\t\t\terrs = append(errs, fmt.Errorf(\"invalid IPv4 address: %s\", cfg.IPv4Address))\n\t\t}\n\t}\n\tif cfg.IPv6Address != \"\" {\n\t\tif addr := net.ParseIP(cfg.IPv6Address); addr == nil || addr.To4() != nil || addr.IsUnspecified() {\n\t\t\terrs = append(errs, fmt.Errorf(\"invalid IPv6 address: %s\", cfg.IPv6Address))\n\t\t}\n\t}\n\tfor _, addr := range cfg.LinkLocalIPs {\n\t\tif parsed := net.ParseIP(addr); parsed == nil || parsed.IsUnspecified() {\n\t\t\terrs = append(errs, fmt.Errorf(\"invalid link-local IP address: %s\", addr))\n\t\t}\n\t}\n\n\treturn multierror.Join(errs...)\n}\n\n### Source File Dependency Files Content\n### Dependency File: moby\\endpoint_test.go\\dependent_files\\multierror.go\npackage multierror\n\nimport (\n\t\"strings\"\n)\n\n// Join is a drop-in replacement for errors.Join with better formatting.\nfunc Join(errs ...error) error {\n\tn := 0\n\tfor _, err := range errs {\n\t\tif err != nil {\n\t\t\tn++\n\t\t}\n\t}\n\tif n == 0 {\n\t\treturn nil\n\t}\n\te := &joinError{\n\t\terrs: make([]error, 0, n),\n\t}\n\tfor _, err := range errs {\n\t\tif err != nil {\n\t\t\te.errs = append(e.errs, err)\n\t\t}\n\t}\n\treturn e\n}\n\ntype joinError struct {\n\terrs []error\n}\n\nfunc (e *joinError) Error() string {\n\tif len(e.errs) == 1 {\n\t\treturn strings.TrimSpace(e.errs[0].Error())\n\t}\n\tstringErrs := make([]string, 0, len(e.errs))\n\tfor _, subErr := range e.errs {\n\t\tstringErrs = append(stringErrs, strings.Replace(subErr.Error(), \"\\n\", \"\\n\\t\", -1))\n\t}\n\treturn \"* \" + strings.Join(stringErrs, \"\\n* \")\n}\n\nfunc (e *joinError) Unwrap() []error {\n\treturn e.errs\n}\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:51:53"}, {"role": "assistant", "content": "```go\npackage network\n\nimport (\n\t\"net\"\n\t\"testing\"\n\n\t\"gotest.tools/v3/assert\"\n\tis \"gotest.tools/v3/assert/cmp\"\n)\n\ntype subnetStub struct {\n\tstatic   bool\n\tcontains map[string]bool\n}\n\nfunc (stub subnetStub) IsStatic() bool {\n\treturn stub.static\n}\n\nfunc (stub subnetStub) Contains(addr net.IP) bool {\n\tv, ok := stub.contains[addr.String()]\n\treturn ok && v\n}\n\nfunc TestEndpointIPAMConfigWithOutOfRangeAddrs(t *testing.T) {\n\ttests := []struct {\n\t\tname           string\n\t\tipamConfig     *EndpointIPAMConfig\n\t\tv4Subnets      []NetworkSubnet\n\t\tv6Subnets      []NetworkSubnet\n\t\texpectedErrors []string\n\t}{\n\t\t{\n\t\t\tname: \"valid config\",\n\t\t\tipamConfig: &EndpointIPAMConfig{\n\t\t\t\tIPv4Address:  \"192.168.100.10\",\n\t\t\t\tIPv6Address:  \"2a01:d2:af:420b:25c1:1816:bb33:855c\",\n\t\t\t\tLinkLocalIPs: []string{\"169.254.169.254\", \"fe80::42:a8ff:fe33:6230\"},\n\t\t\t},\n\t\t\tv4Subnets: []NetworkSubnet{\n\t\t\t\tsubnetStub{static: true, contains: map[string]bool{\"192.168.100.10\": true}},\n\t\t\t},\n\t\t\tv6Subnets: []NetworkSubnet{\n\t\t\t\tsubnetStub{static: true, contains: map[string]bool{\"2a01:d2:af:420b:25c1:1816:bb33:855c\": true}},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"static addresses out of range\",\n\t\t\tipamConfig: &EndpointIPAMConfig{\n\t\t\t\tIPv4Address: \"192.168.100.10\",\n\t\t\t\tIPv6Address: \"2a01:d2:af:420b:25c1:1816:bb33:855c\",\n\t\t\t},\n\t\t\tv4Subnets: []NetworkSubnet{\n\t\t\t\tsubnetStub{static: true, contains: map[string]bool{\"192.168.100.10\": false}},\n\t\t\t},\n\t\t\tv6Subnets: []NetworkSubnet{\n\t\t\t\tsubnetStub{static: true, contains: map[string]bool{\"2a01:d2:af:420b:25c1:1816:bb33:855c\": false}},\n\t\t\t},\n\t\t\texpectedErrors: []string{\n\t\t\t\t\"no configured subnet or ip-range contain the IP address 192.168.100.10\",\n\t\t\t\t\"no configured subnet or ip-range contain the IP address 2a01:d2:af:420b:25c1:1816:bb33:855c\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"static addresses with dynamic network subnets\",\n\t\t\tipamConfig: &EndpointIPAMConfig{\n\t\t\t\tIPv4Address: \"192.168.100.10\",\n\t\t\t\tIPv6Address: \"2a01:d2:af:420b:25c1:1816:bb33:855c\",\n\t\t\t},\n\t\t\tv4Subnets: []NetworkSubnet{\n\t\t\t\tsubnetStub{static: false},\n\t\t\t},\n\t\t\tv6Subnets: []NetworkSubnet{\n\t\t\t\tsubnetStub{static: false},\n\t\t\t},\n\t\t\texpectedErrors: []string{\n\t\t\t\t\"user specified IP address is supported only when connecting to networks with user configured subnets\",\n\t\t\t\t\"user specified IP address is supported only when connecting to networks with user configured subnets\",\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\tt.Parallel()\n\n\t\t\terr := tc.ipamConfig.IsInRange(tc.v4Subnets, tc.v6Subnets)\n\t\t\tif tc.expectedErrors == nil {\n\t\t\t\tassert.NilError(t, err)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tif _, ok := err.(interface{ Unwrap() []error }); !ok {\n\t\t\t\tt.Fatal(\"returned error isn't a multierror\")\n\t\t\t}\n\t\t\terrs := err.(interface{ Unwrap() []error }).Unwrap()\n\t\t\tassert.Check(t, len(errs) == len(tc.expectedErrors), \"errs: %+v\", errs)\n\n\t\t\tfor _, expected := range tc.expectedErrors {\n\t\t\t\tassert.Check(t, is.ErrorContains(err, expected))\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestEndpointIPAMConfigWithInvalidConfig(t *testing.T) {\n\ttests := []struct {\n\t\tname           string\n\t\tipamConfig     *EndpointIPAMConfig\n\t\texpectedErrors []string\n\t}{\n\t\t{\n\t\t\tname: \"valid config\",\n\t\t\tipamConfig: &EndpointIPAMConfig{\n\t\t\t\tIPv4Address:  \"192.168.100.10\",\n\t\t\t\tIPv6Address:  \"2a01:d2:af:420b:25c1:1816:bb33:855c\",\n\t\t\t\tLinkLocalIPs: []string{\"169.254.169.254\", \"fe80::42:a8ff:fe33:6230\"},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"invalid IP addresses\",\n\t\t\tipamConfig: &EndpointIPAMConfig{\n\t\t\t\tIPv4Address:  \"foo\",\n\t\t\t\tIPv6Address:  \"bar\",\n\t\t\t\tLinkLocalIPs: []string{\"baz\", \"foobar\"},\n\t\t\t},\n\t\t\texpectedErrors: []string{\n\t\t\t\t\"invalid IPv4 address: foo\",\n\t\t\t\t\"invalid IPv6 address: bar\",\n\t\t\t\t\"invalid link-local IP address: baz\",\n\t\t\t\t\"invalid link-local IP address: foobar\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname:       \"ipv6 address with a zone\",\n\t\t\tipamConfig: &EndpointIPAMConfig{IPv6Address: \"fe80::1cc0:3e8c:119f:c2e1%ens18\"},\n\t\t\texpectedErrors: []string{\n\t\t\t\t\"invalid IPv6 address: fe80::1cc0:3e8c:119f:c2e1%ens18\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"unspecified address is invalid\",\n\t\t\tipamConfig: &EndpointIPAMConfig{\n\t\t\t\tIPv4Address:  \"0.0.0.0\",\n\t\t\t\tIPv6Address:  \"::\",\n\t\t\t\tLinkLocalIPs: []string{\"0.0.0.0\", \"::\"},\n\t\t\t},\n\t\t\texpectedErrors: []string{\n\t\t\t\t\"invalid IPv4 address: 0.0.0.0\",\n\t\t\t\t\"invalid IPv6 address: ::\",\n\t\t\t\t\"invalid link-local IP address: 0.0.0.0\",\n\t\t\t\t\"invalid link-local IP address: ::\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"empty link-local\",\n\t\t\tipamConfig: &EndpointIPAMConfig{\n\t\t\t\tLinkLocalIPs: []string{\"\"},\n\t\t\t},\n\t\t\texpectedErrors: []string{\"invalid link-local IP address:\"},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\tt.Parallel()\n\n\t\t\terr := tc.ipamConfig.Validate()\n\t\t\tif tc.expectedErrors == nil {\n\t\t\t\tassert.NilError(t, err)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tif _, ok := err.(interface{ Unwrap() []error }); !ok {\n\t\t\t\tt.Fatal(\"returned error isn't a multierror\")\n\t\t\t}\n\t\t\terrs := err.(interface{ Unwrap() []error }).Unwrap()\n\t\t\tassert.Check(t, len(errs) == len(tc.expectedErrors), \"errs: %+v\", errs)\n\n\t\t\tfor _, expected := range tc.expectedErrors {\n\t\t\t\tassert.Check(t, is.ErrorContains(err, expected))\n\t\t\t}\n\t\t})\n\t}\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: moby\nSource File Path(s): ['moby\\\\ipam_test.go\\\\source_files\\\\ipam.go']\nTest File Path: moby\\ipam_test.go\\ipam_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: moby\\ipam_test.go\\source_files\\ipam.go\npackage network\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"net/netip\"\n\n\t\"github.com/docker/docker/internal/multierror\"\n)\n\n// IPAM represents IP Address Management\ntype IPAM struct {\n\tDriver  string\n\tOptions map[string]string // Per network IPAM driver options\n\tConfig  []IPAMConfig\n}\n\n// IPAMConfig represents IPAM configurations\ntype IPAMConfig struct {\n\tSubnet     string            `json:\",omitempty\"`\n\tIPRange    string            `json:\",omitempty\"`\n\tGateway    string            `json:\",omitempty\"`\n\tAuxAddress map[string]string `json:\"AuxiliaryAddresses,omitempty\"`\n}\n\ntype ipFamily string\n\nconst (\n\tip4 ipFamily = \"IPv4\"\n\tip6 ipFamily = \"IPv6\"\n)\n\n// ValidateIPAM checks whether the network's IPAM passed as argument is valid. It returns a joinError of the list of\n// errors found.\nfunc ValidateIPAM(ipam *IPAM, enableIPv6 bool) error {\n\tif ipam == nil {\n\t\treturn nil\n\t}\n\n\tvar errs []error\n\tfor _, cfg := range ipam.Config {\n\t\tsubnet, err := netip.ParsePrefix(cfg.Subnet)\n\t\tif err != nil {\n\t\t\terrs = append(errs, fmt.Errorf(\"invalid subnet %s: invalid CIDR block notation\", cfg.Subnet))\n\t\t\tcontinue\n\t\t}\n\t\tsubnetFamily := ip4\n\t\tif subnet.Addr().Is6() {\n\t\t\tsubnetFamily = ip6\n\t\t}\n\n\t\tif !enableIPv6 && subnetFamily == ip6 {\n\t\t\tcontinue\n\t\t}\n\n\t\tif subnet != subnet.Masked() {\n\t\t\terrs = append(errs, fmt.Errorf(\"invalid subnet %s: it should be %s\", subnet, subnet.Masked()))\n\t\t}\n\n\t\tif ipRangeErrs := validateIPRange(cfg.IPRange, subnet, subnetFamily); len(ipRangeErrs) > 0 {\n\t\t\terrs = append(errs, ipRangeErrs...)\n\t\t}\n\n\t\tif err := validateAddress(cfg.Gateway, subnet, subnetFamily); err != nil {\n\t\t\terrs = append(errs, fmt.Errorf(\"invalid gateway %s: %w\", cfg.Gateway, err))\n\t\t}\n\n\t\tfor auxName, aux := range cfg.AuxAddress {\n\t\t\tif err := validateAddress(aux, subnet, subnetFamily); err != nil {\n\t\t\t\terrs = append(errs, fmt.Errorf(\"invalid auxiliary address %s: %w\", auxName, err))\n\t\t\t}\n\t\t}\n\t}\n\n\tif err := multierror.Join(errs...); err != nil {\n\t\treturn fmt.Errorf(\"invalid network config:\\n%w\", err)\n\t}\n\n\treturn nil\n}\n\nfunc validateIPRange(ipRange string, subnet netip.Prefix, subnetFamily ipFamily) []error {\n\tif ipRange == \"\" {\n\t\treturn nil\n\t}\n\tprefix, err := netip.ParsePrefix(ipRange)\n\tif err != nil {\n\t\treturn []error{fmt.Errorf(\"invalid ip-range %s: invalid CIDR block notation\", ipRange)}\n\t}\n\tfamily := ip4\n\tif prefix.Addr().Is6() {\n\t\tfamily = ip6\n\t}\n\n\tif family != subnetFamily {\n\t\treturn []error{fmt.Errorf(\"invalid ip-range %s: parent subnet is an %s block\", ipRange, subnetFamily)}\n\t}\n\n\tvar errs []error\n\tif prefix.Bits() < subnet.Bits() {\n\t\terrs = append(errs, fmt.Errorf(\"invalid ip-range %s: CIDR block is bigger than its parent subnet %s\", ipRange, subnet))\n\t}\n\tif prefix != prefix.Masked() {\n\t\terrs = append(errs, fmt.Errorf(\"invalid ip-range %s: it should be %s\", prefix, prefix.Masked()))\n\t}\n\tif !subnet.Overlaps(prefix) {\n\t\terrs = append(errs, fmt.Errorf(\"invalid ip-range %s: parent subnet %s doesn't contain ip-range\", ipRange, subnet))\n\t}\n\n\treturn errs\n}\n\nfunc validateAddress(address string, subnet netip.Prefix, subnetFamily ipFamily) error {\n\tif address == \"\" {\n\t\treturn nil\n\t}\n\taddr, err := netip.ParseAddr(address)\n\tif err != nil {\n\t\treturn errors.New(\"invalid address\")\n\t}\n\tfamily := ip4\n\tif addr.Is6() {\n\t\tfamily = ip6\n\t}\n\n\tif family != subnetFamily {\n\t\treturn fmt.Errorf(\"parent subnet is an %s block\", subnetFamily)\n\t}\n\tif !subnet.Contains(addr) {\n\t\treturn fmt.Errorf(\"parent subnet %s doesn't contain this address\", subnet)\n\t}\n\n\treturn nil\n}\n\n### Source File Dependency Files Content\n### Dependency File: moby\\ipam_test.go\\dependent_files\\multierror.go\npackage multierror\n\nimport (\n\t\"strings\"\n)\n\n// Join is a drop-in replacement for errors.Join with better formatting.\nfunc Join(errs ...error) error {\n\tn := 0\n\tfor _, err := range errs {\n\t\tif err != nil {\n\t\t\tn++\n\t\t}\n\t}\n\tif n == 0 {\n\t\treturn nil\n\t}\n\te := &joinError{\n\t\terrs: make([]error, 0, n),\n\t}\n\tfor _, err := range errs {\n\t\tif err != nil {\n\t\t\te.errs = append(e.errs, err)\n\t\t}\n\t}\n\treturn e\n}\n\ntype joinError struct {\n\terrs []error\n}\n\nfunc (e *joinError) Error() string {\n\tif len(e.errs) == 1 {\n\t\treturn strings.TrimSpace(e.errs[0].Error())\n\t}\n\tstringErrs := make([]string, 0, len(e.errs))\n\tfor _, subErr := range e.errs {\n\t\tstringErrs = append(stringErrs, strings.Replace(subErr.Error(), \"\\n\", \"\\n\\t\", -1))\n\t}\n\treturn \"* \" + strings.Join(stringErrs, \"\\n* \")\n}\n\nfunc (e *joinError) Unwrap() []error {\n\treturn e.errs\n}\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:51:53"}, {"role": "assistant", "content": "```go\npackage network\n\nimport (\n\t\"testing\"\n\n\t\"gotest.tools/v3/assert\"\n\tis \"gotest.tools/v3/assert/cmp\"\n)\n\nfunc TestNetworkWithInvalidIPAM(t *testing.T) {\n\ttests := []struct {\n\t\tname           string\n\t\tipam           IPAM\n\t\tipv6           bool\n\t\texpectedErrors []string\n\t}{\n\t\t{\n\t\t\tname: \"IP version mismatch\",\n\t\t\tipam: IPAM{\n\t\t\t\tConfig: []IPAMConfig{{\n\t\t\t\t\tSubnet:     \"10.10.10.0/24\",\n\t\t\t\t\tIPRange:    \"2001:db8::/32\",\n\t\t\t\t\tGateway:    \"2001:db8::1\",\n\t\t\t\t\tAuxAddress: map[string]string{\"DefaultGatewayIPv4\": \"2001:db8::1\"},\n\t\t\t\t}},\n\t\t\t},\n\t\t\texpectedErrors: []string{\n\t\t\t\t\"invalid ip-range 2001:db8::/32: parent subnet is an IPv4 block\",\n\t\t\t\t\"invalid gateway 2001:db8::1: parent subnet is an IPv4 block\",\n\t\t\t\t\"invalid auxiliary address DefaultGatewayIPv4: parent subnet is an IPv4 block\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t// Regression test for https://github.com/moby/moby/issues/47202\n\t\t\tname: \"IPv6 subnet is discarded with no error when IPv6 is disabled\",\n\t\t\tipam: IPAM{Config: []IPAMConfig{{Subnet: \"2001:db8::/32\"}}},\n\t\t\tipv6: false,\n\t\t},\n\t\t{\n\t\t\tname: \"Invalid data - Subnet\",\n\t\t\tipam: IPAM{Config: []IPAMConfig{{Subnet: \"foobar\"}}},\n\t\t\texpectedErrors: []string{\n\t\t\t\t`invalid subnet foobar: invalid CIDR block notation`,\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"Invalid data\",\n\t\t\tipam: IPAM{\n\t\t\t\tConfig: []IPAMConfig{{\n\t\t\t\t\tSubnet:     \"10.10.10.0/24\",\n\t\t\t\t\tIPRange:    \"foobar\",\n\t\t\t\t\tGateway:    \"1001.10.5.3\",\n\t\t\t\t\tAuxAddress: map[string]string{\"DefaultGatewayIPv4\": \"dummy\"},\n\t\t\t\t}},\n\t\t\t},\n\t\t\texpectedErrors: []string{\n\t\t\t\t\"invalid ip-range foobar: invalid CIDR block notation\",\n\t\t\t\t\"invalid gateway 1001.10.5.3: invalid address\",\n\t\t\t\t\"invalid auxiliary address DefaultGatewayIPv4: invalid address\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"IPRange bigger than its subnet\",\n\t\t\tipam: IPAM{\n\t\t\t\tConfig: []IPAMConfig{\n\t\t\t\t\t{Subnet: \"10.10.10.0/24\", IPRange: \"10.0.0.0/8\"},\n\t\t\t\t},\n\t\t\t},\n\t\t\texpectedErrors: []string{\n\t\t\t\t\"invalid ip-range 10.0.0.0/8: CIDR block is bigger than its parent subnet 10.10.10.0/24\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"Out of range prefix & addresses\",\n\t\t\tipam: IPAM{\n\t\t\t\tConfig: []IPAMConfig{{\n\t\t\t\t\tSubnet:     \"10.0.0.0/8\",\n\t\t\t\t\tIPRange:    \"192.168.0.1/24\",\n\t\t\t\t\tGateway:    \"192.168.0.1\",\n\t\t\t\t\tAuxAddress: map[string]string{\"DefaultGatewayIPv4\": \"192.168.0.1\"},\n\t\t\t\t}},\n\t\t\t},\n\t\t\texpectedErrors: []string{\n\t\t\t\t\"invalid ip-range 192.168.0.1/24: it should be 192.168.0.0/24\",\n\t\t\t\t\"invalid ip-range 192.168.0.1/24: parent subnet 10.0.0.0/8 doesn't contain ip-range\",\n\t\t\t\t\"invalid gateway 192.168.0.1: parent subnet 10.0.0.0/8 doesn't contain this address\",\n\t\t\t\t\"invalid auxiliary address DefaultGatewayIPv4: parent subnet 10.0.0.0/8 doesn't contain this address\",\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tname: \"Subnet with host fragment set\",\n\t\t\tipam: IPAM{\n\t\t\t\tConfig: []IPAMConfig{{\n\t\t\t\t\tSubnet: \"10.10.10.0/8\",\n\t\t\t\t}},\n\t\t\t},\n\t\t\texpectedErrors: []string{\"invalid subnet 10.10.10.0/8: it should be 10.0.0.0/8\"},\n\t\t},\n\t\t{\n\t\t\tname: \"IPRange with host fragment set\",\n\t\t\tipam: IPAM{\n\t\t\t\tConfig: []IPAMConfig{{\n\t\t\t\t\tSubnet:  \"10.0.0.0/8\",\n\t\t\t\t\tIPRange: \"10.10.10.0/16\",\n\t\t\t\t}},\n\t\t\t},\n\t\t\texpectedErrors: []string{\"invalid ip-range 10.10.10.0/16: it should be 10.10.0.0/16\"},\n\t\t},\n\t\t{\n\t\t\tname: \"Empty IPAM is valid\",\n\t\t\tipam: IPAM{},\n\t\t},\n\t\t{\n\t\t\tname: \"Valid IPAM\",\n\t\t\tipam: IPAM{\n\t\t\t\tConfig: []IPAMConfig{{\n\t\t\t\t\tSubnet:     \"10.0.0.0/8\",\n\t\t\t\t\tIPRange:    \"10.10.0.0/16\",\n\t\t\t\t\tGateway:    \"10.10.0.1\",\n\t\t\t\t\tAuxAddress: map[string]string{\"DefaultGatewayIPv4\": \"10.10.0.1\"},\n\t\t\t\t}},\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, tc := range tests {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\tt.Parallel()\n\n\t\t\terrs := ValidateIPAM(&tc.ipam, tc.ipv6)\n\t\t\tif tc.expectedErrors == nil {\n\t\t\t\tassert.NilError(t, errs)\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tassert.Check(t, is.ErrorContains(errs, \"invalid network config\"))\n\t\t\tfor _, expected := range tc.expectedErrors {\n\t\t\t\tassert.Check(t, is.ErrorContains(errs, expected))\n\t\t\t}\n\t\t})\n\t}\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: moby\nSource File Path(s): ['moby\\\\parse_test.go\\\\source_files\\\\parse.go']\nTest File Path: moby\\parse_test.go\\parse_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: moby\\parse_test.go\\source_files\\parse.go\n/*\nPackage filters provides tools for encoding a mapping of keys to a set of\nmultiple values.\n*/\npackage filters // import \"github.com/docker/docker/api/types/filters\"\n\nimport (\n\t\"encoding/json\"\n\t\"regexp\"\n\t\"strings\"\n\n\t\"github.com/docker/docker/api/types/versions\"\n)\n\n// Args stores a mapping of keys to a set of multiple values.\ntype Args struct {\n\tfields map[string]map[string]bool\n}\n\n// KeyValuePair are used to initialize a new Args\ntype KeyValuePair struct {\n\tKey   string\n\tValue string\n}\n\n// Arg creates a new KeyValuePair for initializing Args\nfunc Arg(key, value string) KeyValuePair {\n\treturn KeyValuePair{Key: key, Value: value}\n}\n\n// NewArgs returns a new Args populated with the initial args\nfunc NewArgs(initialArgs ...KeyValuePair) Args {\n\targs := Args{fields: map[string]map[string]bool{}}\n\tfor _, arg := range initialArgs {\n\t\targs.Add(arg.Key, arg.Value)\n\t}\n\treturn args\n}\n\n// Keys returns all the keys in list of Args\nfunc (args Args) Keys() []string {\n\tkeys := make([]string, 0, len(args.fields))\n\tfor k := range args.fields {\n\t\tkeys = append(keys, k)\n\t}\n\treturn keys\n}\n\n// MarshalJSON returns a JSON byte representation of the Args\nfunc (args Args) MarshalJSON() ([]byte, error) {\n\tif len(args.fields) == 0 {\n\t\treturn []byte(\"{}\"), nil\n\t}\n\treturn json.Marshal(args.fields)\n}\n\n// ToJSON returns the Args as a JSON encoded string\nfunc ToJSON(a Args) (string, error) {\n\tif a.Len() == 0 {\n\t\treturn \"\", nil\n\t}\n\tbuf, err := json.Marshal(a)\n\treturn string(buf), err\n}\n\n// ToParamWithVersion encodes Args as a JSON string. If version is less than 1.22\n// then the encoded format will use an older legacy format where the values are a\n// list of strings, instead of a set.\n//\n// Deprecated: do not use in any new code; use ToJSON instead\nfunc ToParamWithVersion(version string, a Args) (string, error) {\n\tif a.Len() == 0 {\n\t\treturn \"\", nil\n\t}\n\n\tif version != \"\" && versions.LessThan(version, \"1.22\") {\n\t\tbuf, err := json.Marshal(convertArgsToSlice(a.fields))\n\t\treturn string(buf), err\n\t}\n\n\treturn ToJSON(a)\n}\n\n// FromJSON decodes a JSON encoded string into Args\nfunc FromJSON(p string) (Args, error) {\n\targs := NewArgs()\n\n\tif p == \"\" {\n\t\treturn args, nil\n\t}\n\n\traw := []byte(p)\n\terr := json.Unmarshal(raw, &args)\n\tif err == nil {\n\t\treturn args, nil\n\t}\n\n\t// Fallback to parsing arguments in the legacy slice format\n\tdeprecated := map[string][]string{}\n\tif legacyErr := json.Unmarshal(raw, &deprecated); legacyErr != nil {\n\t\treturn args, &invalidFilter{}\n\t}\n\n\targs.fields = deprecatedArgs(deprecated)\n\treturn args, nil\n}\n\n// UnmarshalJSON populates the Args from JSON encode bytes\nfunc (args Args) UnmarshalJSON(raw []byte) error {\n\treturn json.Unmarshal(raw, &args.fields)\n}\n\n// Get returns the list of values associated with the key\nfunc (args Args) Get(key string) []string {\n\tvalues := args.fields[key]\n\tif values == nil {\n\t\treturn make([]string, 0)\n\t}\n\tslice := make([]string, 0, len(values))\n\tfor key := range values {\n\t\tslice = append(slice, key)\n\t}\n\treturn slice\n}\n\n// Add a new value to the set of values\nfunc (args Args) Add(key, value string) {\n\tif _, ok := args.fields[key]; ok {\n\t\targs.fields[key][value] = true\n\t} else {\n\t\targs.fields[key] = map[string]bool{value: true}\n\t}\n}\n\n// Del removes a value from the set\nfunc (args Args) Del(key, value string) {\n\tif _, ok := args.fields[key]; ok {\n\t\tdelete(args.fields[key], value)\n\t\tif len(args.fields[key]) == 0 {\n\t\t\tdelete(args.fields, key)\n\t\t}\n\t}\n}\n\n// Len returns the number of keys in the mapping\nfunc (args Args) Len() int {\n\treturn len(args.fields)\n}\n\n// MatchKVList returns true if all the pairs in sources exist as key=value\n// pairs in the mapping at key, or if there are no values at key.\nfunc (args Args) MatchKVList(key string, sources map[string]string) bool {\n\tfieldValues := args.fields[key]\n\n\t// do not filter if there is no filter set or cannot determine filter\n\tif len(fieldValues) == 0 {\n\t\treturn true\n\t}\n\n\tif len(sources) == 0 {\n\t\treturn false\n\t}\n\n\tfor value := range fieldValues {\n\t\ttestK, testV, hasValue := strings.Cut(value, \"=\")\n\n\t\tv, ok := sources[testK]\n\t\tif !ok {\n\t\t\treturn false\n\t\t}\n\t\tif hasValue && testV != v {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// Match returns true if any of the values at key match the source string\nfunc (args Args) Match(field, source string) bool {\n\tif args.ExactMatch(field, source) {\n\t\treturn true\n\t}\n\n\tfieldValues := args.fields[field]\n\tfor name2match := range fieldValues {\n\t\tmatch, err := regexp.MatchString(name2match, source)\n\t\tif err != nil {\n\t\t\tcontinue\n\t\t}\n\t\tif match {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// GetBoolOrDefault returns a boolean value of the key if the key is present\n// and is interpretable as a boolean value. Otherwise the default value is returned.\n// Error is not nil only if the filter values are not valid boolean or are conflicting.\nfunc (args Args) GetBoolOrDefault(key string, defaultValue bool) (bool, error) {\n\tfieldValues, ok := args.fields[key]\n\tif !ok {\n\t\treturn defaultValue, nil\n\t}\n\n\tif len(fieldValues) == 0 {\n\t\treturn defaultValue, &invalidFilter{key, nil}\n\t}\n\n\tisFalse := fieldValues[\"0\"] || fieldValues[\"false\"]\n\tisTrue := fieldValues[\"1\"] || fieldValues[\"true\"]\n\tif isFalse == isTrue {\n\t\t// Either no or conflicting truthy/falsy value were provided\n\t\treturn defaultValue, &invalidFilter{key, args.Get(key)}\n\t}\n\treturn isTrue, nil\n}\n\n// ExactMatch returns true if the source matches exactly one of the values.\nfunc (args Args) ExactMatch(key, source string) bool {\n\tfieldValues, ok := args.fields[key]\n\t// do not filter if there is no filter set or cannot determine filter\n\tif !ok || len(fieldValues) == 0 {\n\t\treturn true\n\t}\n\n\t// try to match full name value to avoid O(N) regular expression matching\n\treturn fieldValues[source]\n}\n\n// UniqueExactMatch returns true if there is only one value and the source\n// matches exactly the value.\nfunc (args Args) UniqueExactMatch(key, source string) bool {\n\tfieldValues := args.fields[key]\n\t// do not filter if there is no filter set or cannot determine filter\n\tif len(fieldValues) == 0 {\n\t\treturn true\n\t}\n\tif len(args.fields[key]) != 1 {\n\t\treturn false\n\t}\n\n\t// try to match full name value to avoid O(N) regular expression matching\n\treturn fieldValues[source]\n}\n\n// FuzzyMatch returns true if the source matches exactly one value,  or the\n// source has one of the values as a prefix.\nfunc (args Args) FuzzyMatch(key, source string) bool {\n\tif args.ExactMatch(key, source) {\n\t\treturn true\n\t}\n\n\tfieldValues := args.fields[key]\n\tfor prefix := range fieldValues {\n\t\tif strings.HasPrefix(source, prefix) {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// Contains returns true if the key exists in the mapping\nfunc (args Args) Contains(field string) bool {\n\t_, ok := args.fields[field]\n\treturn ok\n}\n\n// Validate compared the set of accepted keys against the keys in the mapping.\n// An error is returned if any mapping keys are not in the accepted set.\nfunc (args Args) Validate(accepted map[string]bool) error {\n\tfor name := range args.fields {\n\t\tif !accepted[name] {\n\t\t\treturn &invalidFilter{name, nil}\n\t\t}\n\t}\n\treturn nil\n}\n\n// WalkValues iterates over the list of values for a key in the mapping and calls\n// op() for each value. If op returns an error the iteration stops and the\n// error is returned.\nfunc (args Args) WalkValues(field string, op func(value string) error) error {\n\tif _, ok := args.fields[field]; !ok {\n\t\treturn nil\n\t}\n\tfor v := range args.fields[field] {\n\t\tif err := op(v); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\n// Clone returns a copy of args.\nfunc (args Args) Clone() (newArgs Args) {\n\tnewArgs.fields = make(map[string]map[string]bool, len(args.fields))\n\tfor k, m := range args.fields {\n\t\tvar mm map[string]bool\n\t\tif m != nil {\n\t\t\tmm = make(map[string]bool, len(m))\n\t\t\tfor kk, v := range m {\n\t\t\t\tmm[kk] = v\n\t\t\t}\n\t\t}\n\t\tnewArgs.fields[k] = mm\n\t}\n\treturn newArgs\n}\n\nfunc deprecatedArgs(d map[string][]string) map[string]map[string]bool {\n\tm := map[string]map[string]bool{}\n\tfor k, v := range d {\n\t\tvalues := map[string]bool{}\n\t\tfor _, vv := range v {\n\t\t\tvalues[vv] = true\n\t\t}\n\t\tm[k] = values\n\t}\n\treturn m\n}\n\nfunc convertArgsToSlice(f map[string]map[string]bool) map[string][]string {\n\tm := map[string][]string{}\n\tfor k, v := range f {\n\t\tvalues := []string{}\n\t\tfor kk := range v {\n\t\t\tif v[kk] {\n\t\t\t\tvalues = append(values, kk)\n\t\t\t}\n\t\t}\n\t\tm[k] = values\n\t}\n\treturn m\n}\n\n### Source File Dependency Files Content\n### Dependency File: moby\\parse_test.go\\dependent_files\\compare.go\npackage versions // import \"github.com/docker/docker/api/types/versions\"\n\nimport (\n\t\"strconv\"\n\t\"strings\"\n)\n\n// compare compares two version strings\n// returns -1 if v1 < v2, 1 if v1 > v2, 0 otherwise.\nfunc compare(v1, v2 string) int {\n\tif v1 == v2 {\n\t\treturn 0\n\t}\n\tvar (\n\t\tcurrTab  = strings.Split(v1, \".\")\n\t\totherTab = strings.Split(v2, \".\")\n\t)\n\n\tmaxVer := len(currTab)\n\tif len(otherTab) > maxVer {\n\t\tmaxVer = len(otherTab)\n\t}\n\tfor i := 0; i < maxVer; i++ {\n\t\tvar currInt, otherInt int\n\n\t\tif len(currTab) > i {\n\t\t\tcurrInt, _ = strconv.Atoi(currTab[i])\n\t\t}\n\t\tif len(otherTab) > i {\n\t\t\totherInt, _ = strconv.Atoi(otherTab[i])\n\t\t}\n\t\tif currInt > otherInt {\n\t\t\treturn 1\n\t\t}\n\t\tif otherInt > currInt {\n\t\t\treturn -1\n\t\t}\n\t}\n\treturn 0\n}\n\n// LessThan checks if a version is less than another\nfunc LessThan(v, other string) bool {\n\treturn compare(v, other) == -1\n}\n\n// LessThanOrEqualTo checks if a version is less than or equal to another\nfunc LessThanOrEqualTo(v, other string) bool {\n\treturn compare(v, other) <= 0\n}\n\n// GreaterThan checks if a version is greater than another\nfunc GreaterThan(v, other string) bool {\n\treturn compare(v, other) == 1\n}\n\n// GreaterThanOrEqualTo checks if a version is greater than or equal to another\nfunc GreaterThanOrEqualTo(v, other string) bool {\n\treturn compare(v, other) >= 0\n}\n\n// Equal checks if a version is equal to another\nfunc Equal(v, other string) bool {\n\treturn compare(v, other) == 0\n}\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:51:53"}, {"role": "assistant", "content": "```go\npackage filters // import \"github.com/docker/docker/api/types/filters\"\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"sort\"\n\t\"testing\"\n\n\t\"gotest.tools/v3/assert\"\n\tis \"gotest.tools/v3/assert/cmp\"\n)\n\nfunc TestMarshalJSON(t *testing.T) {\n\ta := NewArgs(\n\t\tArg(\"created\", \"today\"),\n\t\tArg(\"image.name\", \"ubuntu*\"),\n\t\tArg(\"image.name\", \"*untu\"),\n\t)\n\n\ts, err := a.MarshalJSON()\n\tassert.Check(t, err)\n\tconst expected = `{\"created\":{\"today\":true},\"image.name\":{\"*untu\":true,\"ubuntu*\":true}}`\n\tassert.Check(t, is.Equal(string(s), expected))\n}\n\nfunc TestMarshalJSONWithEmpty(t *testing.T) {\n\ts, err := json.Marshal(NewArgs())\n\tassert.Check(t, err)\n\tconst expected = `{}`\n\tassert.Check(t, is.Equal(string(s), expected))\n}\n\nfunc TestToJSON(t *testing.T) {\n\ta := NewArgs(\n\t\tArg(\"created\", \"today\"),\n\t\tArg(\"image.name\", \"ubuntu*\"),\n\t\tArg(\"image.name\", \"*untu\"),\n\t)\n\n\ts, err := ToJSON(a)\n\tassert.Check(t, err)\n\tconst expected = `{\"created\":{\"today\":true},\"image.name\":{\"*untu\":true,\"ubuntu*\":true}}`\n\tassert.Check(t, is.Equal(s, expected))\n}\n\nfunc TestToParamWithVersion(t *testing.T) {\n\ta := NewArgs(\n\t\tArg(\"created\", \"today\"),\n\t\tArg(\"image.name\", \"ubuntu*\"),\n\t\tArg(\"image.name\", \"*untu\"),\n\t)\n\n\tstr1, err := ToParamWithVersion(\"1.21\", a)\n\tassert.Check(t, err)\n\tstr2, err := ToParamWithVersion(\"1.22\", a)\n\tassert.Check(t, err)\n\tif str1 != `{\"created\":[\"today\"],\"image.name\":[\"*untu\",\"ubuntu*\"]}` &&\n\t\tstr1 != `{\"created\":[\"today\"],\"image.name\":[\"ubuntu*\",\"*untu\"]}` {\n\t\tt.Errorf(\"incorrectly marshaled the filters: %s\", str1)\n\t}\n\tif str2 != `{\"created\":{\"today\":true},\"image.name\":{\"*untu\":true,\"ubuntu*\":true}}` &&\n\t\tstr2 != `{\"created\":{\"today\":true},\"image.name\":{\"ubuntu*\":true,\"*untu\":true}}` {\n\t\tt.Errorf(\"incorrectly marshaled the filters: %s\", str2)\n\t}\n}\n\nfunc TestFromJSON(t *testing.T) {\n\tinvalids := []string{\n\t\t\"anything\",\n\t\t\"['a','list']\",\n\t\t\"{'key': 'value'}\",\n\t\t`{\"key\": \"value\"}`,\n\t}\n\tvalid := map[*Args][]string{\n\t\t{fields: map[string]map[string]bool{\"key\": {\"value\": true}}}: {\n\t\t\t`{\"key\": [\"value\"]}`,\n\t\t\t`{\"key\": {\"value\": true}}`,\n\t\t},\n\t\t{fields: map[string]map[string]bool{\"key\": {\"value1\": true, \"value2\": true}}}: {\n\t\t\t`{\"key\": [\"value1\", \"value2\"]}`,\n\t\t\t`{\"key\": {\"value1\": true, \"value2\": true}}`,\n\t\t},\n\t\t{fields: map[string]map[string]bool{\"key1\": {\"value1\": true}, \"key2\": {\"value2\": true}}}: {\n\t\t\t`{\"key1\": [\"value1\"], \"key2\": [\"value2\"]}`,\n\t\t\t`{\"key1\": {\"value1\": true}, \"key2\": {\"value2\": true}}`,\n\t\t},\n\t}\n\n\tfor _, invalid := range invalids {\n\t\tt.Run(invalid, func(t *testing.T) {\n\t\t\t_, err := FromJSON(invalid)\n\t\t\tif err == nil {\n\t\t\t\tt.Fatalf(\"Expected an error with %v, got nothing\", invalid)\n\t\t\t}\n\t\t\tvar invalidFilterError *invalidFilter\n\t\t\tassert.Check(t, is.ErrorType(err, invalidFilterError))\n\t\t\twrappedErr := fmt.Errorf(\"something went wrong: %w\", err)\n\t\t\tassert.Check(t, is.ErrorIs(wrappedErr, err))\n\t\t})\n\t}\n\n\tfor expectedArgs, matchers := range valid {\n\t\tfor _, jsonString := range matchers {\n\t\t\targs, err := FromJSON(jsonString)\n\t\t\tassert.Check(t, err)\n\t\t\tassert.Check(t, is.Equal(args.Len(), expectedArgs.Len()))\n\t\t\tfor key, expectedValues := range expectedArgs.fields {\n\t\t\t\tvalues := args.Get(key)\n\t\t\t\tassert.Check(t, is.Len(values, len(expectedValues)), expectedArgs)\n\n\t\t\t\tfor _, v := range values {\n\t\t\t\t\tif !expectedValues[v] {\n\t\t\t\t\t\tt.Errorf(\"Expected %v, go %v\", expectedArgs, args)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestEmpty(t *testing.T) {\n\ta := Args{}\n\tassert.Check(t, is.Equal(a.Len(), 0))\n\tv, err := ToJSON(a)\n\tassert.Check(t, err)\n\tv1, err := FromJSON(v)\n\tassert.Check(t, err)\n\tassert.Check(t, is.Equal(v1.Len(), 0))\n}\n\nfunc TestArgsMatchKVListEmptySources(t *testing.T) {\n\targs := NewArgs()\n\tif !args.MatchKVList(\"created\", map[string]string{}) {\n\t\tt.Fatalf(\"Expected true for (%v,created), got true\", args)\n\t}\n\n\targs = Args{map[string]map[string]bool{\"created\": {\"today\": true}}}\n\tif args.MatchKVList(\"created\", map[string]string{}) {\n\t\tt.Fatalf(\"Expected false for (%v,created), got true\", args)\n\t}\n}\n\nfunc TestArgsMatchKVList(t *testing.T) {\n\t// Not empty sources\n\tsources := map[string]string{\n\t\t\"key1\": \"value1\",\n\t\t\"key2\": \"value2\",\n\t\t\"key3\": \"value3\",\n\t}\n\n\tmatches := map[*Args]string{\n\t\t{}: \"field\",\n\t\t{\n\t\t\tmap[string]map[string]bool{\n\t\t\t\t\"created\": {\"today\": true},\n\t\t\t\t\"labels\":  {\"key1\": true},\n\t\t\t},\n\t\t}: \"labels\",\n\t\t{\n\t\t\tmap[string]map[string]bool{\n\t\t\t\t\"created\": {\"today\": true},\n\t\t\t\t\"labels\":  {\"key1=value1\": true},\n\t\t\t},\n\t\t}: \"labels\",\n\t}\n\n\tfor args, field := range matches {\n\t\tif !args.MatchKVList(field, sources) {\n\t\t\tt.Errorf(\"Expected true for %v on %v, got false\", sources, args)\n\t\t}\n\t}\n\n\tdiffers := map[*Args]string{\n\t\t{\n\t\t\tmap[string]map[string]bool{\n\t\t\t\t\"created\": {\"today\": true},\n\t\t\t},\n\t\t}: \"created\",\n\t\t{\n\t\t\tmap[string]map[string]bool{\n\t\t\t\t\"created\": {\"today\": true},\n\t\t\t\t\"labels\":  {\"key4\": true},\n\t\t\t},\n\t\t}: \"labels\",\n\t\t{\n\t\t\tmap[string]map[string]bool{\n\t\t\t\t\"created\": {\"today\": true},\n\t\t\t\t\"labels\":  {\"key1=value3\": true},\n\t\t\t},\n\t\t}: \"labels\",\n\t}\n\n\tfor args, field := range differs {\n\t\tif args.MatchKVList(field, sources) {\n\t\t\tt.Errorf(\"Expected false for %v on %v, got true\", sources, args)\n\t\t}\n\t}\n}\n\nfunc TestArgsMatch(t *testing.T) {\n\tsource := \"today\"\n\n\tmatches := map[*Args]string{\n\t\t{}: \"field\",\n\t\t{\n\t\t\tmap[string]map[string]bool{\n\t\t\t\t\"created\": {\"today\": true},\n\t\t\t},\n\t\t}: \"today\",\n\t\t{\n\t\t\tmap[string]map[string]bool{\n\t\t\t\t\"created\": {\"to*\": true},\n\t\t\t},\n\t\t}: \"created\",\n\t\t{\n\t\t\tmap[string]map[string]bool{\n\t\t\t\t\"created\": {\"to(.*)\": true},\n\t\t\t},\n\t\t}: \"created\",\n\t\t{\n\t\t\tmap[string]map[string]bool{\n\t\t\t\t\"created\": {\"tod\": true},\n\t\t\t},\n\t\t}: \"created\",\n\t\t{\n\t\t\tmap[string]map[string]bool{\n\t\t\t\t\"created\": {\"anything\": true, \"to*\": true},\n\t\t\t},\n\t\t}: \"created\",\n\t}\n\n\tfor args, field := range matches {\n\t\tassert.Check(t, args.Match(field, source), \"Expected field %s to match %s\", field, source)\n\t}\n\n\tdiffers := map[*Args]string{\n\t\t{\n\t\t\tmap[string]map[string]bool{\n\t\t\t\t\"created\": {\"tomorrow\": true},\n\t\t\t},\n\t\t}: \"created\",\n\t\t{\n\t\t\tmap[string]map[string]bool{\n\t\t\t\t\"created\": {\"to(day\": true},\n\t\t\t},\n\t\t}: \"created\",\n\t\t{\n\t\t\tmap[string]map[string]bool{\n\t\t\t\t\"created\": {\"tom(.*)\": true},\n\t\t\t},\n\t\t}: \"created\",\n\t\t{\n\t\t\tmap[string]map[string]bool{\n\t\t\t\t\"created\": {\"tom\": true},\n\t\t\t},\n\t\t}: \"created\",\n\t\t{\n\t\t\tmap[string]map[string]bool{\n\t\t\t\t\"created\": {\"today1\": true},\n\t\t\t\t\"labels\":  {\"today\": true},\n\t\t\t},\n\t\t}: \"created\",\n\t}\n\n\tfor args, field := range differs {\n\t\tassert.Check(t, !args.Match(field, source), \"Expected field %s to not match %s\", field, source)\n\t}\n}\n\nfunc TestAdd(t *testing.T) {\n\tf := NewArgs()\n\tf.Add(\"status\", \"running\")\n\tv := f.Get(\"status\")\n\tassert.Check(t, is.DeepEqual(v, []string{\"running\"}))\n\n\tf.Add(\"status\", \"paused\")\n\tv = f.Get(\"status\")\n\tassert.Check(t, is.Len(v, 2))\n\tassert.Check(t, is.Contains(v, \"running\"))\n\tassert.Check(t, is.Contains(v, \"paused\"))\n}\n\nfunc TestDel(t *testing.T) {\n\tf := NewArgs()\n\tf.Add(\"status\", \"running\")\n\tf.Del(\"status\", \"running\")\n\tassert.Check(t, is.Equal(f.Len(), 0))\n\tassert.Check(t, is.DeepEqual(f.Get(\"status\"), []string{}))\n}\n\nfunc TestLen(t *testing.T) {\n\tf := NewArgs()\n\tassert.Check(t, is.Equal(f.Len(), 0))\n\tf.Add(\"status\", \"running\")\n\tassert.Check(t, is.Equal(f.Len(), 1))\n}\n\nfunc TestExactMatch(t *testing.T) {\n\tf := NewArgs()\n\n\tassert.Check(t, f.ExactMatch(\"status\", \"running\"), \"Expected to match `running` when there are no filters\")\n\n\tf.Add(\"status\", \"running\")\n\tf.Add(\"status\", \"pause*\")\n\n\tassert.Check(t, f.ExactMatch(\"status\", \"running\"), \"Expected to match `running` with one of the filters\")\n\tassert.Check(t, !f.ExactMatch(\"status\", \"paused\"), \"Expected to not match `paused` with one of the filters\")\n}\n\nfunc TestOnlyOneExactMatch(t *testing.T) {\n\tf := NewArgs()\n\n\tassert.Check(t, f.ExactMatch(\"status\", \"running\"), \"Expected to match `running` when there are no filters\")\n\n\tf.Add(\"status\", \"running\")\n\tassert.Check(t, f.ExactMatch(\"status\", \"running\"), \"Expected to match `running` with one of the filters\")\n\tassert.Check(t, !f.UniqueExactMatch(\"status\", \"paused\"), \"Expected to not match `paused` with one of the filters\")\n\n\tf.Add(\"status\", \"pause\")\n\tassert.Check(t, !f.UniqueExactMatch(\"status\", \"running\"), \"Expected to not match only `running` with two filters\")\n}\n\nfunc TestContains(t *testing.T) {\n\tf := NewArgs()\n\tassert.Check(t, !f.Contains(\"status\"))\n\n\tf.Add(\"status\", \"running\")\n\tassert.Check(t, f.Contains(\"status\"))\n}\n\nfunc TestValidate(t *testing.T) {\n\tf := NewArgs()\n\tf.Add(\"status\", \"running\")\n\n\tvalid := map[string]bool{\n\t\t\"status\":   true,\n\t\t\"dangling\": true,\n\t}\n\n\tassert.Check(t, f.Validate(valid))\n\n\tf.Add(\"bogus\", \"running\")\n\terr := f.Validate(valid)\n\tvar invalidFilterError *invalidFilter\n\tassert.Check(t, is.ErrorType(err, invalidFilterError))\n\twrappedErr := fmt.Errorf(\"something went wrong: %w\", err)\n\tassert.Check(t, is.ErrorIs(wrappedErr, err))\n}\n\nfunc TestWalkValues(t *testing.T) {\n\tf := NewArgs()\n\tf.Add(\"status\", \"running\")\n\tf.Add(\"status\", \"paused\")\n\n\terr := f.WalkValues(\"status\", func(value string) error {\n\t\tif value != \"running\" && value != \"paused\" {\n\t\t\tt.Fatalf(\"Unexpected value %s\", value)\n\t\t}\n\t\treturn nil\n\t})\n\tassert.Check(t, err)\n\n\tloops1 := 0\n\terr = f.WalkValues(\"status\", func(value string) error {\n\t\tloops1++\n\t\treturn nil\n\t})\n\tassert.Check(t, err)\n\tassert.Check(t, is.Equal(loops1, 2), \"Expected to not iterate when the field doesn't exist\")\n\n\tloops2 := 0\n\terr = f.WalkValues(\"unknown-key\", func(value string) error {\n\t\tloops2++\n\t\treturn nil\n\t})\n\tassert.Check(t, err)\n\tassert.Check(t, is.Equal(loops2, 0), \"Expected to not iterate when the field doesn't exist\")\n}\n\nfunc TestFuzzyMatch(t *testing.T) {\n\tf := NewArgs()\n\tf.Add(\"container\", \"foo\")\n\n\tcases := map[string]bool{\n\t\t\"foo\":    true,\n\t\t\"foobar\": true,\n\t\t\"barfoo\": false,\n\t\t\"bar\":    false,\n\t}\n\tfor source, match := range cases {\n\t\tgot := f.FuzzyMatch(\"container\", source)\n\t\tif got != match {\n\t\t\tt.Errorf(\"Expected %v, got %v: %s\", match, got, source)\n\t\t}\n\t}\n}\n\nfunc TestClone(t *testing.T) {\n\tf := NewArgs()\n\tf.Add(\"foo\", \"bar\")\n\tf2 := f.Clone()\n\tf2.Add(\"baz\", \"qux\")\n\tassert.Check(t, is.Len(f.Get(\"baz\"), 0))\n}\n\nfunc TestGetBoolOrDefault(t *testing.T) {\n\tfor _, tc := range []struct {\n\t\tname          string\n\t\targs          map[string][]string\n\t\tdefValue      bool\n\t\texpectedErr   error\n\t\texpectedValue bool\n\t}{\n\t\t{\n\t\t\tname: \"single true\",\n\t\t\targs: map[string][]string{\n\t\t\t\t\"dangling\": {\"true\"},\n\t\t\t},\n\t\t\tdefValue:      false,\n\t\t\texpectedErr:   nil,\n\t\t\texpectedValue: true,\n\t\t},\n\t\t{\n\t\t\tname: \"single false\",\n\t\t\targs: map[string][]string{\n\t\t\t\t\"dangling\": {\"false\"},\n\t\t\t},\n\t\t\tdefValue:      true,\n\t\t\texpectedErr:   nil,\n\t\t\texpectedValue: false,\n\t\t},\n\t\t{\n\t\t\tname: \"single bad value\",\n\t\t\targs: map[string][]string{\n\t\t\t\t\"dangling\": {\"potato\"},\n\t\t\t},\n\t\t\tdefValue:      true,\n\t\t\texpectedErr:   &invalidFilter{Filter: \"dangling\", Value: []string{\"potato\"}},\n\t\t\texpectedValue: true,\n\t\t},\n\t\t{\n\t\t\tname: \"two bad values\",\n\t\t\targs: map[string][]string{\n\t\t\t\t\"dangling\": {\"banana\", \"potato\"},\n\t\t\t},\n\t\t\tdefValue:      true,\n\t\t\texpectedErr:   &invalidFilter{Filter: \"dangling\", Value: []string{\"banana\", \"potato\"}},\n\t\t\texpectedValue: true,\n\t\t},\n\t\t{\n\t\t\tname: \"two conflicting values\",\n\t\t\targs: map[string][]string{\n\t\t\t\t\"dangling\": {\"false\", \"true\"},\n\t\t\t},\n\t\t\tdefValue:      false,\n\t\t\texpectedErr:   &invalidFilter{Filter: \"dangling\", Value: []string{\"false\", \"true\"}},\n\t\t\texpectedValue: false,\n\t\t},\n\t\t{\n\t\t\tname: \"multiple conflicting values\",\n\t\t\targs: map[string][]string{\n\t\t\t\t\"dangling\": {\"false\", \"true\", \"1\"},\n\t\t\t},\n\t\t\tdefValue:      true,\n\t\t\texpectedErr:   &invalidFilter{Filter: \"dangling\", Value: []string{\"false\", \"true\", \"1\"}},\n\t\t\texpectedValue: true,\n\t\t},\n\t\t{\n\t\t\tname: \"1 means true\",\n\t\t\targs: map[string][]string{\n\t\t\t\t\"dangling\": {\"1\"},\n\t\t\t},\n\t\t\tdefValue:      false,\n\t\t\texpectedErr:   nil,\n\t\t\texpectedValue: true,\n\t\t},\n\t\t{\n\t\t\tname: \"0 means false\",\n\t\t\targs: map[string][]string{\n\t\t\t\t\"dangling\": {\"0\"},\n\t\t\t},\n\t\t\tdefValue:      true,\n\t\t\texpectedErr:   nil,\n\t\t\texpectedValue: false,\n\t\t},\n\t} {\n\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\ta := NewArgs()\n\n\t\t\tfor key, values := range tc.args {\n\t\t\t\tfor _, value := range values {\n\t\t\t\t\ta.Add(key, value)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tvalue, err := a.GetBoolOrDefault(\"dangling\", tc.defValue)\n\n\t\t\tif tc.expectedErr == nil {\n\t\t\t\tassert.Check(t, is.Nil(err))\n\t\t\t} else {\n\t\t\t\tassert.Check(t, is.ErrorType(err, tc.expectedErr))\n\n\t\t\t\t// Check if error is the same.\n\t\t\t\texpected := tc.expectedErr.(*invalidFilter)\n\t\t\t\tactual := err.(*invalidFilter)\n\n\t\t\t\tassert.Check(t, is.Equal(expected.Filter, actual.Filter))\n\n\t\t\t\tsort.Strings(expected.Value)\n\t\t\t\tsort.Strings(actual.Value)\n\t\t\t\tassert.Check(t, is.DeepEqual(expected.Value, actual.Value))\n\n\t\t\t\twrappedErr := fmt.Errorf(\"something went wrong: %w\", err)\n\t\t\t\tassert.Check(t, is.ErrorIs(wrappedErr, err), \"Expected a wrapped error to be detected as invalidFilter\")\n\t\t\t}\n\n\t\t\tassert.Check(t, is.Equal(tc.expectedValue, value))\n\t\t})\n\t}\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: moby\nSource File Path(s): ['moby\\\\readers_test.go\\\\source_files\\\\readers.go']\nTest File Path: moby\\readers_test.go\\readers_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: moby\\readers_test.go\\source_files\\readers.go\npackage ioutils // import \"github.com/docker/docker/pkg/ioutils\"\n\nimport (\n\t\"context\"\n\t\"io\"\n\t\"runtime/debug\"\n\t\"sync/atomic\"\n\n\t\"github.com/containerd/log\"\n)\n\n// readCloserWrapper wraps an io.Reader, and implements an io.ReadCloser\n// It calls the given callback function when closed. It should be constructed\n// with NewReadCloserWrapper\ntype readCloserWrapper struct {\n\tio.Reader\n\tcloser func() error\n\tclosed atomic.Bool\n}\n\n// Close calls back the passed closer function\nfunc (r *readCloserWrapper) Close() error {\n\tif !r.closed.CompareAndSwap(false, true) {\n\t\tsubsequentCloseWarn(\"ReadCloserWrapper\")\n\t\treturn nil\n\t}\n\treturn r.closer()\n}\n\n// NewReadCloserWrapper wraps an io.Reader, and implements an io.ReadCloser.\n// It calls the given callback function when closed.\nfunc NewReadCloserWrapper(r io.Reader, closer func() error) io.ReadCloser {\n\treturn &readCloserWrapper{\n\t\tReader: r,\n\t\tcloser: closer,\n\t}\n}\n\n// cancelReadCloser wraps an io.ReadCloser with a context for cancelling read\n// operations.\ntype cancelReadCloser struct {\n\tcancel func()\n\tpR     *io.PipeReader // Stream to read from\n\tpW     *io.PipeWriter\n\tclosed atomic.Bool\n}\n\n// NewCancelReadCloser creates a wrapper that closes the ReadCloser when the\n// context is cancelled. The returned io.ReadCloser must be closed when it is\n// no longer needed.\nfunc NewCancelReadCloser(ctx context.Context, in io.ReadCloser) io.ReadCloser {\n\tpR, pW := io.Pipe()\n\n\t// Create a context used to signal when the pipe is closed\n\tdoneCtx, cancel := context.WithCancel(context.Background())\n\n\tp := &cancelReadCloser{\n\t\tcancel: cancel,\n\t\tpR:     pR,\n\t\tpW:     pW,\n\t}\n\n\tgo func() {\n\t\t_, err := io.Copy(pW, in)\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\t// If the context was closed, p.closeWithError\n\t\t\t// was already called. Calling it again would\n\t\t\t// change the error that Read returns.\n\t\tdefault:\n\t\t\tp.closeWithError(err)\n\t\t}\n\t\tin.Close()\n\t}()\n\tgo func() {\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\tp.closeWithError(ctx.Err())\n\t\t\tcase <-doneCtx.Done():\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n\n\treturn p\n}\n\n// Read wraps the Read method of the pipe that provides data from the wrapped\n// ReadCloser.\nfunc (p *cancelReadCloser) Read(buf []byte) (n int, err error) {\n\treturn p.pR.Read(buf)\n}\n\n// closeWithError closes the wrapper and its underlying reader. It will\n// cause future calls to Read to return err.\nfunc (p *cancelReadCloser) closeWithError(err error) {\n\tp.pW.CloseWithError(err)\n\tp.cancel()\n}\n\n// Close closes the wrapper its underlying reader. It will cause\n// future calls to Read to return io.EOF.\nfunc (p *cancelReadCloser) Close() error {\n\tif !p.closed.CompareAndSwap(false, true) {\n\t\tsubsequentCloseWarn(\"cancelReadCloser\")\n\t\treturn nil\n\t}\n\tp.closeWithError(io.EOF)\n\treturn nil\n}\n\nfunc subsequentCloseWarn(name string) {\n\tlog.G(context.TODO()).Error(\"subsequent attempt to close \" + name)\n\tif log.GetLevel() >= log.DebugLevel {\n\t\tlog.G(context.TODO()).Errorf(\"stack trace: %s\", string(debug.Stack()))\n\t}\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:51:53"}, {"role": "assistant", "content": "```go\npackage ioutils // import \"github.com/docker/docker/pkg/ioutils\"\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"io\"\n\t\"strings\"\n\t\"testing\"\n\t\"time\"\n)\n\nfunc TestReadCloserWrapperClose(t *testing.T) {\n\tconst text = \"hello world\"\n\ttestErr := errors.New(\"this will be called when closing\")\n\twrapper := NewReadCloserWrapper(strings.NewReader(text), func() error {\n\t\treturn testErr\n\t})\n\n\tbuf, err := io.ReadAll(wrapper)\n\tif err != nil {\n\t\tt.Errorf(\"io.ReadAll(wrapper) err = %v\", err)\n\t}\n\tif string(buf) != text {\n\t\tt.Errorf(\"expected %v, got: %v\", text, string(buf))\n\t}\n\terr = wrapper.Close()\n\tif !errors.Is(err, testErr) {\n\t\t// readCloserWrapper should have called the anonymous func and thus, fail\n\t\tt.Errorf(\"expected %v, got: %v\", testErr, err)\n\t}\n}\n\ntype perpetualReader struct{}\n\nfunc (p *perpetualReader) Read(buf []byte) (n int, err error) {\n\tfor i := 0; i != len(buf); i++ {\n\t\tbuf[i] = 'a'\n\t}\n\treturn len(buf), nil\n}\n\nfunc TestCancelReadCloser(t *testing.T) {\n\tctx, cancel := context.WithTimeout(context.Background(), 100*time.Millisecond)\n\tdefer cancel()\n\tcrc := NewCancelReadCloser(ctx, io.NopCloser(&perpetualReader{}))\n\tfor {\n\t\tvar buf [128]byte\n\t\t_, err := crc.Read(buf[:])\n\t\tif err == context.DeadlineExceeded {\n\t\t\tbreak\n\t\t} else if err != nil {\n\t\t\tt.Fatalf(\"got unexpected error: %v\", err)\n\t\t}\n\t}\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: moby\nSource File Path(s): ['moby\\\\registry_test.go\\\\source_files\\\\registry.go']\nTest File Path: moby\\registry_test.go\\registry_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: moby\\registry_test.go\\source_files\\registry.go\npackage registry // import \"github.com/docker/docker/api/types/registry\"\n\nimport (\n\t\"encoding/json\"\n\t\"net\"\n\n\tocispec \"github.com/opencontainers/image-spec/specs-go/v1\"\n)\n\n// ServiceConfig stores daemon registry services configuration.\ntype ServiceConfig struct {\n\tAllowNondistributableArtifactsCIDRs     []*NetIPNet `json:\"AllowNondistributableArtifactsCIDRs,omitempty\"`     // Deprecated: non-distributable artifacts are deprecated and enabled by default. This field will be removed in the next release.\n\tAllowNondistributableArtifactsHostnames []string    `json:\"AllowNondistributableArtifactsHostnames,omitempty\"` // Deprecated: non-distributable artifacts are deprecated and enabled by default. This field will be removed in the next release.\n\n\tInsecureRegistryCIDRs []*NetIPNet           `json:\"InsecureRegistryCIDRs\"`\n\tIndexConfigs          map[string]*IndexInfo `json:\"IndexConfigs\"`\n\tMirrors               []string\n}\n\n// MarshalJSON implements a custom marshaler to include legacy fields\n// in API responses.\nfunc (sc ServiceConfig) MarshalJSON() ([]byte, error) {\n\ttmp := map[string]interface{}{\n\t\t\"InsecureRegistryCIDRs\": sc.InsecureRegistryCIDRs,\n\t\t\"IndexConfigs\":          sc.IndexConfigs,\n\t\t\"Mirrors\":               sc.Mirrors,\n\t}\n\tif sc.AllowNondistributableArtifactsCIDRs != nil {\n\t\ttmp[\"AllowNondistributableArtifactsCIDRs\"] = nil\n\t}\n\tif sc.AllowNondistributableArtifactsHostnames != nil {\n\t\ttmp[\"AllowNondistributableArtifactsHostnames\"] = nil\n\t}\n\treturn json.Marshal(tmp)\n}\n\n// NetIPNet is the net.IPNet type, which can be marshalled and\n// unmarshalled to JSON\ntype NetIPNet net.IPNet\n\n// String returns the CIDR notation of ipnet\nfunc (ipnet *NetIPNet) String() string {\n\treturn (*net.IPNet)(ipnet).String()\n}\n\n// MarshalJSON returns the JSON representation of the IPNet\nfunc (ipnet *NetIPNet) MarshalJSON() ([]byte, error) {\n\treturn json.Marshal((*net.IPNet)(ipnet).String())\n}\n\n// UnmarshalJSON sets the IPNet from a byte array of JSON\nfunc (ipnet *NetIPNet) UnmarshalJSON(b []byte) (err error) {\n\tvar ipnetStr string\n\tif err = json.Unmarshal(b, &ipnetStr); err == nil {\n\t\tvar cidr *net.IPNet\n\t\tif _, cidr, err = net.ParseCIDR(ipnetStr); err == nil {\n\t\t\t*ipnet = NetIPNet(*cidr)\n\t\t}\n\t}\n\treturn\n}\n\n// IndexInfo contains information about a registry\n//\n// RepositoryInfo Examples:\n//\n//\t{\n//\t  \"Index\" : {\n//\t    \"Name\" : \"docker.io\",\n//\t    \"Mirrors\" : [\"https://registry-2.docker.io/v1/\", \"https://registry-3.docker.io/v1/\"],\n//\t    \"Secure\" : true,\n//\t    \"Official\" : true,\n//\t  },\n//\t  \"RemoteName\" : \"library/debian\",\n//\t  \"LocalName\" : \"debian\",\n//\t  \"CanonicalName\" : \"docker.io/debian\"\n//\t  \"Official\" : true,\n//\t}\n//\n//\t{\n//\t  \"Index\" : {\n//\t    \"Name\" : \"127.0.0.1:5000\",\n//\t    \"Mirrors\" : [],\n//\t    \"Secure\" : false,\n//\t    \"Official\" : false,\n//\t  },\n//\t  \"RemoteName\" : \"user/repo\",\n//\t  \"LocalName\" : \"127.0.0.1:5000/user/repo\",\n//\t  \"CanonicalName\" : \"127.0.0.1:5000/user/repo\",\n//\t  \"Official\" : false,\n//\t}\ntype IndexInfo struct {\n\t// Name is the name of the registry, such as \"docker.io\"\n\tName string\n\t// Mirrors is a list of mirrors, expressed as URIs\n\tMirrors []string\n\t// Secure is set to false if the registry is part of the list of\n\t// insecure registries. Insecure registries accept HTTP and/or accept\n\t// HTTPS with certificates from unknown CAs.\n\tSecure bool\n\t// Official indicates whether this is an official registry\n\tOfficial bool\n}\n\n// DistributionInspect describes the result obtained from contacting the\n// registry to retrieve image metadata\ntype DistributionInspect struct {\n\t// Descriptor contains information about the manifest, including\n\t// the content addressable digest\n\tDescriptor ocispec.Descriptor\n\t// Platforms contains the list of platforms supported by the image,\n\t// obtained by parsing the manifest\n\tPlatforms []ocispec.Platform\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:51:53"}, {"role": "assistant", "content": "```go\npackage registry\n\nimport (\n\t\"encoding/json\"\n\t\"testing\"\n\n\t\"gotest.tools/v3/assert\"\n\tis \"gotest.tools/v3/assert/cmp\"\n)\n\nfunc TestServiceConfigMarshalLegacyFields(t *testing.T) {\n\tt.Run(\"without legacy fields\", func(t *testing.T) {\n\t\tb, err := json.Marshal(&ServiceConfig{})\n\t\tassert.NilError(t, err)\n\t\tconst expected = `{\"IndexConfigs\":null,\"InsecureRegistryCIDRs\":null,\"Mirrors\":null}`\n\t\tassert.Check(t, is.Equal(string(b), expected), \"Legacy nondistributable-artifacts fields should be omitted in output\")\n\t})\n\n\t// Legacy fields should be returned when set to an empty slice. This is\n\t// used for API versions < 1.49.\n\tt.Run(\"with legacy fields\", func(t *testing.T) {\n\t\tb, err := json.Marshal(&ServiceConfig{\n\t\t\tAllowNondistributableArtifactsCIDRs:     []*NetIPNet{},\n\t\t\tAllowNondistributableArtifactsHostnames: []string{},\n\t\t})\n\t\tassert.NilError(t, err)\n\t\tconst expected = `{\"AllowNondistributableArtifactsCIDRs\":null,\"AllowNondistributableArtifactsHostnames\":null,\"IndexConfigs\":null,\"InsecureRegistryCIDRs\":null,\"Mirrors\":null}`\n\t\tassert.Check(t, is.Equal(string(b), expected))\n\t})\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: moby\nSource File Path(s): ['moby\\\\writers_test.go\\\\source_files\\\\writers.go']\nTest File Path: moby\\writers_test.go\\writers_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: moby\\writers_test.go\\source_files\\writers.go\npackage ioutils // import \"github.com/docker/docker/pkg/ioutils\"\n\nimport (\n\t\"io\"\n\t\"sync/atomic\"\n)\n\ntype writeCloserWrapper struct {\n\tio.Writer\n\tcloser func() error\n\tclosed atomic.Bool\n}\n\nfunc (r *writeCloserWrapper) Close() error {\n\tif !r.closed.CompareAndSwap(false, true) {\n\t\tsubsequentCloseWarn(\"WriteCloserWrapper\")\n\t\treturn nil\n\t}\n\treturn r.closer()\n}\n\n// NewWriteCloserWrapper returns a new io.WriteCloser.\nfunc NewWriteCloserWrapper(r io.Writer, closer func() error) io.WriteCloser {\n\treturn &writeCloserWrapper{\n\t\tWriter: r,\n\t\tcloser: closer,\n\t}\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:51:53"}, {"role": "assistant", "content": "```go\npackage ioutils // import \"github.com/docker/docker/pkg/ioutils\"\n\nimport (\n\t\"bytes\"\n\t\"testing\"\n)\n\nfunc TestWriteCloserWrapperClose(t *testing.T) {\n\tcalled := false\n\twriter := bytes.NewBuffer([]byte{})\n\twrapper := NewWriteCloserWrapper(writer, func() error {\n\t\tcalled = true\n\t\treturn nil\n\t})\n\tif err := wrapper.Close(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif !called {\n\t\tt.Fatalf(\"writeCloserWrapper should have call the anonymous function.\")\n\t}\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: pocketbase\nSource File Path(s): ['pocketbase\\\\auth_origin_query_test.go\\\\source_files\\\\auth_origin_query.go']\nTest File Path: pocketbase\\auth_origin_query_test.go\\auth_origin_query_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: pocketbase\\auth_origin_query_test.go\\source_files\\auth_origin_query.go\npackage core\n\nimport (\n\t\"errors\"\n\n\t\"github.com/pocketbase/dbx\"\n)\n\n// FindAllAuthOriginsByRecord returns all AuthOrigin models linked to the provided auth record (in DESC order).\nfunc (app *BaseApp) FindAllAuthOriginsByRecord(authRecord *Record) ([]*AuthOrigin, error) {\n\tresult := []*AuthOrigin{}\n\n\terr := app.RecordQuery(CollectionNameAuthOrigins).\n\t\tAndWhere(dbx.HashExp{\n\t\t\t\"collectionRef\": authRecord.Collection().Id,\n\t\t\t\"recordRef\":     authRecord.Id,\n\t\t}).\n\t\tOrderBy(\"created DESC\").\n\t\tAll(&result)\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn result, nil\n}\n\n// FindAllAuthOriginsByCollection returns all AuthOrigin models linked to the provided collection (in DESC order).\nfunc (app *BaseApp) FindAllAuthOriginsByCollection(collection *Collection) ([]*AuthOrigin, error) {\n\tresult := []*AuthOrigin{}\n\n\terr := app.RecordQuery(CollectionNameAuthOrigins).\n\t\tAndWhere(dbx.HashExp{\"collectionRef\": collection.Id}).\n\t\tOrderBy(\"created DESC\").\n\t\tAll(&result)\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn result, nil\n}\n\n// FindAuthOriginById returns a single AuthOrigin model by its id.\nfunc (app *BaseApp) FindAuthOriginById(id string) (*AuthOrigin, error) {\n\tresult := &AuthOrigin{}\n\n\terr := app.RecordQuery(CollectionNameAuthOrigins).\n\t\tAndWhere(dbx.HashExp{\"id\": id}).\n\t\tLimit(1).\n\t\tOne(result)\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn result, nil\n}\n\n// FindAuthOriginByRecordAndFingerprint returns a single AuthOrigin model\n// by its authRecord relation and fingerprint.\nfunc (app *BaseApp) FindAuthOriginByRecordAndFingerprint(authRecord *Record, fingerprint string) (*AuthOrigin, error) {\n\tresult := &AuthOrigin{}\n\n\terr := app.RecordQuery(CollectionNameAuthOrigins).\n\t\tAndWhere(dbx.HashExp{\n\t\t\t\"collectionRef\": authRecord.Collection().Id,\n\t\t\t\"recordRef\":     authRecord.Id,\n\t\t\t\"fingerprint\":   fingerprint,\n\t\t}).\n\t\tLimit(1).\n\t\tOne(result)\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn result, nil\n}\n\n// DeleteAllAuthOriginsByRecord deletes all AuthOrigin models associated with the provided record.\n//\n// Returns a combined error with the failed deletes.\nfunc (app *BaseApp) DeleteAllAuthOriginsByRecord(authRecord *Record) error {\n\tmodels, err := app.FindAllAuthOriginsByRecord(authRecord)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tvar errs []error\n\tfor _, m := range models {\n\t\tif err := app.Delete(m); err != nil {\n\t\t\terrs = append(errs, err)\n\t\t}\n\t}\n\tif len(errs) > 0 {\n\t\treturn errors.Join(errs...)\n\t}\n\n\treturn nil\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:52:19"}, {"role": "assistant", "content": "```go\npackage core_test\n\nimport (\n\t\"fmt\"\n\t\"slices\"\n\t\"testing\"\n\n\t\"github.com/pocketbase/pocketbase/core\"\n\t\"github.com/pocketbase/pocketbase/tests\"\n)\n\nfunc TestFindAllAuthOriginsByRecord(t *testing.T) {\n\tt.Parallel()\n\n\tapp, _ := tests.NewTestApp()\n\tdefer app.Cleanup()\n\n\tdemo1, err := app.FindRecordById(\"demo1\", \"84nmscqy84lsi1t\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tsuperuser2, err := app.FindAuthRecordByEmail(core.CollectionNameSuperusers, \"test2@example.com\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tsuperuser4, err := app.FindAuthRecordByEmail(core.CollectionNameSuperusers, \"test4@example.com\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tclient1, err := app.FindAuthRecordByEmail(\"clients\", \"test@example.com\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tscenarios := []struct {\n\t\trecord   *core.Record\n\t\texpected []string\n\t}{\n\t\t{demo1, nil},\n\t\t{superuser2, []string{\"5798yh833k6w6w0\", \"ic55o70g4f8pcl4\", \"dmy260k6ksjr4ib\"}},\n\t\t{superuser4, nil},\n\t\t{client1, []string{\"9r2j0m74260ur8i\"}},\n\t}\n\n\tfor _, s := range scenarios {\n\t\tt.Run(s.record.Collection().Name+\"_\"+s.record.Id, func(t *testing.T) {\n\t\t\tresult, err := app.FindAllAuthOriginsByRecord(s.record)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\tif len(result) != len(s.expected) {\n\t\t\t\tt.Fatalf(\"Expected total origins %d, got %d\", len(s.expected), len(result))\n\t\t\t}\n\n\t\t\tfor i, id := range s.expected {\n\t\t\t\tif result[i].Id != id {\n\t\t\t\t\tt.Errorf(\"[%d] Expected id %q, got %q\", i, id, result[i].Id)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestFindAllAuthOriginsByCollection(t *testing.T) {\n\tt.Parallel()\n\n\tapp, _ := tests.NewTestApp()\n\tdefer app.Cleanup()\n\n\tdemo1, err := app.FindCollectionByNameOrId(\"demo1\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tsuperusers, err := app.FindCollectionByNameOrId(core.CollectionNameSuperusers)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tclients, err := app.FindCollectionByNameOrId(\"clients\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tscenarios := []struct {\n\t\tcollection *core.Collection\n\t\texpected   []string\n\t}{\n\t\t{demo1, nil},\n\t\t{superusers, []string{\"5798yh833k6w6w0\", \"ic55o70g4f8pcl4\", \"dmy260k6ksjr4ib\", \"5f29jy38bf5zm3f\"}},\n\t\t{clients, []string{\"9r2j0m74260ur8i\"}},\n\t}\n\n\tfor _, s := range scenarios {\n\t\tt.Run(s.collection.Name, func(t *testing.T) {\n\t\t\tresult, err := app.FindAllAuthOriginsByCollection(s.collection)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\tif len(result) != len(s.expected) {\n\t\t\t\tt.Fatalf(\"Expected total origins %d, got %d\", len(s.expected), len(result))\n\t\t\t}\n\n\t\t\tfor i, id := range s.expected {\n\t\t\t\tif result[i].Id != id {\n\t\t\t\t\tt.Errorf(\"[%d] Expected id %q, got %q\", i, id, result[i].Id)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestFindAuthOriginById(t *testing.T) {\n\tt.Parallel()\n\n\tapp, _ := tests.NewTestApp()\n\tdefer app.Cleanup()\n\n\tscenarios := []struct {\n\t\tid          string\n\t\texpectError bool\n\t}{\n\t\t{\"\", true},\n\t\t{\"84nmscqy84lsi1t\", true}, // non-origin id\n\t\t{\"9r2j0m74260ur8i\", false},\n\t}\n\n\tfor _, s := range scenarios {\n\t\tt.Run(s.id, func(t *testing.T) {\n\t\t\tresult, err := app.FindAuthOriginById(s.id)\n\n\t\t\thasErr := err != nil\n\t\t\tif hasErr != s.expectError {\n\t\t\t\tt.Fatalf(\"Expected hasErr %v, got %v (%v)\", s.expectError, hasErr, err)\n\t\t\t}\n\n\t\t\tif hasErr {\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tif result.Id != s.id {\n\t\t\t\tt.Fatalf(\"Expected record with id %q, got %q\", s.id, result.Id)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestFindAuthOriginByRecordAndFingerprint(t *testing.T) {\n\tt.Parallel()\n\n\tapp, _ := tests.NewTestApp()\n\tdefer app.Cleanup()\n\n\tdemo1, err := app.FindRecordById(\"demo1\", \"84nmscqy84lsi1t\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tsuperuser2, err := app.FindAuthRecordByEmail(core.CollectionNameSuperusers, \"test2@example.com\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tscenarios := []struct {\n\t\trecord      *core.Record\n\t\tfingerprint string\n\t\texpectError bool\n\t}{\n\t\t{demo1, \"6afbfe481c31c08c55a746cccb88ece0\", true},\n\t\t{superuser2, \"\", true},\n\t\t{superuser2, \"abc\", true},\n\t\t{superuser2, \"22bbbcbed36e25321f384ccf99f60057\", false}, // fingerprint from different origin\n\t\t{superuser2, \"6afbfe481c31c08c55a746cccb88ece0\", false},\n\t}\n\n\tfor i, s := range scenarios {\n\t\tt.Run(fmt.Sprintf(\"%d_%s_%s\", i, s.record.Id, s.fingerprint), func(t *testing.T) {\n\t\t\tresult, err := app.FindAuthOriginByRecordAndFingerprint(s.record, s.fingerprint)\n\n\t\t\thasErr := err != nil\n\t\t\tif hasErr != s.expectError {\n\t\t\t\tt.Fatalf(\"Expected hasErr %v, got %v (%v)\", s.expectError, hasErr, err)\n\t\t\t}\n\n\t\t\tif hasErr {\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\tif result.Fingerprint() != s.fingerprint {\n\t\t\t\tt.Fatalf(\"Expected origin with fingerprint %q, got %q\", s.fingerprint, result.Fingerprint())\n\t\t\t}\n\n\t\t\tif result.RecordRef() != s.record.Id || result.CollectionRef() != s.record.Collection().Id {\n\t\t\t\tt.Fatalf(\"Expected record %q (%q), got %q (%q)\", s.record.Id, s.record.Collection().Id, result.RecordRef(), result.CollectionRef())\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestDeleteAllAuthOriginsByRecord(t *testing.T) {\n\tt.Parallel()\n\n\ttestApp, _ := tests.NewTestApp()\n\tdefer testApp.Cleanup()\n\n\tdemo1, err := testApp.FindRecordById(\"demo1\", \"84nmscqy84lsi1t\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tsuperuser2, err := testApp.FindAuthRecordByEmail(core.CollectionNameSuperusers, \"test2@example.com\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tsuperuser4, err := testApp.FindAuthRecordByEmail(core.CollectionNameSuperusers, \"test4@example.com\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tclient1, err := testApp.FindAuthRecordByEmail(\"clients\", \"test@example.com\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tscenarios := []struct {\n\t\trecord     *core.Record\n\t\tdeletedIds []string\n\t}{\n\t\t{demo1, nil}, // non-auth record\n\t\t{superuser2, []string{\"5798yh833k6w6w0\", \"ic55o70g4f8pcl4\", \"dmy260k6ksjr4ib\"}},\n\t\t{superuser4, nil},\n\t\t{client1, []string{\"9r2j0m74260ur8i\"}},\n\t}\n\n\tfor i, s := range scenarios {\n\t\tt.Run(fmt.Sprintf(\"%d_%s_%s\", i, s.record.Collection().Name, s.record.Id), func(t *testing.T) {\n\t\t\tapp, _ := tests.NewTestApp()\n\t\t\tdefer app.Cleanup()\n\n\t\t\tdeletedIds := []string{}\n\t\t\tapp.OnRecordDelete().BindFunc(func(e *core.RecordEvent) error {\n\t\t\t\tdeletedIds = append(deletedIds, e.Record.Id)\n\t\t\t\treturn e.Next()\n\t\t\t})\n\n\t\t\terr := app.DeleteAllAuthOriginsByRecord(s.record)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\tif len(deletedIds) != len(s.deletedIds) {\n\t\t\t\tt.Fatalf(\"Expected deleted ids\\n%v\\ngot\\n%v\", s.deletedIds, deletedIds)\n\t\t\t}\n\n\t\t\tfor _, id := range s.deletedIds {\n\t\t\t\tif !slices.Contains(deletedIds, id) {\n\t\t\t\t\tt.Errorf(\"Expected to find deleted id %q in %v\", id, deletedIds)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: pocketbase\nSource File Path(s): ['pocketbase\\\\collection_import_test.go\\\\source_files\\\\collection_import.go']\nTest File Path: pocketbase\\collection_import_test.go\\collection_import_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: pocketbase\\collection_import_test.go\\source_files\\collection_import.go\npackage core\n\nimport (\n\t\"cmp\"\n\t\"context\"\n\t\"database/sql\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"slices\"\n\n\tvalidation \"github.com/go-ozzo/ozzo-validation/v4\"\n\t\"github.com/spf13/cast\"\n)\n\n// ImportCollectionsByMarshaledJSON is the same as [ImportCollections]\n// but accept marshaled json array as import data (usually used for the autogenerated snapshots).\nfunc (app *BaseApp) ImportCollectionsByMarshaledJSON(rawSliceOfMaps []byte, deleteMissing bool) error {\n\tdata := []map[string]any{}\n\n\terr := json.Unmarshal(rawSliceOfMaps, &data)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn app.ImportCollections(data, deleteMissing)\n}\n\n// ImportCollections imports the provided collections data in a single transaction.\n//\n// For existing matching collections, the imported data is unmarshaled on top of the existing model.\n//\n// NB! If deleteMissing is true, ALL NON-SYSTEM COLLECTIONS AND SCHEMA FIELDS,\n// that are not present in the imported configuration, WILL BE DELETED\n// (this includes their related records data).\nfunc (app *BaseApp) ImportCollections(toImport []map[string]any, deleteMissing bool) error {\n\tif len(toImport) == 0 {\n\t\t// prevent accidentally deleting all collections\n\t\treturn errors.New(\"no collections to import\")\n\t}\n\n\timportedCollections := make([]*Collection, len(toImport))\n\tmappedImported := make(map[string]*Collection, len(toImport))\n\n\t// normalize imported collections data to ensure that all\n\t// collection fields are present and properly initialized\n\tfor i, data := range toImport {\n\t\tvar imported *Collection\n\n\t\tidentifier := cast.ToString(data[\"id\"])\n\t\tif identifier == \"\" {\n\t\t\tidentifier = cast.ToString(data[\"name\"])\n\t\t}\n\n\t\texisting, err := app.FindCollectionByNameOrId(identifier)\n\t\tif err != nil && !errors.Is(err, sql.ErrNoRows) {\n\t\t\treturn err\n\t\t}\n\n\t\tif existing != nil {\n\t\t\t// refetch for deep copy\n\t\t\timported, err = app.FindCollectionByNameOrId(existing.Id)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\t// ensure that the fields will be cleared\n\t\t\tif data[\"fields\"] == nil && deleteMissing {\n\t\t\t\tdata[\"fields\"] = []map[string]any{}\n\t\t\t}\n\n\t\t\trawData, err := json.Marshal(data)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\t// load the imported data\n\t\t\terr = json.Unmarshal(rawData, imported)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\t// extend with the existing fields if necessary\n\t\t\tfor _, f := range existing.Fields {\n\t\t\t\tif !f.GetSystem() && deleteMissing {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif imported.Fields.GetById(f.GetId()) == nil {\n\t\t\t\t\t// replace with the existing id to prevent accidental column deletion\n\t\t\t\t\t// since otherwise the imported field will be treated as a new one\n\t\t\t\t\tfound := imported.Fields.GetByName(f.GetName())\n\t\t\t\t\tif found != nil && found.Type() == f.Type() {\n\t\t\t\t\t\tfound.SetId(f.GetId())\n\t\t\t\t\t}\n\t\t\t\t\timported.Fields.Add(f)\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\timported = &Collection{}\n\n\t\t\trawData, err := json.Marshal(data)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\t// load the imported data\n\t\t\terr = json.Unmarshal(rawData, imported)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\n\t\timported.IntegrityChecks(false)\n\n\t\timportedCollections[i] = imported\n\t\tmappedImported[imported.Id] = imported\n\t}\n\n\t// reorder views last since the view query could depend on some of the other collections\n\tslices.SortStableFunc(importedCollections, func(a, b *Collection) int {\n\t\tcmpA := -1\n\t\tif a.IsView() {\n\t\t\tcmpA = 1\n\t\t}\n\n\t\tcmpB := -1\n\t\tif b.IsView() {\n\t\t\tcmpB = 1\n\t\t}\n\n\t\tres := cmp.Compare(cmpA, cmpB)\n\t\tif res == 0 {\n\t\t\tres = a.Created.Compare(b.Created)\n\t\t\tif res == 0 {\n\t\t\t\tres = a.Updated.Compare(b.Updated)\n\t\t\t}\n\t\t}\n\t\treturn res\n\t})\n\n\treturn app.RunInTransaction(func(txApp App) error {\n\t\texistingCollections := []*Collection{}\n\t\tif err := txApp.CollectionQuery().OrderBy(\"updated ASC\").All(&existingCollections); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tmappedExisting := make(map[string]*Collection, len(existingCollections))\n\t\tfor _, existing := range existingCollections {\n\t\t\texisting.IntegrityChecks(false)\n\t\t\tmappedExisting[existing.Id] = existing\n\t\t}\n\n\t\t// delete old collections not available in the new configuration\n\t\t// (before saving the imports in case a deleted collection name is being reused)\n\t\tif deleteMissing {\n\t\t\tfor _, existing := range existingCollections {\n\t\t\t\tif mappedImported[existing.Id] != nil || existing.System {\n\t\t\t\t\tcontinue // exist or system\n\t\t\t\t}\n\n\t\t\t\t// delete collection\n\t\t\t\tif err := txApp.Delete(existing); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// upsert imported collections\n\t\tfor _, imported := range importedCollections {\n\t\t\tif err := txApp.SaveNoValidate(imported); err != nil {\n\t\t\t\treturn fmt.Errorf(\"failed to save collection %q: %w\", imported.Name, err)\n\t\t\t}\n\t\t}\n\n\t\t// run validations\n\t\tfor _, imported := range importedCollections {\n\t\t\toriginal := mappedExisting[imported.Id]\n\t\t\tif original == nil {\n\t\t\t\toriginal = imported\n\t\t\t}\n\n\t\t\tvalidator := newCollectionValidator(\n\t\t\t\tcontext.Background(),\n\t\t\t\ttxApp,\n\t\t\t\timported,\n\t\t\t\toriginal,\n\t\t\t)\n\t\t\tif err := validator.run(); err != nil {\n\t\t\t\t// serialize the validation error(s)\n\t\t\t\tserializedErr, _ := json.MarshalIndent(err, \"\", \"  \")\n\n\t\t\t\treturn validation.Errors{\"collections\": validation.NewError(\n\t\t\t\t\t\"validation_collections_import_failure\",\n\t\t\t\t\tfmt.Sprintf(\"Data validations failed for collection %q (%s):\\n%s\", imported.Name, imported.Id, serializedErr),\n\t\t\t\t)}\n\t\t\t}\n\t\t}\n\n\t\treturn nil\n\t})\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:52:19"}, {"role": "assistant", "content": "```go\npackage core_test\n\nimport (\n\t\"encoding/json\"\n\t\"strings\"\n\t\"testing\"\n\n\t\"github.com/pocketbase/dbx\"\n\t\"github.com/pocketbase/pocketbase/core\"\n\t\"github.com/pocketbase/pocketbase/tests\"\n)\n\nfunc TestImportCollections(t *testing.T) {\n\tt.Parallel()\n\n\ttestApp, _ := tests.NewTestApp()\n\tdefer testApp.Cleanup()\n\n\tvar regularCollections []*core.Collection\n\terr := testApp.CollectionQuery().AndWhere(dbx.HashExp{\"system\": false}).All(&regularCollections)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tvar systemCollections []*core.Collection\n\terr = testApp.CollectionQuery().AndWhere(dbx.HashExp{\"system\": true}).All(&systemCollections)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttotalRegularCollections := len(regularCollections)\n\ttotalSystemCollections := len(systemCollections)\n\ttotalCollections := totalRegularCollections + totalSystemCollections\n\n\tscenarios := []struct {\n\t\tname                   string\n\t\tdata                   []map[string]any\n\t\tdeleteMissing          bool\n\t\texpectError            bool\n\t\texpectCollectionsCount int\n\t\tafterTestFunc          func(testApp *tests.TestApp, resultCollections []*core.Collection)\n\t}{\n\t\t{\n\t\t\tname:                   \"empty collections\",\n\t\t\tdata:                   []map[string]any{},\n\t\t\texpectError:            true,\n\t\t\texpectCollectionsCount: totalCollections,\n\t\t},\n\t\t{\n\t\t\tname: \"minimal collection import (with missing system fields)\",\n\t\t\tdata: []map[string]any{\n\t\t\t\t{\"name\": \"import_test1\", \"type\": \"auth\"},\n\t\t\t\t{\n\t\t\t\t\t\"name\": \"import_test2\", \"fields\": []map[string]any{\n\t\t\t\t\t\t{\"name\": \"test\", \"type\": \"text\"},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\tdeleteMissing:          false,\n\t\t\texpectError:            false,\n\t\t\texpectCollectionsCount: totalCollections + 2,\n\t\t},\n\t\t{\n\t\t\tname: \"minimal collection import (trigger collection model validations)\",\n\t\t\tdata: []map[string]any{\n\t\t\t\t{\"name\": \"\"},\n\t\t\t\t{\n\t\t\t\t\t\"name\": \"import_test2\", \"fields\": []map[string]any{\n\t\t\t\t\t\t{\"name\": \"test\", \"type\": \"text\"},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\tdeleteMissing:          false,\n\t\t\texpectError:            true,\n\t\t\texpectCollectionsCount: totalCollections,\n\t\t},\n\t\t{\n\t\t\tname: \"minimal collection import (trigger field settings validation)\",\n\t\t\tdata: []map[string]any{\n\t\t\t\t{\"name\": \"import_test\", \"fields\": []map[string]any{{\"name\": \"test\", \"type\": \"text\", \"min\": -1}}},\n\t\t\t},\n\t\t\tdeleteMissing:          false,\n\t\t\texpectError:            true,\n\t\t\texpectCollectionsCount: totalCollections,\n\t\t},\n\t\t{\n\t\t\tname: \"new + update + delete (system collections delete should be ignored)\",\n\t\t\tdata: []map[string]any{\n\t\t\t\t{\n\t\t\t\t\t\"id\":   \"wsmn24bux7wo113\",\n\t\t\t\t\t\"name\": \"demo\",\n\t\t\t\t\t\"fields\": []map[string]any{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"id\":       \"_2hlxbmp\",\n\t\t\t\t\t\t\t\"name\":     \"title\",\n\t\t\t\t\t\t\t\"type\":     \"text\",\n\t\t\t\t\t\t\t\"system\":   false,\n\t\t\t\t\t\t\t\"required\": true,\n\t\t\t\t\t\t\t\"min\":      3,\n\t\t\t\t\t\t\t\"max\":      nil,\n\t\t\t\t\t\t\t\"pattern\":  \"\",\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t\t\"indexes\": []string{},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"name\": \"import1\",\n\t\t\t\t\t\"fields\": []map[string]any{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"name\": \"active\",\n\t\t\t\t\t\t\t\"type\": \"bool\",\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\tdeleteMissing:          true,\n\t\t\texpectError:            false,\n\t\t\texpectCollectionsCount: totalSystemCollections + 2,\n\t\t},\n\t\t{\n\t\t\tname: \"test with deleteMissing: false\",\n\t\t\tdata: []map[string]any{\n\t\t\t\t{\n\t\t\t\t\t// \"id\":   \"wsmn24bux7wo113\", // test update with only name as identifier\n\t\t\t\t\t\"name\": \"demo1\",\n\t\t\t\t\t\"fields\": []map[string]any{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"id\":       \"_2hlxbmp\",\n\t\t\t\t\t\t\t\"name\":     \"title\",\n\t\t\t\t\t\t\t\"type\":     \"text\",\n\t\t\t\t\t\t\t\"system\":   false,\n\t\t\t\t\t\t\t\"required\": true,\n\t\t\t\t\t\t\t\"min\":      3,\n\t\t\t\t\t\t\t\"max\":      nil,\n\t\t\t\t\t\t\t\"pattern\":  \"\",\n\t\t\t\t\t\t},\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"id\":       \"_2hlxbmp\",\n\t\t\t\t\t\t\t\"name\":     \"field_with_duplicate_id\",\n\t\t\t\t\t\t\t\"type\":     \"text\",\n\t\t\t\t\t\t\t\"system\":   false,\n\t\t\t\t\t\t\t\"required\": true,\n\t\t\t\t\t\t\t\"unique\":   false,\n\t\t\t\t\t\t\t\"min\":      4,\n\t\t\t\t\t\t\t\"max\":      nil,\n\t\t\t\t\t\t\t\"pattern\":  \"\",\n\t\t\t\t\t\t},\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"id\":   \"abcd_import\",\n\t\t\t\t\t\t\t\"name\": \"new_field\",\n\t\t\t\t\t\t\t\"type\": \"text\",\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"name\": \"new_import\",\n\t\t\t\t\t\"fields\": []map[string]any{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"id\":   \"abcd_import\",\n\t\t\t\t\t\t\t\"name\": \"active\",\n\t\t\t\t\t\t\t\"type\": \"bool\",\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\tdeleteMissing:          false,\n\t\t\texpectError:            false,\n\t\t\texpectCollectionsCount: totalCollections + 1,\n\t\t\tafterTestFunc: func(testApp *tests.TestApp, resultCollections []*core.Collection) {\n\t\t\t\texpectedCollectionFields := map[string]int{\n\t\t\t\t\tcore.CollectionNameAuthOrigins: 6,\n\t\t\t\t\t\"nologin\":                      10,\n\t\t\t\t\t\"demo1\":                        18,\n\t\t\t\t\t\"demo2\":                        5,\n\t\t\t\t\t\"demo3\":                        5,\n\t\t\t\t\t\"demo4\":                        16,\n\t\t\t\t\t\"demo5\":                        9,\n\t\t\t\t\t\"new_import\":                   2,\n\t\t\t\t}\n\t\t\t\tfor name, expectedCount := range expectedCollectionFields {\n\t\t\t\t\tcollection, err := testApp.FindCollectionByNameOrId(name)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tt.Fatal(err)\n\t\t\t\t\t}\n\n\t\t\t\t\tif totalFields := len(collection.Fields); totalFields != expectedCount {\n\t\t\t\t\t\tt.Errorf(\"Expected %d %q fields, got %d\", expectedCount, collection.Name, totalFields)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t},\n\t\t},\n\t}\n\n\tfor _, s := range scenarios {\n\t\tt.Run(s.name, func(t *testing.T) {\n\t\t\ttestApp, _ := tests.NewTestApp()\n\t\t\tdefer testApp.Cleanup()\n\n\t\t\terr := testApp.ImportCollections(s.data, s.deleteMissing)\n\n\t\t\thasErr := err != nil\n\t\t\tif hasErr != s.expectError {\n\t\t\t\tt.Fatalf(\"Expected hasErr to be %v, got %v (%v)\", s.expectError, hasErr, err)\n\t\t\t}\n\n\t\t\t// check collections count\n\t\t\tcollections := []*core.Collection{}\n\t\t\tif err := testApp.CollectionQuery().All(&collections); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tif len(collections) != s.expectCollectionsCount {\n\t\t\t\tt.Fatalf(\"Expected %d collections, got %d\", s.expectCollectionsCount, len(collections))\n\t\t\t}\n\n\t\t\tif s.afterTestFunc != nil {\n\t\t\t\ts.afterTestFunc(testApp, collections)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestImportCollectionsByMarshaledJSON(t *testing.T) {\n\tt.Parallel()\n\n\ttestApp, _ := tests.NewTestApp()\n\tdefer testApp.Cleanup()\n\n\tvar regularCollections []*core.Collection\n\terr := testApp.CollectionQuery().AndWhere(dbx.HashExp{\"system\": false}).All(&regularCollections)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tvar systemCollections []*core.Collection\n\terr = testApp.CollectionQuery().AndWhere(dbx.HashExp{\"system\": true}).All(&systemCollections)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttotalRegularCollections := len(regularCollections)\n\ttotalSystemCollections := len(systemCollections)\n\ttotalCollections := totalRegularCollections + totalSystemCollections\n\n\tscenarios := []struct {\n\t\tname                   string\n\t\tdata                   string\n\t\tdeleteMissing          bool\n\t\texpectError            bool\n\t\texpectCollectionsCount int\n\t\tafterTestFunc          func(testApp *tests.TestApp, resultCollections []*core.Collection)\n\t}{\n\t\t{\n\t\t\tname:                   \"invalid json array\",\n\t\t\tdata:                   `{\"test\":123}`,\n\t\t\texpectError:            true,\n\t\t\texpectCollectionsCount: totalCollections,\n\t\t},\n\t\t{\n\t\t\tname: \"new + update + delete (system collections delete should be ignored)\",\n\t\t\tdata: `[\n\t\t\t\t{\n\t\t\t\t\t\"id\":   \"wsmn24bux7wo113\",\n\t\t\t\t\t\"name\": \"demo\",\n\t\t\t\t\t\"fields\": [\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"id\":       \"_2hlxbmp\",\n\t\t\t\t\t\t\t\"name\":     \"title\",\n\t\t\t\t\t\t\t\"type\":     \"text\",\n\t\t\t\t\t\t\t\"system\":   false,\n\t\t\t\t\t\t\t\"required\": true,\n\t\t\t\t\t\t\t\"min\":      3,\n\t\t\t\t\t\t\t\"max\":      null,\n\t\t\t\t\t\t\t\"pattern\":  \"\"\n\t\t\t\t\t\t}\n\t\t\t\t\t],\n\t\t\t\t\t\"indexes\": []\n\t\t\t\t},\n\t\t\t\t{\n\t\t\t\t\t\"name\": \"import1\",\n\t\t\t\t\t\"fields\": [\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\"name\": \"active\",\n\t\t\t\t\t\t\t\"type\": \"bool\"\n\t\t\t\t\t\t}\n\t\t\t\t\t]\n\t\t\t\t}\n\t\t\t]`,\n\t\t\tdeleteMissing:          true,\n\t\t\texpectError:            false,\n\t\t\texpectCollectionsCount: totalSystemCollections + 2,\n\t\t},\n\t}\n\n\tfor _, s := range scenarios {\n\t\tt.Run(s.name, func(t *testing.T) {\n\t\t\ttestApp, _ := tests.NewTestApp()\n\t\t\tdefer testApp.Cleanup()\n\n\t\t\terr := testApp.ImportCollectionsByMarshaledJSON([]byte(s.data), s.deleteMissing)\n\n\t\t\thasErr := err != nil\n\t\t\tif hasErr != s.expectError {\n\t\t\t\tt.Fatalf(\"Expected hasErr to be %v, got %v (%v)\", s.expectError, hasErr, err)\n\t\t\t}\n\n\t\t\t// check collections count\n\t\t\tcollections := []*core.Collection{}\n\t\t\tif err := testApp.CollectionQuery().All(&collections); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t\tif len(collections) != s.expectCollectionsCount {\n\t\t\t\tt.Fatalf(\"Expected %d collections, got %d\", s.expectCollectionsCount, len(collections))\n\t\t\t}\n\n\t\t\tif s.afterTestFunc != nil {\n\t\t\t\ts.afterTestFunc(testApp, collections)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestImportCollectionsUpdateRules(t *testing.T) {\n\tt.Parallel()\n\n\tscenarios := []struct {\n\t\tname          string\n\t\tdata          map[string]any\n\t\tdeleteMissing bool\n\t}{\n\t\t{\n\t\t\t\"extend existing by name (without deleteMissing)\",\n\t\t\tmap[string]any{\"name\": \"clients\", \"authToken\": map[string]any{\"duration\": 100}, \"fields\": []map[string]any{{\"name\": \"test\", \"type\": \"text\"}}},\n\t\t\tfalse,\n\t\t},\n\t\t{\n\t\t\t\"extend existing by id (without deleteMissing)\",\n\t\t\tmap[string]any{\"id\": \"v851q4r790rhknl\", \"authToken\": map[string]any{\"duration\": 100}, \"fields\": []map[string]any{{\"name\": \"test\", \"type\": \"text\"}}},\n\t\t\tfalse,\n\t\t},\n\t\t{\n\t\t\t\"extend with delete missing\",\n\t\t\tmap[string]any{\n\t\t\t\t\"id\":           \"v851q4r790rhknl\",\n\t\t\t\t\"authToken\":    map[string]any{\"duration\": 100},\n\t\t\t\t\"fields\":       []map[string]any{{\"name\": \"test\", \"type\": \"text\"}},\n\t\t\t\t\"passwordAuth\": map[string]any{\"identityFields\": []string{\"email\"}},\n\t\t\t\t\"indexes\": []string{\n\t\t\t\t\t// min required system fields indexes\n\t\t\t\t\t\"CREATE UNIQUE INDEX `_v851q4r790rhknl_email_idx` ON `clients` (email) WHERE email != ''\",\n\t\t\t\t\t\"CREATE UNIQUE INDEX `_v851q4r790rhknl_tokenKey_idx` ON `clients` (tokenKey)\",\n\t\t\t\t},\n\t\t\t},\n\t\t\ttrue,\n\t\t},\n\t}\n\n\tfor _, s := range scenarios {\n\t\tt.Run(s.name, func(t *testing.T) {\n\t\t\ttestApp, _ := tests.NewTestApp()\n\t\t\tdefer testApp.Cleanup()\n\n\t\t\tbeforeCollection, err := testApp.FindCollectionByNameOrId(\"clients\")\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\terr = testApp.ImportCollections([]map[string]any{s.data}, s.deleteMissing)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\tafterCollection, err := testApp.FindCollectionByNameOrId(\"clients\")\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\tif afterCollection.AuthToken.Duration != 100 {\n\t\t\t\tt.Fatalf(\"Expected AuthToken duration to be %d, got %d\", 100, afterCollection.AuthToken.Duration)\n\t\t\t}\n\t\t\tif beforeCollection.AuthToken.Secret != afterCollection.AuthToken.Secret {\n\t\t\t\tt.Fatalf(\"Expected AuthToken secrets to remain the same, got\\n%q\\nVS\\n%q\", beforeCollection.AuthToken.Secret, afterCollection.AuthToken.Secret)\n\t\t\t}\n\t\t\tif beforeCollection.Name != afterCollection.Name {\n\t\t\t\tt.Fatalf(\"Expected Name to remain the same, got\\n%q\\nVS\\n%q\", beforeCollection.Name, afterCollection.Name)\n\t\t\t}\n\t\t\tif beforeCollection.Id != afterCollection.Id {\n\t\t\t\tt.Fatalf(\"Expected Id to remain the same, got\\n%q\\nVS\\n%q\", beforeCollection.Id, afterCollection.Id)\n\t\t\t}\n\n\t\t\tif !s.deleteMissing {\n\t\t\t\ttotalExpectedFields := len(beforeCollection.Fields) + 1\n\t\t\t\tif v := len(afterCollection.Fields); v != totalExpectedFields {\n\t\t\t\t\tt.Fatalf(\"Expected %d total fields, got %d\", totalExpectedFields, v)\n\t\t\t\t}\n\n\t\t\t\tif afterCollection.Fields.GetByName(\"test\") == nil {\n\t\t\t\t\tt.Fatalf(\"Missing new field %q\", \"test\")\n\t\t\t\t}\n\n\t\t\t\t// ensure that the old fields still exist\n\t\t\t\toldFields := beforeCollection.Fields.FieldNames()\n\t\t\t\tfor _, name := range oldFields {\n\t\t\t\t\tif afterCollection.Fields.GetByName(name) == nil {\n\t\t\t\t\t\tt.Fatalf(\"Missing expected old field %q\", name)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\ttotalExpectedFields := 1\n\t\t\t\tfor _, f := range beforeCollection.Fields {\n\t\t\t\t\tif f.GetSystem() {\n\t\t\t\t\t\ttotalExpectedFields++\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tif v := len(afterCollection.Fields); v != totalExpectedFields {\n\t\t\t\t\tt.Fatalf(\"Expected %d total fields, got %d\", totalExpectedFields, v)\n\t\t\t\t}\n\n\t\t\t\tif afterCollection.Fields.GetByName(\"test\") == nil {\n\t\t\t\t\tt.Fatalf(\"Missing new field %q\", \"test\")\n\t\t\t\t}\n\n\t\t\t\t// ensure that the old system fields still exist\n\t\t\t\tfor _, f := range beforeCollection.Fields {\n\t\t\t\t\tif f.GetSystem() && afterCollection.Fields.GetByName(f.GetName()) == nil {\n\t\t\t\t\t\tt.Fatalf(\"Missing expected old field %q\", f.GetName())\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestImportCollectionsCreateRules(t *testing.T) {\n\tt.Parallel()\n\n\ttestApp, _ := tests.NewTestApp()\n\tdefer testApp.Cleanup()\n\n\terr := testApp.ImportCollections([]map[string]any{\n\t\t{\"name\": \"new_test\", \"type\": \"auth\", \"authToken\": map[string]any{\"duration\": 123}, \"fields\": []map[string]any{{\"name\": \"test\", \"type\": \"text\"}}},\n\t}, false)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tcollection, err := testApp.FindCollectionByNameOrId(\"new_test\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\traw, err := json.Marshal(collection)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\trawStr := string(raw)\n\n\texpectedParts := []string{\n\t\t`\"name\":\"new_test\"`,\n\t\t`\"fields\":[`,\n\t\t`\"name\":\"id\"`,\n\t\t`\"name\":\"email\"`,\n\t\t`\"name\":\"tokenKey\"`,\n\t\t`\"name\":\"password\"`,\n\t\t`\"name\":\"test\"`,\n\t\t`\"indexes\":[`,\n\t\t`CREATE UNIQUE INDEX`,\n\t\t`\"duration\":123`,\n\t}\n\n\tfor _, part := range expectedParts {\n\t\tif !strings.Contains(rawStr, part) {\n\t\t\tt.Errorf(\"Missing %q in\\n%s\", part, rawStr)\n\t\t}\n\t}\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: pocketbase\nSource File Path(s): ['pocketbase\\\\collection_query_test.go\\\\source_files\\\\collection_query.go']\nTest File Path: pocketbase\\collection_query_test.go\\collection_query_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: pocketbase\\collection_query_test.go\\source_files\\collection_query.go\npackage core\n\nimport (\n\t\"bytes\"\n\t\"database/sql\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"slices\"\n\t\"strings\"\n\n\t\"github.com/pocketbase/dbx\"\n\t\"github.com/pocketbase/pocketbase/tools/list\"\n)\n\nconst StoreKeyCachedCollections = \"pbAppCachedCollections\"\n\n// CollectionQuery returns a new Collection select query.\nfunc (app *BaseApp) CollectionQuery() *dbx.SelectQuery {\n\treturn app.ModelQuery(&Collection{})\n}\n\n// FindCollections finds all collections by the given type(s).\n//\n// If collectionTypes is not set, it returns all collections.\n//\n// Example:\n//\n//\tapp.FindAllCollections() // all collections\n//\tapp.FindAllCollections(\"auth\", \"view\") // only auth and view collections\nfunc (app *BaseApp) FindAllCollections(collectionTypes ...string) ([]*Collection, error) {\n\tcollections := []*Collection{}\n\n\tq := app.CollectionQuery()\n\n\ttypes := list.NonzeroUniques(collectionTypes)\n\tif len(types) > 0 {\n\t\tq.AndWhere(dbx.In(\"type\", list.ToInterfaceSlice(types)...))\n\t}\n\n\terr := q.OrderBy(\"created ASC\").All(&collections)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn collections, nil\n}\n\n// ReloadCachedCollections fetches all collections and caches them into the app store.\nfunc (app *BaseApp) ReloadCachedCollections() error {\n\tcollections, err := app.FindAllCollections()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tapp.Store().Set(StoreKeyCachedCollections, collections)\n\n\treturn nil\n}\n\n// FindCollectionByNameOrId finds a single collection by its name (case insensitive) or id.\nfunc (app *BaseApp) FindCollectionByNameOrId(nameOrId string) (*Collection, error) {\n\tm := &Collection{}\n\n\terr := app.CollectionQuery().\n\t\tAndWhere(dbx.NewExp(\"[[id]]={:id} OR LOWER([[name]])={:name}\", dbx.Params{\n\t\t\t\"id\":   nameOrId,\n\t\t\t\"name\": strings.ToLower(nameOrId),\n\t\t})).\n\t\tLimit(1).\n\t\tOne(m)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn m, nil\n}\n\n// FindCachedCollectionByNameOrId is similar to [BaseApp.FindCollectionByNameOrId]\n// but retrieves the Collection from the app cache instead of making a db call.\n//\n// NB! This method is suitable for read-only Collection operations.\n//\n// Returns [sql.ErrNoRows] if no Collection is found for consistency\n// with the [BaseApp.FindCollectionByNameOrId] method.\n//\n// If you plan making changes to the returned Collection model,\n// use [BaseApp.FindCollectionByNameOrId] instead.\n//\n// Caveats:\n//\n//   - The returned Collection should be used only for read-only operations.\n//     Avoid directly modifying the returned cached Collection as it will affect\n//     the global cached value even if you don't persist the changes in the database!\n//   - If you are updating a Collection in a transaction and then call this method before commit,\n//     it'll return the cached Collection state and not the one from the uncommitted transaction.\n//   - The cache is automatically updated on collections db change (create/update/delete).\n//     To manually reload the cache you can call [BaseApp.ReloadCachedCollections].\nfunc (app *BaseApp) FindCachedCollectionByNameOrId(nameOrId string) (*Collection, error) {\n\tcollections, _ := app.Store().Get(StoreKeyCachedCollections).([]*Collection)\n\tif collections == nil {\n\t\t// cache is not initialized yet (eg. run in a system migration)\n\t\treturn app.FindCollectionByNameOrId(nameOrId)\n\t}\n\n\tfor _, c := range collections {\n\t\tif strings.EqualFold(c.Name, nameOrId) || c.Id == nameOrId {\n\t\t\treturn c, nil\n\t\t}\n\t}\n\n\treturn nil, sql.ErrNoRows\n}\n\n// FindCollectionReferences returns information for all relation fields\n// referencing the provided collection.\n//\n// If the provided collection has reference to itself then it will be\n// also included in the result. To exclude it, pass the collection id\n// as the excludeIds argument.\nfunc (app *BaseApp) FindCollectionReferences(collection *Collection, excludeIds ...string) (map[*Collection][]Field, error) {\n\tcollections := []*Collection{}\n\n\tquery := app.CollectionQuery()\n\n\tif uniqueExcludeIds := list.NonzeroUniques(excludeIds); len(uniqueExcludeIds) > 0 {\n\t\tquery.AndWhere(dbx.NotIn(\"id\", list.ToInterfaceSlice(uniqueExcludeIds)...))\n\t}\n\n\tif err := query.All(&collections); err != nil {\n\t\treturn nil, err\n\t}\n\n\tresult := map[*Collection][]Field{}\n\n\tfor _, c := range collections {\n\t\tfor _, rawField := range c.Fields {\n\t\t\tf, ok := rawField.(*RelationField)\n\t\t\tif ok && f.CollectionId == collection.Id {\n\t\t\t\tresult[c] = append(result[c], f)\n\t\t\t}\n\t\t}\n\t}\n\n\treturn result, nil\n}\n\n// FindCachedCollectionReferences is similar to [BaseApp.FindCollectionReferences]\n// but retrieves the Collection from the app cache instead of making a db call.\n//\n// NB! This method is suitable for read-only Collection operations.\n//\n// If you plan making changes to the returned Collection model,\n// use [BaseApp.FindCollectionReferences] instead.\n//\n// Caveats:\n//\n//   - The returned Collection should be used only for read-only operations.\n//     Avoid directly modifying the returned cached Collection as it will affect\n//     the global cached value even if you don't persist the changes in the database!\n//   - If you are updating a Collection in a transaction and then call this method before commit,\n//     it'll return the cached Collection state and not the one from the uncommitted transaction.\n//   - The cache is automatically updated on collections db change (create/update/delete).\n//     To manually reload the cache you can call [BaseApp.ReloadCachedCollections].\nfunc (app *BaseApp) FindCachedCollectionReferences(collection *Collection, excludeIds ...string) (map[*Collection][]Field, error) {\n\tcollections, _ := app.Store().Get(StoreKeyCachedCollections).([]*Collection)\n\tif collections == nil {\n\t\t// cache is not initialized yet (eg. run in a system migration)\n\t\treturn app.FindCollectionReferences(collection, excludeIds...)\n\t}\n\n\tresult := map[*Collection][]Field{}\n\n\tfor _, c := range collections {\n\t\tif slices.Contains(excludeIds, c.Id) {\n\t\t\tcontinue\n\t\t}\n\n\t\tfor _, rawField := range c.Fields {\n\t\t\tf, ok := rawField.(*RelationField)\n\t\t\tif ok && f.CollectionId == collection.Id {\n\t\t\t\tresult[c] = append(result[c], f)\n\t\t\t}\n\t\t}\n\t}\n\n\treturn result, nil\n}\n\n// IsCollectionNameUnique checks that there is no existing collection\n// with the provided name (case insensitive!).\n//\n// Note: case insensitive check because the name is used also as\n// table name for the records.\nfunc (app *BaseApp) IsCollectionNameUnique(name string, excludeIds ...string) bool {\n\tif name == \"\" {\n\t\treturn false\n\t}\n\n\tquery := app.CollectionQuery().\n\t\tSelect(\"count(*)\").\n\t\tAndWhere(dbx.NewExp(\"LOWER([[name]])={:name}\", dbx.Params{\"name\": strings.ToLower(name)})).\n\t\tLimit(1)\n\n\tif uniqueExcludeIds := list.NonzeroUniques(excludeIds); len(uniqueExcludeIds) > 0 {\n\t\tquery.AndWhere(dbx.NotIn(\"id\", list.ToInterfaceSlice(uniqueExcludeIds)...))\n\t}\n\n\tvar total int\n\n\treturn query.Row(&total) == nil && total == 0\n}\n\n// TruncateCollection deletes all records associated with the provided collection.\n//\n// The truncate operation is executed in a single transaction,\n// aka. either everything is deleted or none.\n//\n// Note that this method will also trigger the records related\n// cascade and file delete actions.\nfunc (app *BaseApp) TruncateCollection(collection *Collection) error {\n\tif collection.IsView() {\n\t\treturn errors.New(\"view collections cannot be truncated since they don't store their own records.\")\n\t}\n\n\treturn app.RunInTransaction(func(txApp App) error {\n\t\trecords := make([]*Record, 0, 500)\n\n\t\tfor {\n\t\t\terr := txApp.RecordQuery(collection).Limit(500).All(&records)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tif len(records) == 0 {\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\t\tfor _, record := range records {\n\t\t\t\terr = txApp.Delete(record)\n\t\t\t\tif err != nil && !errors.Is(err, sql.ErrNoRows) {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\n\t\t\trecords = records[:0]\n\t\t}\n\t})\n}\n\n// -------------------------------------------------------------------\n\n// saveViewCollection persists the provided View collection changes:\n//   - deletes the old related SQL view (if any)\n//   - creates a new SQL view with the latest newCollection.Options.Query\n//   - generates new feilds list  based on newCollection.Options.Query\n//   - updates newCollection.Fields based on the generated view table info and query\n//   - saves the newCollection\n//\n// This method returns an error if newCollection is not a \"view\".\nfunc saveViewCollection(app App, newCollection, oldCollection *Collection) error {\n\tif !newCollection.IsView() {\n\t\treturn errors.New(\"not a view collection\")\n\t}\n\n\treturn app.RunInTransaction(func(txApp App) error {\n\t\tquery := newCollection.ViewQuery\n\n\t\t// generate collection fields from the query\n\t\tviewFields, err := txApp.CreateViewFields(query)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// delete old renamed view\n\t\tif oldCollection != nil {\n\t\t\tif err := txApp.DeleteView(oldCollection.Name); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\n\t\t// wrap view query if necessary\n\t\tquery, err = normalizeViewQueryId(txApp, query)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to normalize view query id: %w\", err)\n\t\t}\n\n\t\t// (re)create the view\n\t\tif err := txApp.SaveView(newCollection.Name, query); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tnewCollection.Fields = viewFields\n\n\t\treturn txApp.Save(newCollection)\n\t})\n}\n\n// normalizeViewQueryId wraps (if necessary) the provided view query\n// with a subselect to ensure that the id column is a text since\n// currently we don't support non-string model ids\n// (see https://github.com/pocketbase/pocketbase/issues/3110).\nfunc normalizeViewQueryId(app App, query string) (string, error) {\n\tquery = strings.Trim(strings.TrimSpace(query), \";\")\n\n\tinfo, err := getQueryTableInfo(app, query)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\tfor _, row := range info {\n\t\tif strings.EqualFold(row.Name, FieldNameId) && strings.EqualFold(row.Type, \"TEXT\") {\n\t\t\treturn query, nil // no wrapping needed\n\t\t}\n\t}\n\n\t// raw parse to preserve the columns order\n\trawParsed := new(identifiersParser)\n\tif err := rawParsed.parse(query); err != nil {\n\t\treturn \"\", err\n\t}\n\n\tcolumns := make([]string, 0, len(rawParsed.columns))\n\tfor _, col := range rawParsed.columns {\n\t\tif col.alias == FieldNameId {\n\t\t\tcolumns = append(columns, fmt.Sprintf(\"CAST([[%s]] as TEXT) [[%s]]\", col.alias, col.alias))\n\t\t} else {\n\t\t\tcolumns = append(columns, \"[[\"+col.alias+\"]]\")\n\t\t}\n\t}\n\n\tquery = fmt.Sprintf(\"SELECT %s FROM (%s)\", strings.Join(columns, \",\"), query)\n\n\treturn query, nil\n}\n\n// resaveViewsWithChangedFields updates all view collections with changed fields.\nfunc resaveViewsWithChangedFields(app App, excludeIds ...string) error {\n\tcollections, err := app.FindAllCollections(CollectionTypeView)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn app.RunInTransaction(func(txApp App) error {\n\t\tfor _, collection := range collections {\n\t\t\tif len(excludeIds) > 0 && list.ExistInSlice(collection.Id, excludeIds) {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// clone the existing fields for temp modifications\n\t\t\toldFields, err := collection.Fields.Clone()\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\t// generate new fields from the query\n\t\t\tnewFields, err := txApp.CreateViewFields(collection.ViewQuery)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\t// unset the fields' ids to exclude from the comparison\n\t\t\tfor _, f := range oldFields {\n\t\t\t\tf.SetId(\"\")\n\t\t\t}\n\t\t\tfor _, f := range newFields {\n\t\t\t\tf.SetId(\"\")\n\t\t\t}\n\n\t\t\tencodedNewFields, err := json.Marshal(newFields)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tencodedOldFields, err := json.Marshal(oldFields)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tif bytes.EqualFold(encodedNewFields, encodedOldFields) {\n\t\t\t\tcontinue // no changes\n\t\t\t}\n\n\t\t\tif err := saveViewCollection(txApp, collection, nil); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\n\t\treturn nil\n\t})\n}\n\n### Source File Dependency Files Content\n### Dependency File: pocketbase\\collection_query_test.go\\dependent_files\\list.go\npackage list\n\nimport (\n\t\"encoding/json\"\n\t\"regexp\"\n\t\"strings\"\n\n\t\"github.com/pocketbase/pocketbase/tools/store\"\n\t\"github.com/spf13/cast\"\n)\n\nvar cachedPatterns = store.New[string, *regexp.Regexp](nil)\n\n// SubtractSlice returns a new slice with only the \"base\" elements\n// that don't exist in \"subtract\".\nfunc SubtractSlice[T comparable](base []T, subtract []T) []T {\n\tvar result = make([]T, 0, len(base))\n\n\tfor _, b := range base {\n\t\tif !ExistInSlice(b, subtract) {\n\t\t\tresult = append(result, b)\n\t\t}\n\t}\n\n\treturn result\n}\n\n// ExistInSlice checks whether a comparable element exists in a slice of the same type.\nfunc ExistInSlice[T comparable](item T, list []T) bool {\n\tfor _, v := range list {\n\t\tif v == item {\n\t\t\treturn true\n\t\t}\n\t}\n\n\treturn false\n}\n\n// ExistInSliceWithRegex checks whether a string exists in a slice\n// either by direct match, or by a regular expression (eg. `^\\w+$`).\n//\n// Note: Only list items starting with '^' and ending with '$' are treated as regular expressions!\nfunc ExistInSliceWithRegex(str string, list []string) bool {\n\tfor _, field := range list {\n\t\tisRegex := strings.HasPrefix(field, \"^\") && strings.HasSuffix(field, \"$\")\n\n\t\tif !isRegex {\n\t\t\t// check for direct match\n\t\t\tif str == field {\n\t\t\t\treturn true\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\t// check for regex match\n\t\tpattern := cachedPatterns.Get(field)\n\t\tif pattern == nil {\n\t\t\tvar err error\n\t\t\tpattern, err = regexp.Compile(field)\n\t\t\tif err != nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// \"cache\" the pattern to avoid compiling it every time\n\t\t\t// (the limit size is arbitrary and it is there to prevent the cache growing too big)\n\t\t\t//\n\t\t\t// @todo consider replacing with TTL or LRU type cache\n\t\t\tcachedPatterns.SetIfLessThanLimit(field, pattern, 500)\n\t\t}\n\n\t\tif pattern != nil && pattern.MatchString(str) {\n\t\t\treturn true\n\t\t}\n\t}\n\n\treturn false\n}\n\n// ToInterfaceSlice converts a generic slice to slice of interfaces.\nfunc ToInterfaceSlice[T any](list []T) []any {\n\tresult := make([]any, len(list))\n\n\tfor i := range list {\n\t\tresult[i] = list[i]\n\t}\n\n\treturn result\n}\n\n// NonzeroUniques returns only the nonzero unique values from a slice.\nfunc NonzeroUniques[T comparable](list []T) []T {\n\tresult := make([]T, 0, len(list))\n\texistMap := make(map[T]struct{}, len(list))\n\n\tvar zeroVal T\n\n\tfor _, val := range list {\n\t\tif val == zeroVal {\n\t\t\tcontinue\n\t\t}\n\t\tif _, ok := existMap[val]; ok {\n\t\t\tcontinue\n\t\t}\n\t\texistMap[val] = struct{}{}\n\t\tresult = append(result, val)\n\t}\n\n\treturn result\n}\n\n// ToUniqueStringSlice casts `value` to a slice of non-zero unique strings.\nfunc ToUniqueStringSlice(value any) (result []string) {\n\tswitch val := value.(type) {\n\tcase nil:\n\t\t// nothing to cast\n\tcase []string:\n\t\tresult = val\n\tcase string:\n\t\tif val == \"\" {\n\t\t\tbreak\n\t\t}\n\n\t\t// check if it is a json encoded array of strings\n\t\tif strings.Contains(val, \"[\") {\n\t\t\tif err := json.Unmarshal([]byte(val), &result); err != nil {\n\t\t\t\t// not a json array, just add the string as single array element\n\t\t\t\tresult = append(result, val)\n\t\t\t}\n\t\t} else {\n\t\t\t// just add the string as single array element\n\t\t\tresult = append(result, val)\n\t\t}\n\tcase json.Marshaler: // eg. JSONArray\n\t\traw, _ := val.MarshalJSON()\n\t\t_ = json.Unmarshal(raw, &result)\n\tdefault:\n\t\tresult = cast.ToStringSlice(value)\n\t}\n\n\treturn NonzeroUniques(result)\n}\n\n// ToChunks splits list into chunks.\n//\n// Zero or negative chunkSize argument is normalized to 1.\n//\n// See https://go.dev/wiki/SliceTricks#batching-with-minimal-allocation.\nfunc ToChunks[T any](list []T, chunkSize int) [][]T {\n\tif chunkSize <= 0 {\n\t\tchunkSize = 1\n\t}\n\n\tchunks := make([][]T, 0, (len(list)+chunkSize-1)/chunkSize)\n\n\tif len(list) == 0 {\n\t\treturn chunks\n\t}\n\n\tfor chunkSize < len(list) {\n\t\tlist, chunks = list[chunkSize:], append(chunks, list[0:chunkSize:chunkSize])\n\t}\n\n\treturn append(chunks, list)\n}\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:52:19"}, {"role": "assistant", "content": "```go\npackage core_test\n\nimport (\n\t\"context\"\n\t\"database/sql\"\n\t\"fmt\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"slices\"\n\t\"strings\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/pocketbase/dbx\"\n\t\"github.com/pocketbase/pocketbase/core\"\n\t\"github.com/pocketbase/pocketbase/tests\"\n\t\"github.com/pocketbase/pocketbase/tools/list\"\n)\n\nfunc TestCollectionQuery(t *testing.T) {\n\tt.Parallel()\n\n\tapp, _ := tests.NewTestApp()\n\tdefer app.Cleanup()\n\n\texpected := \"SELECT {{_collections}}.* FROM `_collections`\"\n\n\tsql := app.CollectionQuery().Build().SQL()\n\tif sql != expected {\n\t\tt.Errorf(\"Expected sql %s, got %s\", expected, sql)\n\t}\n}\n\nfunc TestReloadCachedCollections(t *testing.T) {\n\tt.Parallel()\n\n\tapp, _ := tests.NewTestApp()\n\tdefer app.Cleanup()\n\n\terr := app.ReloadCachedCollections()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tcached := app.Store().Get(core.StoreKeyCachedCollections)\n\n\tcachedCollections, ok := cached.([]*core.Collection)\n\tif !ok {\n\t\tt.Fatalf(\"Expected []*core.Collection, got %T\", cached)\n\t}\n\n\tcollections, err := app.FindAllCollections()\n\tif err != nil {\n\t\tt.Fatalf(\"Failed to retrieve all collections: %v\", err)\n\t}\n\n\tif len(cachedCollections) != len(collections) {\n\t\tt.Fatalf(\"Expected %d collections, got %d\", len(collections), len(cachedCollections))\n\t}\n\n\tfor _, c := range collections {\n\t\tvar exists bool\n\t\tfor _, cc := range cachedCollections {\n\t\t\tif cc.Id == c.Id {\n\t\t\t\texists = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif !exists {\n\t\t\tt.Fatalf(\"The collections cache is missing collection %q\", c.Name)\n\t\t}\n\t}\n}\n\nfunc TestFindAllCollections(t *testing.T) {\n\tt.Parallel()\n\n\tapp, _ := tests.NewTestApp()\n\tdefer app.Cleanup()\n\n\tscenarios := []struct {\n\t\tcollectionTypes []string\n\t\texpectTotal     int\n\t}{\n\t\t{nil, 16},\n\t\t{[]string{}, 16},\n\t\t{[]string{\"\"}, 16},\n\t\t{[]string{\"unknown\"}, 0},\n\t\t{[]string{\"unknown\", core.CollectionTypeAuth}, 4},\n\t\t{[]string{core.CollectionTypeAuth, core.CollectionTypeView}, 7},\n\t}\n\n\tfor i, s := range scenarios {\n\t\tt.Run(fmt.Sprintf(\"%d_%s\", i, strings.Join(s.collectionTypes, \"_\")), func(t *testing.T) {\n\t\t\tcollections, err := app.FindAllCollections(s.collectionTypes...)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\tif len(collections) != s.expectTotal {\n\t\t\t\tt.Fatalf(\"Expected %d collections, got %d\", s.expectTotal, len(collections))\n\t\t\t}\n\n\t\t\texpectedTypes := list.NonzeroUniques(s.collectionTypes)\n\t\t\tif len(expectedTypes) > 0 {\n\t\t\t\tfor _, c := range collections {\n\t\t\t\t\tif !slices.Contains(expectedTypes, c.Type) {\n\t\t\t\t\t\tt.Fatalf(\"Unexpected collection type %s\\n%v\", c.Type, c)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestFindCollectionByNameOrId(t *testing.T) {\n\tt.Parallel()\n\n\tapp, _ := tests.NewTestApp()\n\tdefer app.Cleanup()\n\n\tscenarios := []struct {\n\t\tnameOrId    string\n\t\texpectError bool\n\t}{\n\t\t{\"\", true},\n\t\t{\"missing\", true},\n\t\t{\"wsmn24bux7wo113\", false},\n\t\t{\"demo1\", false},\n\t\t{\"DEMO1\", false}, // case insensitive\n\t}\n\n\tfor i, s := range scenarios {\n\t\tt.Run(fmt.Sprintf(\"%d_%s\", i, s.nameOrId), func(t *testing.T) {\n\t\t\tmodel, err := app.FindCollectionByNameOrId(s.nameOrId)\n\n\t\t\thasErr := err != nil\n\t\t\tif hasErr != s.expectError {\n\t\t\t\tt.Fatalf(\"Expected hasErr to be %v, got %v (%v)\", s.expectError, hasErr, err)\n\t\t\t}\n\n\t\t\tif model != nil && model.Id != s.nameOrId && !strings.EqualFold(model.Name, s.nameOrId) {\n\t\t\t\tt.Fatalf(\"Expected model with identifier %s, got %v\", s.nameOrId, model)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestFindCachedCollectionByNameOrId(t *testing.T) {\n\tt.Parallel()\n\n\tapp, _ := tests.NewTestApp()\n\tdefer app.Cleanup()\n\n\ttotalQueries := 0\n\tapp.DB().(*dbx.DB).QueryLogFunc = func(ctx context.Context, t time.Duration, sql string, rows *sql.Rows, err error) {\n\t\ttotalQueries++\n\t}\n\n\trun := func(withCache bool) {\n\t\tscenarios := []struct {\n\t\t\tnameOrId    string\n\t\t\texpectError bool\n\t\t}{\n\t\t\t{\"\", true},\n\t\t\t{\"missing\", true},\n\t\t\t{\"wsmn24bux7wo113\", false},\n\t\t\t{\"demo1\", false},\n\t\t\t{\"DEMO1\", false}, // case insensitive\n\t\t}\n\n\t\tvar expectedTotalQueries int\n\n\t\tif withCache {\n\t\t\terr := app.ReloadCachedCollections()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t} else {\n\t\t\tapp.Store().Reset(nil)\n\t\t\texpectedTotalQueries = len(scenarios)\n\t\t}\n\n\t\ttotalQueries = 0\n\n\t\tfor i, s := range scenarios {\n\t\t\tt.Run(fmt.Sprintf(\"%d_%s\", i, s.nameOrId), func(t *testing.T) {\n\t\t\t\tmodel, err := app.FindCachedCollectionByNameOrId(s.nameOrId)\n\n\t\t\t\thasErr := err != nil\n\t\t\t\tif hasErr != s.expectError {\n\t\t\t\t\tt.Fatalf(\"Expected hasErr to be %v, got %v (%v)\", s.expectError, hasErr, err)\n\t\t\t\t}\n\n\t\t\t\tif model != nil && model.Id != s.nameOrId && !strings.EqualFold(model.Name, s.nameOrId) {\n\t\t\t\t\tt.Fatalf(\"Expected model with identifier %s, got %v\", s.nameOrId, model)\n\t\t\t\t}\n\t\t\t})\n\t\t}\n\n\t\tif totalQueries != expectedTotalQueries {\n\t\t\tt.Fatalf(\"Expected %d totalQueries, got %d\", expectedTotalQueries, totalQueries)\n\t\t}\n\t}\n\n\trun(true)\n\n\trun(false)\n}\n\nfunc TestFindCollectionReferences(t *testing.T) {\n\tt.Parallel()\n\n\tapp, _ := tests.NewTestApp()\n\tdefer app.Cleanup()\n\n\tcollection, err := app.FindCollectionByNameOrId(\"demo3\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tresult, err := app.FindCollectionReferences(\n\t\tcollection,\n\t\tcollection.Id,\n\t\t// test whether \"nonempty\" exclude ids condition will be skipped\n\t\t\"\",\n\t\t\"\",\n\t)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif len(result) != 1 {\n\t\tt.Fatalf(\"Expected 1 collection, got %d: %v\", len(result), result)\n\t}\n\n\texpectedFields := []string{\n\t\t\"rel_one_no_cascade\",\n\t\t\"rel_one_no_cascade_required\",\n\t\t\"rel_one_cascade\",\n\t\t\"rel_one_unique\",\n\t\t\"rel_many_no_cascade\",\n\t\t\"rel_many_no_cascade_required\",\n\t\t\"rel_many_cascade\",\n\t\t\"rel_many_unique\",\n\t}\n\n\tfor col, fields := range result {\n\t\tif col.Name != \"demo4\" {\n\t\t\tt.Fatalf(\"Expected collection demo4, got %s\", col.Name)\n\t\t}\n\t\tif len(fields) != len(expectedFields) {\n\t\t\tt.Fatalf(\"Expected fields %v, got %v\", expectedFields, fields)\n\t\t}\n\t\tfor i, f := range fields {\n\t\t\tif !slices.Contains(expectedFields, f.GetName()) {\n\t\t\t\tt.Fatalf(\"[%d] Didn't expect field %v\", i, f)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestFindCachedCollectionReferences(t *testing.T) {\n\tt.Parallel()\n\n\tapp, _ := tests.NewTestApp()\n\tdefer app.Cleanup()\n\n\tcollection, err := app.FindCollectionByNameOrId(\"demo3\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ttotalQueries := 0\n\tapp.DB().(*dbx.DB).QueryLogFunc = func(ctx context.Context, t time.Duration, sql string, rows *sql.Rows, err error) {\n\t\ttotalQueries++\n\t}\n\n\trun := func(withCache bool) {\n\t\tvar expectedTotalQueries int\n\n\t\tif withCache {\n\t\t\terr := app.ReloadCachedCollections()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t} else {\n\t\t\tapp.Store().Reset(nil)\n\t\t\texpectedTotalQueries = 1\n\t\t}\n\n\t\ttotalQueries = 0\n\n\t\tresult, err := app.FindCachedCollectionReferences(\n\t\t\tcollection,\n\t\t\tcollection.Id,\n\t\t\t// test whether \"nonempty\" exclude ids condition will be skipped\n\t\t\t\"\",\n\t\t\t\"\",\n\t\t)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif len(result) != 1 {\n\t\t\tt.Fatalf(\"Expected 1 collection, got %d: %v\", len(result), result)\n\t\t}\n\n\t\texpectedFields := []string{\n\t\t\t\"rel_one_no_cascade\",\n\t\t\t\"rel_one_no_cascade_required\",\n\t\t\t\"rel_one_cascade\",\n\t\t\t\"rel_one_unique\",\n\t\t\t\"rel_many_no_cascade\",\n\t\t\t\"rel_many_no_cascade_required\",\n\t\t\t\"rel_many_cascade\",\n\t\t\t\"rel_many_unique\",\n\t\t}\n\n\t\tfor col, fields := range result {\n\t\t\tif col.Name != \"demo4\" {\n\t\t\t\tt.Fatalf(\"Expected collection demo4, got %s\", col.Name)\n\t\t\t}\n\t\t\tif len(fields) != len(expectedFields) {\n\t\t\t\tt.Fatalf(\"Expected fields %v, got %v\", expectedFields, fields)\n\t\t\t}\n\t\t\tfor i, f := range fields {\n\t\t\t\tif !slices.Contains(expectedFields, f.GetName()) {\n\t\t\t\t\tt.Fatalf(\"[%d] Didn't expect field %v\", i, f)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif totalQueries != expectedTotalQueries {\n\t\t\tt.Fatalf(\"Expected %d totalQueries, got %d\", expectedTotalQueries, totalQueries)\n\t\t}\n\t}\n\n\trun(true)\n\n\trun(false)\n}\n\nfunc TestIsCollectionNameUnique(t *testing.T) {\n\tt.Parallel()\n\n\tapp, _ := tests.NewTestApp()\n\tdefer app.Cleanup()\n\n\tscenarios := []struct {\n\t\tname      string\n\t\texcludeId string\n\t\texpected  bool\n\t}{\n\t\t{\"\", \"\", false},\n\t\t{\"demo1\", \"\", false},\n\t\t{\"Demo1\", \"\", false},\n\t\t{\"new\", \"\", true},\n\t\t{\"demo1\", \"wsmn24bux7wo113\", true},\n\t}\n\n\tfor i, s := range scenarios {\n\t\tt.Run(fmt.Sprintf(\"%d_%s\", i, s.name), func(t *testing.T) {\n\t\t\tresult := app.IsCollectionNameUnique(s.name, s.excludeId)\n\t\t\tif result != s.expected {\n\t\t\t\tt.Errorf(\"Expected %v, got %v\", s.expected, result)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestFindCollectionTruncate(t *testing.T) {\n\tt.Parallel()\n\n\tapp, _ := tests.NewTestApp()\n\tdefer app.Cleanup()\n\n\tcountFiles := func(collectionId string) (int, error) {\n\t\tentries, err := os.ReadDir(filepath.Join(app.DataDir(), \"storage\", collectionId))\n\t\treturn len(entries), err\n\t}\n\n\tt.Run(\"truncate view\", func(t *testing.T) {\n\t\tview2, err := app.FindCollectionByNameOrId(\"view2\")\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\terr = app.TruncateCollection(view2)\n\t\tif err == nil {\n\t\t\tt.Fatalf(\"Expected truncate to fail because view collections can't be truncated\")\n\t\t}\n\t})\n\n\tt.Run(\"truncate failure\", func(t *testing.T) {\n\t\tdemo3, err := app.FindCollectionByNameOrId(\"demo3\")\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\toriginalTotalRecords, err := app.CountRecords(demo3)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\toriginalTotalFiles, err := countFiles(demo3.Id)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\terr = app.TruncateCollection(demo3)\n\t\tif err == nil {\n\t\t\tt.Fatalf(\"Expected truncate to fail due to cascade delete failed required constraint\")\n\t\t}\n\n\t\t// short delay to ensure that the file delete goroutine has been executed\n\t\ttime.Sleep(100 * time.Millisecond)\n\n\t\ttotalRecords, err := app.CountRecords(demo3)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif totalRecords != originalTotalRecords {\n\t\t\tt.Fatalf(\"Expected %d records, got %d\", originalTotalRecords, totalRecords)\n\t\t}\n\n\t\ttotalFiles, err := countFiles(demo3.Id)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tif totalFiles != originalTotalFiles {\n\t\t\tt.Fatalf(\"Expected %d files, got %d\", originalTotalFiles, totalFiles)\n\t\t}\n\t})\n\n\tt.Run(\"truncate success\", func(t *testing.T) {\n\t\tdemo5, err := app.FindCollectionByNameOrId(\"demo5\")\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\terr = app.TruncateCollection(demo5)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\t// short delay to ensure that the file delete goroutine has been executed\n\t\ttime.Sleep(100 * time.Millisecond)\n\n\t\ttotal, err := app.CountRecords(demo5)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tif total != 0 {\n\t\t\tt.Fatalf(\"Expected all records to be deleted, got %v\", total)\n\t\t}\n\n\t\ttotalFiles, err := countFiles(demo5.Id)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\tif totalFiles != 0 {\n\t\t\tt.Fatalf(\"Expected truncated record files to be deleted, got %d\", totalFiles)\n\t\t}\n\n\t\t// try to truncate again (shouldn't return an error)\n\t\terr = app.TruncateCollection(demo5)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t})\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: pocketbase\nSource File Path(s): ['pocketbase\\\\datetime_test.go\\\\source_files\\\\datetime.go']\nTest File Path: pocketbase\\datetime_test.go\\datetime_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: pocketbase\\datetime_test.go\\source_files\\datetime.go\npackage types\n\nimport (\n\t\"database/sql/driver\"\n\t\"encoding/json\"\n\t\"time\"\n\n\t\"github.com/spf13/cast\"\n)\n\n// DefaultDateLayout specifies the default app date strings layout.\nconst DefaultDateLayout = \"2006-01-02 15:04:05.000Z\"\n\n// NowDateTime returns new DateTime instance with the current local time.\nfunc NowDateTime() DateTime {\n\treturn DateTime{t: time.Now()}\n}\n\n// ParseDateTime creates a new DateTime from the provided value\n// (could be [cast.ToTime] supported string, [time.Time], etc.).\nfunc ParseDateTime(value any) (DateTime, error) {\n\td := DateTime{}\n\terr := d.Scan(value)\n\treturn d, err\n}\n\n// DateTime represents a [time.Time] instance in UTC that is wrapped\n// and serialized using the app default date layout.\ntype DateTime struct {\n\tt time.Time\n}\n\n// Time returns the internal [time.Time] instance.\nfunc (d DateTime) Time() time.Time {\n\treturn d.t\n}\n\n// Add returns a new DateTime based on the current DateTime + the specified duration.\nfunc (d DateTime) Add(duration time.Duration) DateTime {\n\td.t = d.t.Add(duration)\n\treturn d\n}\n\n// Sub returns a [time.Duration] by subtracting the specified DateTime from the current one.\n//\n// If the result exceeds the maximum (or minimum) value that can be stored in a [time.Duration],\n// the maximum (or minimum) duration will be returned.\nfunc (d DateTime) Sub(u DateTime) time.Duration {\n\treturn d.Time().Sub(u.Time())\n}\n\n// AddDate returns a new DateTime based on the current one + duration.\n//\n// It follows the same rules as [time.AddDate].\nfunc (d DateTime) AddDate(years, months, days int) DateTime {\n\td.t = d.t.AddDate(years, months, days)\n\treturn d\n}\n\n// After reports whether the current DateTime instance is after u.\nfunc (d DateTime) After(u DateTime) bool {\n\treturn d.Time().After(u.Time())\n}\n\n// Before reports whether the current DateTime instance is before u.\nfunc (d DateTime) Before(u DateTime) bool {\n\treturn d.Time().Before(u.Time())\n}\n\n// Compare compares the current DateTime instance with u.\n// If the current instance is before u, it returns -1.\n// If the current instance is after u, it returns +1.\n// If they're the same, it returns 0.\nfunc (d DateTime) Compare(u DateTime) int {\n\treturn d.Time().Compare(u.Time())\n}\n\n// Equal reports whether the current DateTime and u represent the same time instant.\n// Two DateTime can be equal even if they are in different locations.\n// For example, 6:00 +0200 and 4:00 UTC are Equal.\nfunc (d DateTime) Equal(u DateTime) bool {\n\treturn d.Time().Equal(u.Time())\n}\n\n// Unix returns the current DateTime as a Unix time, aka.\n// the number of seconds elapsed since January 1, 1970 UTC.\nfunc (d DateTime) Unix() int64 {\n\treturn d.Time().Unix()\n}\n\n// IsZero checks whether the current DateTime instance has zero time value.\nfunc (d DateTime) IsZero() bool {\n\treturn d.Time().IsZero()\n}\n\n// String serializes the current DateTime instance into a formatted\n// UTC date string.\n//\n// The zero value is serialized to an empty string.\nfunc (d DateTime) String() string {\n\tt := d.Time()\n\tif t.IsZero() {\n\t\treturn \"\"\n\t}\n\treturn t.UTC().Format(DefaultDateLayout)\n}\n\n// MarshalJSON implements the [json.Marshaler] interface.\nfunc (d DateTime) MarshalJSON() ([]byte, error) {\n\treturn []byte(`\"` + d.String() + `\"`), nil\n}\n\n// UnmarshalJSON implements the [json.Unmarshaler] interface.\nfunc (d *DateTime) UnmarshalJSON(b []byte) error {\n\tvar raw string\n\tif err := json.Unmarshal(b, &raw); err != nil {\n\t\treturn err\n\t}\n\treturn d.Scan(raw)\n}\n\n// Value implements the [driver.Valuer] interface.\nfunc (d DateTime) Value() (driver.Value, error) {\n\treturn d.String(), nil\n}\n\n// Scan implements [sql.Scanner] interface to scan the provided value\n// into the current DateTime instance.\nfunc (d *DateTime) Scan(value any) error {\n\tswitch v := value.(type) {\n\tcase time.Time:\n\t\td.t = v\n\tcase DateTime:\n\t\td.t = v.Time()\n\tcase string:\n\t\tif v == \"\" {\n\t\t\td.t = time.Time{}\n\t\t} else {\n\t\t\tt, err := time.Parse(DefaultDateLayout, v)\n\t\t\tif err != nil {\n\t\t\t\t// check for other common date layouts\n\t\t\t\tt = cast.ToTime(v)\n\t\t\t}\n\t\t\td.t = t\n\t\t}\n\tcase int, int64, int32, uint, uint64, uint32:\n\t\td.t = cast.ToTime(v)\n\tdefault:\n\t\tstr := cast.ToString(v)\n\t\tif str == \"\" {\n\t\t\td.t = time.Time{}\n\t\t} else {\n\t\t\td.t = cast.ToTime(str)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:52:19"}, {"role": "assistant", "content": "```go\npackage types_test\n\nimport (\n\t\"fmt\"\n\t\"strings\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/pocketbase/pocketbase/tools/types\"\n)\n\nfunc TestNowDateTime(t *testing.T) {\n\tnow := time.Now().UTC().Format(\"2006-01-02 15:04:05\") // without ms part for test consistency\n\tdt := types.NowDateTime()\n\n\tif !strings.Contains(dt.String(), now) {\n\t\tt.Fatalf(\"Expected %q, got %q\", now, dt.String())\n\t}\n}\n\nfunc TestParseDateTime(t *testing.T) {\n\tnowTime := time.Now().UTC()\n\tnowDateTime, _ := types.ParseDateTime(nowTime)\n\tnowStr := nowTime.Format(types.DefaultDateLayout)\n\n\tscenarios := []struct {\n\t\tvalue    any\n\t\texpected string\n\t}{\n\t\t{nil, \"\"},\n\t\t{\"\", \"\"},\n\t\t{\"invalid\", \"\"},\n\t\t{nowDateTime, nowStr},\n\t\t{nowTime, nowStr},\n\t\t{1641024040, \"2022-01-01 08:00:40.000Z\"},\n\t\t{int32(1641024040), \"2022-01-01 08:00:40.000Z\"},\n\t\t{int64(1641024040), \"2022-01-01 08:00:40.000Z\"},\n\t\t{uint(1641024040), \"2022-01-01 08:00:40.000Z\"},\n\t\t{uint64(1641024040), \"2022-01-01 08:00:40.000Z\"},\n\t\t{uint32(1641024040), \"2022-01-01 08:00:40.000Z\"},\n\t\t{\"2022-01-01 11:23:45.678\", \"2022-01-01 11:23:45.678Z\"},\n\t}\n\n\tfor i, s := range scenarios {\n\t\tt.Run(fmt.Sprintf(\"%d_%#v\", i, s.value), func(t *testing.T) {\n\t\t\tdt, err := types.ParseDateTime(s.value)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Failed to parse %v: %v\", s.value, err)\n\t\t\t}\n\n\t\t\tif dt.String() != s.expected {\n\t\t\t\tt.Fatalf(\"Expected %q, got %q\", s.expected, dt.String())\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestDateTimeTime(t *testing.T) {\n\tstr := \"2022-01-01 11:23:45.678Z\"\n\n\texpected, err := time.Parse(types.DefaultDateLayout, str)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tdt, err := types.ParseDateTime(str)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tresult := dt.Time()\n\n\tif !expected.Equal(result) {\n\t\tt.Fatalf(\"Expected time %v, got %v\", expected, result)\n\t}\n}\n\nfunc TestDateTimeAdd(t *testing.T) {\n\tt.Parallel()\n\n\td1, _ := types.ParseDateTime(\"2024-01-01 10:00:00.123Z\")\n\n\td2 := d1.Add(1 * time.Hour)\n\n\tif d1.String() != \"2024-01-01 10:00:00.123Z\" {\n\t\tt.Fatalf(\"Expected d1 to remain unchanged, got %s\", d1.String())\n\t}\n\n\texpected := \"2024-01-01 11:00:00.123Z\"\n\tif d2.String() != expected {\n\t\tt.Fatalf(\"Expected d2 %s, got %s\", expected, d2.String())\n\t}\n}\n\nfunc TestDateTimeSub(t *testing.T) {\n\tt.Parallel()\n\n\td1, _ := types.ParseDateTime(\"2024-01-01 10:00:00.123Z\")\n\td2, _ := types.ParseDateTime(\"2024-01-01 10:30:00.123Z\")\n\n\tresult := d2.Sub(d1)\n\n\tif result.Minutes() != 30 {\n\t\tt.Fatalf(\"Expected %v minutes diff, got %v\", 30, result.Minutes())\n\t}\n}\n\nfunc TestDateTimeAddDate(t *testing.T) {\n\tt.Parallel()\n\n\td1, _ := types.ParseDateTime(\"2024-01-01 10:00:00.123Z\")\n\n\td2 := d1.AddDate(1, 2, 3)\n\n\tif d1.String() != \"2024-01-01 10:00:00.123Z\" {\n\t\tt.Fatalf(\"Expected d1 to remain unchanged, got %s\", d1.String())\n\t}\n\n\texpected := \"2025-03-04 10:00:00.123Z\"\n\tif d2.String() != expected {\n\t\tt.Fatalf(\"Expected d2 %s, got %s\", expected, d2.String())\n\t}\n}\n\nfunc TestDateTimeAfter(t *testing.T) {\n\tt.Parallel()\n\n\td1, _ := types.ParseDateTime(\"2024-01-01 10:00:00.123Z\")\n\td2, _ := types.ParseDateTime(\"2024-01-02 10:00:00.123Z\")\n\td3, _ := types.ParseDateTime(\"2024-01-03 10:00:00.123Z\")\n\n\tscenarios := []struct {\n\t\ta      types.DateTime\n\t\tb      types.DateTime\n\t\texpect bool\n\t}{\n\t\t// d1\n\t\t{d1, d1, false},\n\t\t{d1, d2, false},\n\t\t{d1, d3, false},\n\t\t// d2\n\t\t{d2, d1, true},\n\t\t{d2, d2, false},\n\t\t{d2, d3, false},\n\t\t// d3\n\t\t{d3, d1, true},\n\t\t{d3, d2, true},\n\t\t{d3, d3, false},\n\t}\n\n\tfor i, s := range scenarios {\n\t\tt.Run(fmt.Sprintf(\"after_%d\", i), func(t *testing.T) {\n\t\t\tif v := s.a.After(s.b); v != s.expect {\n\t\t\t\tt.Fatalf(\"Expected %v, got %v\", s.expect, v)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestDateTimeBefore(t *testing.T) {\n\tt.Parallel()\n\n\td1, _ := types.ParseDateTime(\"2024-01-01 10:00:00.123Z\")\n\td2, _ := types.ParseDateTime(\"2024-01-02 10:00:00.123Z\")\n\td3, _ := types.ParseDateTime(\"2024-01-03 10:00:00.123Z\")\n\n\tscenarios := []struct {\n\t\ta      types.DateTime\n\t\tb      types.DateTime\n\t\texpect bool\n\t}{\n\t\t// d1\n\t\t{d1, d1, false},\n\t\t{d1, d2, true},\n\t\t{d1, d3, true},\n\t\t// d2\n\t\t{d2, d1, false},\n\t\t{d2, d2, false},\n\t\t{d2, d3, true},\n\t\t// d3\n\t\t{d3, d1, false},\n\t\t{d3, d2, false},\n\t\t{d3, d3, false},\n\t}\n\n\tfor i, s := range scenarios {\n\t\tt.Run(fmt.Sprintf(\"before_%d\", i), func(t *testing.T) {\n\t\t\tif v := s.a.Before(s.b); v != s.expect {\n\t\t\t\tt.Fatalf(\"Expected %v, got %v\", s.expect, v)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestDateTimeCompare(t *testing.T) {\n\tt.Parallel()\n\n\td1, _ := types.ParseDateTime(\"2024-01-01 10:00:00.123Z\")\n\td2, _ := types.ParseDateTime(\"2024-01-02 10:00:00.123Z\")\n\td3, _ := types.ParseDateTime(\"2024-01-03 10:00:00.123Z\")\n\n\tscenarios := []struct {\n\t\ta      types.DateTime\n\t\tb      types.DateTime\n\t\texpect int\n\t}{\n\t\t// d1\n\t\t{d1, d1, 0},\n\t\t{d1, d2, -1},\n\t\t{d1, d3, -1},\n\t\t// d2\n\t\t{d2, d1, 1},\n\t\t{d2, d2, 0},\n\t\t{d2, d3, -1},\n\t\t// d3\n\t\t{d3, d1, 1},\n\t\t{d3, d2, 1},\n\t\t{d3, d3, 0},\n\t}\n\n\tfor i, s := range scenarios {\n\t\tt.Run(fmt.Sprintf(\"compare_%d\", i), func(t *testing.T) {\n\t\t\tif v := s.a.Compare(s.b); v != s.expect {\n\t\t\t\tt.Fatalf(\"Expected %v, got %v\", s.expect, v)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestDateTimeEqual(t *testing.T) {\n\tt.Parallel()\n\n\td1, _ := types.ParseDateTime(\"2024-01-01 10:00:00.123Z\")\n\td2, _ := types.ParseDateTime(\"2024-01-01 10:00:00.123Z\")\n\td3, _ := types.ParseDateTime(\"2024-01-01 10:00:00.124Z\")\n\n\tscenarios := []struct {\n\t\ta      types.DateTime\n\t\tb      types.DateTime\n\t\texpect bool\n\t}{\n\t\t{d1, d1, true},\n\t\t{d1, d2, true},\n\t\t{d1, d3, false},\n\t}\n\n\tfor i, s := range scenarios {\n\t\tt.Run(fmt.Sprintf(\"equal_%d\", i), func(t *testing.T) {\n\t\t\tif v := s.a.Equal(s.b); v != s.expect {\n\t\t\t\tt.Fatalf(\"Expected %v, got %v\", s.expect, v)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestDateTimeUnix(t *testing.T) {\n\tscenarios := []struct {\n\t\tdate     string\n\t\texpected int64\n\t}{\n\t\t{\"\", -62135596800},\n\t\t{\"2022-01-01 11:23:45.678Z\", 1641036225},\n\t}\n\n\tfor i, s := range scenarios {\n\t\tt.Run(fmt.Sprintf(\"%d_%s\", i, s.date), func(t *testing.T) {\n\t\t\tdt, err := types.ParseDateTime(s.date)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\tv := dt.Unix()\n\n\t\t\tif v != s.expected {\n\t\t\t\tt.Fatalf(\"Expected %d, got %d\", s.expected, v)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestDateTimeIsZero(t *testing.T) {\n\tdt0 := types.DateTime{}\n\tif !dt0.IsZero() {\n\t\tt.Fatalf(\"Expected zero datatime, got %v\", dt0)\n\t}\n\n\tdt1 := types.NowDateTime()\n\tif dt1.IsZero() {\n\t\tt.Fatalf(\"Expected non-zero datatime, got %v\", dt1)\n\t}\n}\n\nfunc TestDateTimeString(t *testing.T) {\n\tdt0 := types.DateTime{}\n\tif dt0.String() != \"\" {\n\t\tt.Fatalf(\"Expected empty string for zer datetime, got %q\", dt0.String())\n\t}\n\n\texpected := \"2022-01-01 11:23:45.678Z\"\n\tdt1, _ := types.ParseDateTime(expected)\n\tif dt1.String() != expected {\n\t\tt.Fatalf(\"Expected %q, got %v\", expected, dt1)\n\t}\n}\n\nfunc TestDateTimeMarshalJSON(t *testing.T) {\n\tscenarios := []struct {\n\t\tdate     string\n\t\texpected string\n\t}{\n\t\t{\"\", `\"\"`},\n\t\t{\"2022-01-01 11:23:45.678\", `\"2022-01-01 11:23:45.678Z\"`},\n\t}\n\n\tfor i, s := range scenarios {\n\t\tt.Run(fmt.Sprintf(\"%d_%s\", i, s.date), func(t *testing.T) {\n\t\t\tdt, err := types.ParseDateTime(s.date)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\tresult, err := dt.MarshalJSON()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\tif string(result) != s.expected {\n\t\t\t\tt.Fatalf(\"Expected %q, got %q\", s.expected, string(result))\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestDateTimeUnmarshalJSON(t *testing.T) {\n\tscenarios := []struct {\n\t\tdate     string\n\t\texpected string\n\t}{\n\t\t{\"\", \"\"},\n\t\t{\"invalid_json\", \"\"},\n\t\t{\"'123'\", \"\"},\n\t\t{\"2022-01-01 11:23:45.678\", \"\"},\n\t\t{`\"2022-01-01 11:23:45.678\"`, \"2022-01-01 11:23:45.678Z\"},\n\t}\n\n\tfor i, s := range scenarios {\n\t\tt.Run(fmt.Sprintf(\"%d_%s\", i, s.date), func(t *testing.T) {\n\t\t\tdt := types.DateTime{}\n\t\t\tdt.UnmarshalJSON([]byte(s.date))\n\n\t\t\tif dt.String() != s.expected {\n\t\t\t\tt.Fatalf(\"Expected %q, got %q\", s.expected, dt.String())\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestDateTimeValue(t *testing.T) {\n\tscenarios := []struct {\n\t\tvalue    any\n\t\texpected string\n\t}{\n\t\t{\"\", \"\"},\n\t\t{\"invalid\", \"\"},\n\t\t{1641024040, \"2022-01-01 08:00:40.000Z\"},\n\t\t{\"2022-01-01 11:23:45.678\", \"2022-01-01 11:23:45.678Z\"},\n\t\t{types.NowDateTime(), types.NowDateTime().String()},\n\t}\n\n\tfor i, s := range scenarios {\n\t\tt.Run(fmt.Sprintf(\"%d_%s\", i, s.value), func(t *testing.T) {\n\t\t\tdt, _ := types.ParseDateTime(s.value)\n\n\t\t\tresult, err := dt.Value()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\tif result != s.expected {\n\t\t\t\tt.Fatalf(\"Expected %q, got %q\", s.expected, result)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestDateTimeScan(t *testing.T) {\n\tnow := time.Now().UTC().Format(\"2006-01-02 15:04:05\") // without ms part for test consistency\n\n\tscenarios := []struct {\n\t\tvalue    any\n\t\texpected string\n\t}{\n\t\t{nil, \"\"},\n\t\t{\"\", \"\"},\n\t\t{\"invalid\", \"\"},\n\t\t{types.NowDateTime(), now},\n\t\t{time.Now(), now},\n\t\t{1.0, \"\"},\n\t\t{1641024040, \"2022-01-01 08:00:40.000Z\"},\n\t\t{\"2022-01-01 11:23:45.678\", \"2022-01-01 11:23:45.678Z\"},\n\t}\n\n\tfor i, s := range scenarios {\n\t\tt.Run(fmt.Sprintf(\"%d_%#v\", i, s.value), func(t *testing.T) {\n\t\t\tdt := types.DateTime{}\n\n\t\t\terr := dt.Scan(s.value)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Failed to parse %v: %v\", s.value, err)\n\t\t\t}\n\n\t\t\tif !strings.Contains(dt.String(), s.expected) {\n\t\t\t\tt.Fatalf(\"Expected %q, got %q\", s.expected, dt.String())\n\t\t\t}\n\t\t})\n\t}\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: pocketbase\nSource File Path(s): ['pocketbase\\\\db_retry_test.go\\\\source_files\\\\db_retry.go']\nTest File Path: pocketbase\\db_retry_test.go\\db_retry_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: pocketbase\\db_retry_test.go\\source_files\\db_retry.go\npackage core\n\nimport (\n\t\"context\"\n\t\"database/sql\"\n\t\"errors\"\n\t\"fmt\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/pocketbase/dbx\"\n)\n\n// default retries intervals (in ms)\nvar defaultRetryIntervals = []int{50, 100, 150, 200, 300, 400, 500, 700, 1000}\n\n// default max retry attempts\nconst defaultMaxLockRetries = 12\n\nfunc execLockRetry(timeout time.Duration, maxRetries int) dbx.ExecHookFunc {\n\treturn func(q *dbx.Query, op func() error) error {\n\t\tif q.Context() == nil {\n\t\t\tcancelCtx, cancel := context.WithTimeout(context.Background(), timeout)\n\t\t\tdefer func() {\n\t\t\t\tcancel()\n\t\t\t\t//nolint:staticcheck\n\t\t\t\tq.WithContext(nil) // reset\n\t\t\t}()\n\t\t\tq.WithContext(cancelCtx)\n\t\t}\n\n\t\texecErr := baseLockRetry(func(attempt int) error {\n\t\t\treturn op()\n\t\t}, maxRetries)\n\t\tif execErr != nil && !errors.Is(execErr, sql.ErrNoRows) {\n\t\t\texecErr = fmt.Errorf(\"%w; failed query: %s\", execErr, q.SQL())\n\t\t}\n\n\t\treturn execErr\n\t}\n}\n\nfunc baseLockRetry(op func(attempt int) error, maxRetries int) error {\n\tattempt := 1\n\nRetry:\n\terr := op(attempt)\n\n\tif err != nil && attempt <= maxRetries {\n\t\terrStr := err.Error()\n\t\t// we are checking the error against the plain error texts since the codes could vary between drivers\n\t\tif strings.Contains(errStr, \"database is locked\") ||\n\t\t\tstrings.Contains(errStr, \"table is locked\") {\n\t\t\t// wait and retry\n\t\t\ttime.Sleep(getDefaultRetryInterval(attempt))\n\t\t\tattempt++\n\t\t\tgoto Retry\n\t\t}\n\t}\n\n\treturn err\n}\n\nfunc getDefaultRetryInterval(attempt int) time.Duration {\n\tif attempt < 0 || attempt > len(defaultRetryIntervals)-1 {\n\t\treturn time.Duration(defaultRetryIntervals[len(defaultRetryIntervals)-1]) * time.Millisecond\n\t}\n\n\treturn time.Duration(defaultRetryIntervals[attempt]) * time.Millisecond\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:52:19"}, {"role": "assistant", "content": "```go\npackage core\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"testing\"\n)\n\nfunc TestGetDefaultRetryInterval(t *testing.T) {\n\tt.Parallel()\n\n\tif i := getDefaultRetryInterval(-1); i.Milliseconds() != 1000 {\n\t\tt.Fatalf(\"Expected 1000ms, got %v\", i)\n\t}\n\n\tif i := getDefaultRetryInterval(999); i.Milliseconds() != 1000 {\n\t\tt.Fatalf(\"Expected 1000ms, got %v\", i)\n\t}\n\n\tif i := getDefaultRetryInterval(3); i.Milliseconds() != 200 {\n\t\tt.Fatalf(\"Expected 500ms, got %v\", i)\n\t}\n}\n\nfunc TestBaseLockRetry(t *testing.T) {\n\tt.Parallel()\n\n\tscenarios := []struct {\n\t\terr              error\n\t\tfailUntilAttempt int\n\t\texpectedAttempts int\n\t}{\n\t\t{nil, 3, 1},\n\t\t{errors.New(\"test\"), 3, 1},\n\t\t{errors.New(\"database is locked\"), 3, 3},\n\t\t{errors.New(\"table is locked\"), 3, 3},\n\t}\n\n\tfor i, s := range scenarios {\n\t\tt.Run(fmt.Sprintf(\"%d_%#v\", i, s.err), func(t *testing.T) {\n\t\t\tlastAttempt := 0\n\n\t\t\terr := baseLockRetry(func(attempt int) error {\n\t\t\t\tlastAttempt = attempt\n\n\t\t\t\tif attempt < s.failUntilAttempt {\n\t\t\t\t\treturn s.err\n\t\t\t\t}\n\n\t\t\t\treturn nil\n\t\t\t}, s.failUntilAttempt+2)\n\n\t\t\tif lastAttempt != s.expectedAttempts {\n\t\t\t\tt.Errorf(\"Expected lastAttempt to be %d, got %d\", s.expectedAttempts, lastAttempt)\n\t\t\t}\n\n\t\t\tif s.failUntilAttempt == s.expectedAttempts && err != nil {\n\t\t\t\tt.Fatalf(\"Expected nil, got err %v\", err)\n\t\t\t}\n\n\t\t\tif s.failUntilAttempt != s.expectedAttempts && s.err != nil && err == nil {\n\t\t\t\tt.Fatalf(\"Expected error %q, got nil\", s.err)\n\t\t\t}\n\t\t})\n\t}\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: pocketbase\nSource File Path(s): ['pocketbase\\\\db_table_test.go\\\\source_files\\\\db_table.go']\nTest File Path: pocketbase\\db_table_test.go\\db_table_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: pocketbase\\db_table_test.go\\source_files\\db_table.go\npackage core\n\nimport (\n\t\"database/sql\"\n\t\"fmt\"\n\n\t\"github.com/pocketbase/dbx\"\n)\n\n// TableColumns returns all column names of a single table by its name.\nfunc (app *BaseApp) TableColumns(tableName string) ([]string, error) {\n\tcolumns := []string{}\n\n\terr := app.DB().NewQuery(\"SELECT name FROM PRAGMA_TABLE_INFO({:tableName})\").\n\t\tBind(dbx.Params{\"tableName\": tableName}).\n\t\tColumn(&columns)\n\n\treturn columns, err\n}\n\ntype TableInfoRow struct {\n\t// the `db:\"pk\"` tag has special semantic so we cannot rename\n\t// the original field without specifying a custom mapper\n\tPK int\n\n\tIndex        int            `db:\"cid\"`\n\tName         string         `db:\"name\"`\n\tType         string         `db:\"type\"`\n\tNotNull      bool           `db:\"notnull\"`\n\tDefaultValue sql.NullString `db:\"dflt_value\"`\n}\n\n// TableInfo returns the \"table_info\" pragma result for the specified table.\nfunc (app *BaseApp) TableInfo(tableName string) ([]*TableInfoRow, error) {\n\tinfo := []*TableInfoRow{}\n\n\terr := app.DB().NewQuery(\"SELECT * FROM PRAGMA_TABLE_INFO({:tableName})\").\n\t\tBind(dbx.Params{\"tableName\": tableName}).\n\t\tAll(&info)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// mattn/go-sqlite3 doesn't throw an error on invalid or missing table\n\t// so we additionally have to check whether the loaded info result is nonempty\n\tif len(info) == 0 {\n\t\treturn nil, fmt.Errorf(\"empty table info probably due to invalid or missing table %s\", tableName)\n\t}\n\n\treturn info, nil\n}\n\n// TableIndexes returns a name grouped map with all non empty index of the specified table.\n//\n// Note: This method doesn't return an error on nonexisting table.\nfunc (app *BaseApp) TableIndexes(tableName string) (map[string]string, error) {\n\tindexes := []struct {\n\t\tName string\n\t\tSql  string\n\t}{}\n\n\terr := app.DB().Select(\"name\", \"sql\").\n\t\tFrom(\"sqlite_master\").\n\t\tAndWhere(dbx.NewExp(\"sql is not null\")).\n\t\tAndWhere(dbx.HashExp{\n\t\t\t\"type\":     \"index\",\n\t\t\t\"tbl_name\": tableName,\n\t\t}).\n\t\tAll(&indexes)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tresult := make(map[string]string, len(indexes))\n\n\tfor _, idx := range indexes {\n\t\tresult[idx.Name] = idx.Sql\n\t}\n\n\treturn result, nil\n}\n\n// DeleteTable drops the specified table.\n//\n// This method is a no-op if a table with the provided name doesn't exist.\n//\n// NB! Be aware that this method is vulnerable to SQL injection and the\n// \"tableName\" argument must come only from trusted input!\nfunc (app *BaseApp) DeleteTable(tableName string) error {\n\t_, err := app.DB().NewQuery(fmt.Sprintf(\n\t\t\"DROP TABLE IF EXISTS {{%s}}\",\n\t\ttableName,\n\t)).Execute()\n\n\treturn err\n}\n\n// HasTable checks if a table (or view) with the provided name exists (case insensitive).\n// in the current app.DB() instance.\nfunc (app *BaseApp) HasTable(tableName string) bool {\n\treturn app.hasTable(app.DB(), tableName)\n}\n\n// AuxHasTable checks if a table (or view) with the provided name exists (case insensitive)\n// in the current app.AuxDB() instance.\nfunc (app *BaseApp) AuxHasTable(tableName string) bool {\n\treturn app.hasTable(app.AuxDB(), tableName)\n}\n\nfunc (app *BaseApp) hasTable(db dbx.Builder, tableName string) bool {\n\tvar exists int\n\n\terr := db.Select(\"(1)\").\n\t\tFrom(\"sqlite_schema\").\n\t\tAndWhere(dbx.HashExp{\"type\": []any{\"table\", \"view\"}}).\n\t\tAndWhere(dbx.NewExp(\"LOWER([[name]])=LOWER({:tableName})\", dbx.Params{\"tableName\": tableName})).\n\t\tLimit(1).\n\t\tRow(&exists)\n\n\treturn err == nil && exists > 0\n}\n\n// Vacuum executes VACUUM on the current app.DB() instance\n// in order to reclaim unused data db disk space.\nfunc (app *BaseApp) Vacuum() error {\n\treturn app.vacuum(app.DB())\n}\n\n// AuxVacuum executes VACUUM on the current app.AuxDB() instance\n// in order to reclaim unused auxiliary db disk space.\nfunc (app *BaseApp) AuxVacuum() error {\n\treturn app.vacuum(app.AuxDB())\n}\n\nfunc (app *BaseApp) vacuum(db dbx.Builder) error {\n\t_, err := db.NewQuery(\"VACUUM\").Execute()\n\n\treturn err\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:52:19"}, {"role": "assistant", "content": "```go\npackage core_test\n\nimport (\n\t\"context\"\n\t\"database/sql\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"slices\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/pocketbase/dbx\"\n\t\"github.com/pocketbase/pocketbase/core\"\n\t\"github.com/pocketbase/pocketbase/tests\"\n)\n\nfunc TestHasTable(t *testing.T) {\n\tt.Parallel()\n\n\tapp, _ := tests.NewTestApp()\n\tdefer app.Cleanup()\n\n\tscenarios := []struct {\n\t\ttableName string\n\t\texpected  bool\n\t}{\n\t\t{\"\", false},\n\t\t{\"test\", false},\n\t\t{core.CollectionNameSuperusers, true},\n\t\t{\"demo3\", true},\n\t\t{\"DEMO3\", true}, // table names are case insensitives by default\n\t\t{\"view1\", true}, // view\n\t}\n\n\tfor _, s := range scenarios {\n\t\tt.Run(s.tableName, func(t *testing.T) {\n\t\t\tresult := app.HasTable(s.tableName)\n\t\t\tif result != s.expected {\n\t\t\t\tt.Fatalf(\"Expected %v, got %v\", s.expected, result)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestAuxHasTable(t *testing.T) {\n\tt.Parallel()\n\n\tapp, _ := tests.NewTestApp()\n\tdefer app.Cleanup()\n\n\tscenarios := []struct {\n\t\ttableName string\n\t\texpected  bool\n\t}{\n\t\t{\"\", false},\n\t\t{\"test\", false},\n\t\t{\"_lOGS\", true}, // table names are case insensitives by default\n\t}\n\n\tfor _, s := range scenarios {\n\t\tt.Run(s.tableName, func(t *testing.T) {\n\t\t\tresult := app.AuxHasTable(s.tableName)\n\t\t\tif result != s.expected {\n\t\t\t\tt.Fatalf(\"Expected %v, got %v\", s.expected, result)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestTableColumns(t *testing.T) {\n\tt.Parallel()\n\n\tapp, _ := tests.NewTestApp()\n\tdefer app.Cleanup()\n\n\tscenarios := []struct {\n\t\ttableName string\n\t\texpected  []string\n\t}{\n\t\t{\"\", nil},\n\t\t{\"_params\", []string{\"id\", \"value\", \"created\", \"updated\"}},\n\t}\n\n\tfor i, s := range scenarios {\n\t\tt.Run(fmt.Sprintf(\"%d_%s\", i, s.tableName), func(t *testing.T) {\n\t\t\tcolumns, _ := app.TableColumns(s.tableName)\n\n\t\t\tif len(columns) != len(s.expected) {\n\t\t\t\tt.Fatalf(\"Expected columns %v, got %v\", s.expected, columns)\n\t\t\t}\n\n\t\t\tfor _, c := range columns {\n\t\t\t\tif !slices.Contains(s.expected, c) {\n\t\t\t\t\tt.Errorf(\"Didn't expect column %s\", c)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestTableInfo(t *testing.T) {\n\tt.Parallel()\n\n\tapp, _ := tests.NewTestApp()\n\tdefer app.Cleanup()\n\n\tscenarios := []struct {\n\t\ttableName string\n\t\texpected  string\n\t}{\n\t\t{\"\", \"null\"},\n\t\t{\"missing\", \"null\"},\n\t\t{\n\t\t\t\"_params\",\n\t\t\t`[{\"PK\":0,\"Index\":0,\"Name\":\"created\",\"Type\":\"TEXT\",\"NotNull\":true,\"DefaultValue\":{\"String\":\"''\",\"Valid\":true}},{\"PK\":1,\"Index\":1,\"Name\":\"id\",\"Type\":\"TEXT\",\"NotNull\":true,\"DefaultValue\":{\"String\":\"'r'||lower(hex(randomblob(7)))\",\"Valid\":true}},{\"PK\":0,\"Index\":2,\"Name\":\"updated\",\"Type\":\"TEXT\",\"NotNull\":true,\"DefaultValue\":{\"String\":\"''\",\"Valid\":true}},{\"PK\":0,\"Index\":3,\"Name\":\"value\",\"Type\":\"JSON\",\"NotNull\":false,\"DefaultValue\":{\"String\":\"NULL\",\"Valid\":true}}]`,\n\t\t},\n\t}\n\n\tfor i, s := range scenarios {\n\t\tt.Run(fmt.Sprintf(\"%d_%s\", i, s.tableName), func(t *testing.T) {\n\t\t\trows, _ := app.TableInfo(s.tableName)\n\n\t\t\traw, err := json.Marshal(rows)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\n\t\t\tif str := string(raw); str != s.expected {\n\t\t\t\tt.Fatalf(\"Expected\\n%s\\ngot\\n%s\", s.expected, str)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestTableIndexes(t *testing.T) {\n\tt.Parallel()\n\n\tapp, _ := tests.NewTestApp()\n\tdefer app.Cleanup()\n\n\tscenarios := []struct {\n\t\ttableName string\n\t\texpected  []string\n\t}{\n\t\t{\"\", nil},\n\t\t{\"missing\", nil},\n\t\t{\n\t\t\tcore.CollectionNameSuperusers,\n\t\t\t[]string{\"idx_email__pbc_3323866339\", \"idx_tokenKey__pbc_3323866339\"},\n\t\t},\n\t}\n\n\tfor i, s := range scenarios {\n\t\tt.Run(fmt.Sprintf(\"%d_%s\", i, s.tableName), func(t *testing.T) {\n\t\t\tindexes, _ := app.TableIndexes(s.tableName)\n\n\t\t\tif len(indexes) != len(s.expected) {\n\t\t\t\tt.Fatalf(\"Expected %d indexes, got %d\\n%v\", len(s.expected), len(indexes), indexes)\n\t\t\t}\n\n\t\t\tfor _, name := range s.expected {\n\t\t\t\tif v, ok := indexes[name]; !ok || v == \"\" {\n\t\t\t\t\tt.Fatalf(\"Expected non-empty index %q in \\n%v\", name, indexes)\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestDeleteTable(t *testing.T) {\n\tt.Parallel()\n\n\tapp, _ := tests.NewTestApp()\n\tdefer app.Cleanup()\n\n\tscenarios := []struct {\n\t\ttableName   string\n\t\texpectError bool\n\t}{\n\t\t{\"\", true},\n\t\t{\"test\", false}, // missing tables are ignored\n\t\t{\"_admins\", false},\n\t\t{\"demo3\", false},\n\t}\n\n\tfor i, s := range scenarios {\n\t\tt.Run(fmt.Sprintf(\"%d_%s\", i, s.tableName), func(t *testing.T) {\n\t\t\terr := app.DeleteTable(s.tableName)\n\n\t\t\thasErr := err != nil\n\t\t\tif hasErr != s.expectError {\n\t\t\t\tt.Fatalf(\"Expected hasErr %v, got %v\", s.expectError, hasErr)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestVacuum(t *testing.T) {\n\tt.Parallel()\n\n\tapp, _ := tests.NewTestApp()\n\tdefer app.Cleanup()\n\n\tcalledQueries := []string{}\n\tapp.DB().(*dbx.DB).QueryLogFunc = func(ctx context.Context, t time.Duration, sql string, rows *sql.Rows, err error) {\n\t\tcalledQueries = append(calledQueries, sql)\n\t}\n\tapp.DB().(*dbx.DB).ExecLogFunc = func(ctx context.Context, t time.Duration, sql string, result sql.Result, err error) {\n\t\tcalledQueries = append(calledQueries, sql)\n\t}\n\n\tif err := app.Vacuum(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif total := len(calledQueries); total != 1 {\n\t\tt.Fatalf(\"Expected 1 query, got %d\", total)\n\t}\n\n\tif calledQueries[0] != \"VACUUM\" {\n\t\tt.Fatalf(\"Expected VACUUM query, got %s\", calledQueries[0])\n\t}\n}\n\nfunc TestAuxVacuum(t *testing.T) {\n\tt.Parallel()\n\n\tapp, _ := tests.NewTestApp()\n\tdefer app.Cleanup()\n\n\tcalledQueries := []string{}\n\tapp.AuxDB().(*dbx.DB).QueryLogFunc = func(ctx context.Context, t time.Duration, sql string, rows *sql.Rows, err error) {\n\t\tcalledQueries = append(calledQueries, sql)\n\t}\n\tapp.AuxDB().(*dbx.DB).ExecLogFunc = func(ctx context.Context, t time.Duration, sql string, result sql.Result, err error) {\n\t\tcalledQueries = append(calledQueries, sql)\n\t}\n\n\tif err := app.AuxVacuum(); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif total := len(calledQueries); total != 1 {\n\t\tt.Fatalf(\"Expected 1 query, got %d\", total)\n\t}\n\n\tif calledQueries[0] != \"VACUUM\" {\n\t\tt.Fatalf(\"Expected VACUUM query, got %s\", calledQueries[0])\n\t}\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: pocketbase\nSource File Path(s): ['pocketbase\\\\event_test.go\\\\source_files\\\\event.go']\nTest File Path: pocketbase\\event_test.go\\event_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: pocketbase\\event_test.go\\source_files\\event.go\npackage hook\n\n// Resolver defines a common interface for a Hook event (see [Event]).\ntype Resolver interface {\n\t// Next triggers the next handler in the hook's chain (if any).\n\tNext() error\n\n\t// note: kept only for the generic interface; may get removed in the future\n\tnextFunc() func() error\n\tsetNextFunc(f func() error)\n}\n\nvar _ Resolver = (*Event)(nil)\n\n// Event implements [Resolver] and it is intended to be used as a base\n// Hook event that you can embed in your custom typed event structs.\n//\n// Example:\n//\n//\ttype CustomEvent struct {\n//\t\thook.Event\n//\n//\t\tSomeField int\n//\t}\ntype Event struct {\n\tnext func() error\n}\n\n// Next calls the next hook handler.\nfunc (e *Event) Next() error {\n\tif e.next != nil {\n\t\treturn e.next()\n\t}\n\treturn nil\n}\n\n// nextFunc returns the function that Next calls.\nfunc (e *Event) nextFunc() func() error {\n\treturn e.next\n}\n\n// setNextFunc sets the function that Next calls.\nfunc (e *Event) setNextFunc(f func() error) {\n\te.next = f\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:52:20"}, {"role": "assistant", "content": "```go\npackage hook\n\nimport \"testing\"\n\nfunc TestEventNext(t *testing.T) {\n\tcalls := 0\n\n\te := Event{}\n\n\tif e.nextFunc() != nil {\n\t\tt.Fatalf(\"Expected nextFunc to be nil\")\n\t}\n\n\te.setNextFunc(func() error {\n\t\tcalls++\n\t\treturn nil\n\t})\n\n\tif e.nextFunc() == nil {\n\t\tt.Fatalf(\"Expected nextFunc to be non-nil\")\n\t}\n\n\te.Next()\n\te.Next()\n\n\tif calls != 2 {\n\t\tt.Fatalf(\"Expected %d calls, got %d\", 2, calls)\n\t}\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: pocketbase\nSource File Path(s): ['pocketbase\\\\migrations_list_test.go\\\\source_files\\\\migrations_list.go']\nTest File Path: pocketbase\\migrations_list_test.go\\migrations_list_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: pocketbase\\migrations_list_test.go\\source_files\\migrations_list.go\npackage core\n\nimport (\n\t\"path/filepath\"\n\t\"runtime\"\n\t\"sort\"\n)\n\ntype Migration struct {\n\tUp               func(txApp App) error\n\tDown             func(txApp App) error\n\tFile             string\n\tReapplyCondition func(txApp App, runner *MigrationsRunner, fileName string) (bool, error)\n}\n\n// MigrationsList defines a list with migration definitions\ntype MigrationsList struct {\n\tlist []*Migration\n}\n\n// Item returns a single migration from the list by its index.\nfunc (l *MigrationsList) Item(index int) *Migration {\n\treturn l.list[index]\n}\n\n// Items returns the internal migrations list slice.\nfunc (l *MigrationsList) Items() []*Migration {\n\treturn l.list\n}\n\n// Copy copies all provided list migrations into the current one.\nfunc (l *MigrationsList) Copy(list MigrationsList) {\n\tfor _, item := range list.Items() {\n\t\tl.Register(item.Up, item.Down, item.File)\n\t}\n}\n\n// Add adds adds an existing migration definition to the list.\n//\n// If m.File is not provided, it will try to get the name from its .go file.\n//\n// The list will be sorted automatically based on the migrations file name.\nfunc (l *MigrationsList) Add(m *Migration) {\n\tif m.File == \"\" {\n\t\t_, path, _, _ := runtime.Caller(1)\n\t\tm.File = filepath.Base(path)\n\t}\n\n\tl.list = append(l.list, m)\n\n\tsort.SliceStable(l.list, func(i int, j int) bool {\n\t\treturn l.list[i].File < l.list[j].File\n\t})\n}\n\n// Register adds new migration definition to the list.\n//\n// If optFilename is not provided, it will try to get the name from its .go file.\n//\n// The list will be sorted automatically based on the migrations file name.\nfunc (l *MigrationsList) Register(\n\tup func(txApp App) error,\n\tdown func(txApp App) error,\n\toptFilename ...string,\n) {\n\tvar file string\n\tif len(optFilename) > 0 {\n\t\tfile = optFilename[0]\n\t} else {\n\t\t_, path, _, _ := runtime.Caller(1)\n\t\tfile = filepath.Base(path)\n\t}\n\n\tl.list = append(l.list, &Migration{\n\t\tFile: file,\n\t\tUp:   up,\n\t\tDown: down,\n\t})\n\n\tsort.SliceStable(l.list, func(i int, j int) bool {\n\t\treturn l.list[i].File < l.list[j].File\n\t})\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:52:20"}, {"role": "assistant", "content": "```go\npackage core_test\n\nimport (\n\t\"testing\"\n\n\t\"github.com/pocketbase/pocketbase/core\"\n)\n\nfunc TestMigrationsList(t *testing.T) {\n\tl1 := core.MigrationsList{}\n\tl1.Add(&core.Migration{File: \"5_test.go\"})\n\tl1.Add(&core.Migration{ /* auto detect file name */ })\n\tl1.Register(nil, nil, \"3_test.go\")\n\tl1.Register(nil, nil, \"1_test.go\")\n\tl1.Register(nil, nil, \"2_test.go\")\n\tl1.Register(nil, nil /* auto detect file name */)\n\n\tl2 := core.MigrationsList{}\n\tl2.Register(nil, nil, \"4_test.go\")\n\tl2.Copy(l1)\n\n\texpected := []string{\n\t\t\"1_test.go\",\n\t\t\"2_test.go\",\n\t\t\"3_test.go\",\n\t\t\"4_test.go\",\n\t\t\"5_test.go\",\n\t\t// twice because there 2 test migrations with auto filename\n\t\t\"migrations_list_test.go\",\n\t\t\"migrations_list_test.go\",\n\t}\n\n\titems := l2.Items()\n\tif len(items) != len(expected) {\n\t\tnames := make([]string, len(items))\n\t\tfor i, item := range items {\n\t\t\tnames[i] = item.File\n\t\t}\n\t\tt.Fatalf(\"Expected %d items, got %d:\\n%v\", len(expected), len(names), names)\n\t}\n\n\tfor i, name := range expected {\n\t\titem := l2.Item(i)\n\t\tif item.File != name {\n\t\t\tt.Fatalf(\"Expected name %s for index %d, got %s\", name, i, item.File)\n\t\t}\n\t}\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: pocketbase\nSource File Path(s): ['pocketbase\\\\store_test.go\\\\source_files\\\\store.go']\nTest File Path: pocketbase\\store_test.go\\store_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: pocketbase\\store_test.go\\source_files\\store.go\npackage store\n\nimport (\n\t\"encoding/json\"\n\t\"sync\"\n)\n\n// @todo remove after https://github.com/golang/go/issues/20135\nconst ShrinkThreshold = 200 // the number is arbitrary chosen\n\n// Store defines a concurrent safe in memory key-value data store.\ntype Store[K comparable, T any] struct {\n\tdata    map[K]T\n\tmu      sync.RWMutex\n\tdeleted int64\n}\n\n// New creates a new Store[T] instance with a shallow copy of the provided data (if any).\nfunc New[K comparable, T any](data map[K]T) *Store[K, T] {\n\ts := &Store[K, T]{}\n\n\ts.Reset(data)\n\n\treturn s\n}\n\n// Reset clears the store and replaces the store data with a\n// shallow copy of the provided newData.\nfunc (s *Store[K, T]) Reset(newData map[K]T) {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\tif len(newData) > 0 {\n\t\ts.data = make(map[K]T, len(newData))\n\t\tfor k, v := range newData {\n\t\t\ts.data[k] = v\n\t\t}\n\t} else {\n\t\ts.data = make(map[K]T)\n\t}\n\n\ts.deleted = 0\n}\n\n// Length returns the current number of elements in the store.\nfunc (s *Store[K, T]) Length() int {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\n\treturn len(s.data)\n}\n\n// RemoveAll removes all the existing store entries.\nfunc (s *Store[K, T]) RemoveAll() {\n\ts.Reset(nil)\n}\n\n// Remove removes a single entry from the store.\n//\n// Remove does nothing if key doesn't exist in the store.\nfunc (s *Store[K, T]) Remove(key K) {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\tdelete(s.data, key)\n\ts.deleted++\n\n\t// reassign to a new map so that the old one can be gc-ed because it doesn't shrink\n\t//\n\t// @todo remove after https://github.com/golang/go/issues/20135\n\tif s.deleted >= ShrinkThreshold {\n\t\tnewData := make(map[K]T, len(s.data))\n\t\tfor k, v := range s.data {\n\t\t\tnewData[k] = v\n\t\t}\n\t\ts.data = newData\n\t\ts.deleted = 0\n\t}\n}\n\n// Has checks if element with the specified key exist or not.\nfunc (s *Store[K, T]) Has(key K) bool {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\n\t_, ok := s.data[key]\n\n\treturn ok\n}\n\n// Get returns a single element value from the store.\n//\n// If key is not set, the zero T value is returned.\nfunc (s *Store[K, T]) Get(key K) T {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\n\treturn s.data[key]\n}\n\n// GetOk is similar to Get but returns also a boolean indicating whether the key exists or not.\nfunc (s *Store[K, T]) GetOk(key K) (T, bool) {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\n\tv, ok := s.data[key]\n\n\treturn v, ok\n}\n\n// GetAll returns a shallow copy of the current store data.\nfunc (s *Store[K, T]) GetAll() map[K]T {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\n\tvar clone = make(map[K]T, len(s.data))\n\n\tfor k, v := range s.data {\n\t\tclone[k] = v\n\t}\n\n\treturn clone\n}\n\n// Values returns a slice with all of the current store values.\nfunc (s *Store[K, T]) Values() []T {\n\ts.mu.RLock()\n\tdefer s.mu.RUnlock()\n\n\tvar values = make([]T, 0, len(s.data))\n\n\tfor _, v := range s.data {\n\t\tvalues = append(values, v)\n\t}\n\n\treturn values\n}\n\n// Set sets (or overwrite if already exist) a new value for key.\nfunc (s *Store[K, T]) Set(key K, value T) {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\tif s.data == nil {\n\t\ts.data = make(map[K]T)\n\t}\n\n\ts.data[key] = value\n}\n\n// GetOrSet retrieves a single existing value for the provided key\n// or stores a new one if it doesn't exist.\nfunc (s *Store[K, T]) GetOrSet(key K, setFunc func() T) T {\n\t// lock only reads to minimize locks contention\n\ts.mu.RLock()\n\tv, ok := s.data[key]\n\ts.mu.RUnlock()\n\n\tif !ok {\n\t\ts.mu.Lock()\n\t\tv = setFunc()\n\t\tif s.data == nil {\n\t\t\ts.data = make(map[K]T)\n\t\t}\n\t\ts.data[key] = v\n\t\ts.mu.Unlock()\n\t}\n\n\treturn v\n}\n\n// SetIfLessThanLimit sets (or overwrite if already exist) a new value for key.\n//\n// This method is similar to Set() but **it will skip adding new elements**\n// to the store if the store length has reached the specified limit.\n// false is returned if maxAllowedElements limit is reached.\nfunc (s *Store[K, T]) SetIfLessThanLimit(key K, value T, maxAllowedElements int) bool {\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\tif s.data == nil {\n\t\ts.data = make(map[K]T)\n\t}\n\n\t// check for existing item\n\t_, ok := s.data[key]\n\n\tif !ok && len(s.data) >= maxAllowedElements {\n\t\t// cannot add more items\n\t\treturn false\n\t}\n\n\t// add/overwrite item\n\ts.data[key] = value\n\n\treturn true\n}\n\n// UnmarshalJSON implements [json.Unmarshaler] and imports the\n// provided JSON data into the store.\n//\n// The store entries that match with the ones from the data will be overwritten with the new value.\nfunc (s *Store[K, T]) UnmarshalJSON(data []byte) error {\n\traw := map[K]T{}\n\tif err := json.Unmarshal(data, &raw); err != nil {\n\t\treturn err\n\t}\n\n\ts.mu.Lock()\n\tdefer s.mu.Unlock()\n\n\tif s.data == nil {\n\t\ts.data = make(map[K]T)\n\t}\n\n\tfor k, v := range raw {\n\t\ts.data[k] = v\n\t}\n\n\treturn nil\n}\n\n// MarshalJSON implements [json.Marshaler] and export the current\n// store data into valid JSON.\nfunc (s *Store[K, T]) MarshalJSON() ([]byte, error) {\n\treturn json.Marshal(s.GetAll())\n}\n\n### Source File Dependency Files Content\n### Dependency File: empty.go\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:52:20"}, {"role": "assistant", "content": "```go\npackage store_test\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\t\"slices\"\n\t\"strconv\"\n\t\"testing\"\n\n\t\"github.com/pocketbase/pocketbase/tools/store\"\n)\n\nfunc TestNew(t *testing.T) {\n\tdata := map[string]int{\"test1\": 1, \"test2\": 2}\n\toriginalRawData, err := json.Marshal(data)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ts := store.New(data)\n\ts.Set(\"test3\", 3) // add 1 item\n\ts.Remove(\"test1\") // remove 1 item\n\n\t// check if data was shallow copied\n\trawData, _ := json.Marshal(data)\n\tif !bytes.Equal(originalRawData, rawData) {\n\t\tt.Fatalf(\"Expected data \\n%s, \\ngot \\n%s\", originalRawData, rawData)\n\t}\n\n\tif s.Has(\"test1\") {\n\t\tt.Fatalf(\"Expected test1 to be deleted, got %v\", s.Get(\"test1\"))\n\t}\n\n\tif v := s.Get(\"test2\"); v != 2 {\n\t\tt.Fatalf(\"Expected test2 to be %v, got %v\", 2, v)\n\t}\n\n\tif v := s.Get(\"test3\"); v != 3 {\n\t\tt.Fatalf(\"Expected test3 to be %v, got %v\", 3, v)\n\t}\n}\n\nfunc TestReset(t *testing.T) {\n\ts := store.New(map[string]int{\"test1\": 1})\n\n\tdata := map[string]int{\"test2\": 2}\n\toriginalRawData, err := json.Marshal(data)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\ts.Reset(data)\n\ts.Set(\"test3\", 3)\n\n\t// check if data was shallow copied\n\trawData, _ := json.Marshal(data)\n\tif !bytes.Equal(originalRawData, rawData) {\n\t\tt.Fatalf(\"Expected data \\n%s, \\ngot \\n%s\", originalRawData, rawData)\n\t}\n\n\tif s.Has(\"test1\") {\n\t\tt.Fatalf(\"Expected test1 to be deleted, got %v\", s.Get(\"test1\"))\n\t}\n\n\tif v := s.Get(\"test2\"); v != 2 {\n\t\tt.Fatalf(\"Expected test2 to be %v, got %v\", 2, v)\n\t}\n\n\tif v := s.Get(\"test3\"); v != 3 {\n\t\tt.Fatalf(\"Expected test3 to be %v, got %v\", 3, v)\n\t}\n}\n\nfunc TestLength(t *testing.T) {\n\ts := store.New(map[string]int{\"test1\": 1})\n\ts.Set(\"test2\", 2)\n\n\tif v := s.Length(); v != 2 {\n\t\tt.Fatalf(\"Expected length %d, got %d\", 2, v)\n\t}\n}\n\nfunc TestRemoveAll(t *testing.T) {\n\ts := store.New(map[string]bool{\"test1\": true, \"test2\": true})\n\n\tkeys := []string{\"test1\", \"test2\"}\n\n\ts.RemoveAll()\n\n\tfor i, key := range keys {\n\t\tif s.Has(key) {\n\t\t\tt.Errorf(\"(%d) Expected %q to be removed\", i, key)\n\t\t}\n\t}\n}\n\nfunc TestRemove(t *testing.T) {\n\ts := store.New(map[string]bool{\"test\": true})\n\n\tkeys := []string{\"test\", \"missing\"}\n\n\tfor i, key := range keys {\n\t\ts.Remove(key)\n\t\tif s.Has(key) {\n\t\t\tt.Errorf(\"(%d) Expected %q to be removed\", i, key)\n\t\t}\n\t}\n}\n\nfunc TestHas(t *testing.T) {\n\ts := store.New(map[string]int{\"test1\": 0, \"test2\": 1})\n\n\tscenarios := []struct {\n\t\tkey   string\n\t\texist bool\n\t}{\n\t\t{\"test1\", true},\n\t\t{\"test2\", true},\n\t\t{\"missing\", false},\n\t}\n\n\tfor i, scenario := range scenarios {\n\t\texist := s.Has(scenario.key)\n\t\tif exist != scenario.exist {\n\t\t\tt.Errorf(\"(%d) Expected %v, got %v\", i, scenario.exist, exist)\n\t\t}\n\t}\n}\n\nfunc TestGet(t *testing.T) {\n\ts := store.New(map[string]int{\"test1\": 0, \"test2\": 1})\n\n\tscenarios := []struct {\n\t\tkey    string\n\t\texpect int\n\t}{\n\t\t{\"test1\", 0},\n\t\t{\"test2\", 1},\n\t\t{\"missing\", 0}, // should auto fallback to the zero value\n\t}\n\n\tfor _, scenario := range scenarios {\n\t\tt.Run(scenario.key, func(t *testing.T) {\n\t\t\tval := s.Get(scenario.key)\n\t\t\tif val != scenario.expect {\n\t\t\t\tt.Fatalf(\"Expected %v, got %v\", scenario.expect, val)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestGetOk(t *testing.T) {\n\ts := store.New(map[string]int{\"test1\": 0, \"test2\": 1})\n\n\tscenarios := []struct {\n\t\tkey         string\n\t\texpectValue int\n\t\texpectOk    bool\n\t}{\n\t\t{\"test1\", 0, true},\n\t\t{\"test2\", 1, true},\n\t\t{\"missing\", 0, false}, // should auto fallback to the zero value\n\t}\n\n\tfor _, scenario := range scenarios {\n\t\tt.Run(scenario.key, func(t *testing.T) {\n\t\t\tval, ok := s.GetOk(scenario.key)\n\n\t\t\tif ok != scenario.expectOk {\n\t\t\t\tt.Fatalf(\"Expected ok %v, got %v\", scenario.expectOk, ok)\n\t\t\t}\n\n\t\t\tif val != scenario.expectValue {\n\t\t\t\tt.Fatalf(\"Expected %v, got %v\", scenario.expectValue, val)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestGetAll(t *testing.T) {\n\tdata := map[string]int{\n\t\t\"a\": 1,\n\t\t\"b\": 2,\n\t}\n\n\ts := store.New(data)\n\n\t// fetch and delete each key to make sure that it was shallow copied\n\tresult := s.GetAll()\n\tfor k := range result {\n\t\tdelete(result, k)\n\t}\n\n\t// refetch again\n\tresult = s.GetAll()\n\n\tif len(result) != len(data) {\n\t\tt.Fatalf(\"Expected %d, got %d items\", len(data), len(result))\n\t}\n\n\tfor k := range result {\n\t\tif result[k] != data[k] {\n\t\t\tt.Fatalf(\"Expected %s to be %v, got %v\", k, data[k], result[k])\n\t\t}\n\t}\n}\n\nfunc TestValues(t *testing.T) {\n\tdata := map[string]int{\n\t\t\"a\": 1,\n\t\t\"b\": 2,\n\t}\n\n\tvalues := store.New(data).Values()\n\n\texpected := []int{1, 2}\n\n\tif len(values) != len(expected) {\n\t\tt.Fatalf(\"Expected %d values, got %d\", len(expected), len(values))\n\t}\n\n\tfor _, v := range expected {\n\t\tif !slices.Contains(values, v) {\n\t\t\tt.Fatalf(\"Missing value %v in\\n%v\", v, values)\n\t\t}\n\t}\n}\n\nfunc TestSet(t *testing.T) {\n\ts := store.Store[string, int]{}\n\n\tdata := map[string]int{\"test1\": 0, \"test2\": 1, \"test3\": 3}\n\n\t// set values\n\tfor k, v := range data {\n\t\ts.Set(k, v)\n\t}\n\n\t// verify that the values are set\n\tfor k, v := range data {\n\t\tif !s.Has(k) {\n\t\t\tt.Errorf(\"Expected key %q\", k)\n\t\t}\n\n\t\tval := s.Get(k)\n\t\tif val != v {\n\t\t\tt.Errorf(\"Expected %v, got %v for key %q\", v, val, k)\n\t\t}\n\t}\n}\n\nfunc TestGetOrSet(t *testing.T) {\n\ts := store.New(map[string]int{\n\t\t\"test1\": 0,\n\t\t\"test2\": 1,\n\t\t\"test3\": 3,\n\t})\n\n\tscenarios := []struct {\n\t\tkey      string\n\t\tvalue    int\n\t\texpected int\n\t}{\n\t\t{\"test2\", 20, 1},\n\t\t{\"test3\", 2, 3},\n\t\t{\"test_new\", 20, 20},\n\t\t{\"test_new\", 50, 20}, // should return the previously inserted value\n\t}\n\n\tfor _, scenario := range scenarios {\n\t\tt.Run(scenario.key, func(t *testing.T) {\n\t\t\tresult := s.GetOrSet(scenario.key, func() int {\n\t\t\t\treturn scenario.value\n\t\t\t})\n\n\t\t\tif result != scenario.expected {\n\t\t\t\tt.Fatalf(\"Expected %v, got %v\", scenario.expected, result)\n\t\t\t}\n\t\t})\n\t}\n}\n\nfunc TestSetIfLessThanLimit(t *testing.T) {\n\ts := store.Store[string, int]{}\n\n\tlimit := 2\n\n\t// set values\n\tscenarios := []struct {\n\t\tkey      string\n\t\tvalue    int\n\t\texpected bool\n\t}{\n\t\t{\"test1\", 1, true},\n\t\t{\"test2\", 2, true},\n\t\t{\"test3\", 3, false},\n\t\t{\"test2\", 4, true}, // overwrite\n\t}\n\n\tfor i, scenario := range scenarios {\n\t\tresult := s.SetIfLessThanLimit(scenario.key, scenario.value, limit)\n\n\t\tif result != scenario.expected {\n\t\t\tt.Errorf(\"(%d) Expected result %v, got %v\", i, scenario.expected, result)\n\t\t}\n\n\t\tif !scenario.expected && s.Has(scenario.key) {\n\t\t\tt.Errorf(\"(%d) Expected key %q to not be set\", i, scenario.key)\n\t\t}\n\n\t\tval := s.Get(scenario.key)\n\t\tif scenario.expected && val != scenario.value {\n\t\t\tt.Errorf(\"(%d) Expected value %v, got %v\", i, scenario.value, val)\n\t\t}\n\t}\n}\n\nfunc TestUnmarshalJSON(t *testing.T) {\n\ts := store.Store[string, string]{}\n\ts.Set(\"b\", \"old\")   // should be overwritten\n\ts.Set(\"c\", \"test3\") // ensures that the old values are not removed\n\n\traw := []byte(`{\"a\":\"test1\", \"b\":\"test2\"}`)\n\tif err := json.Unmarshal(raw, &s); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif v := s.Get(\"a\"); v != \"test1\" {\n\t\tt.Fatalf(\"Expected store.a to be %q, got %q\", \"test1\", v)\n\t}\n\n\tif v := s.Get(\"b\"); v != \"test2\" {\n\t\tt.Fatalf(\"Expected store.b to be %q, got %q\", \"test2\", v)\n\t}\n\n\tif v := s.Get(\"c\"); v != \"test3\" {\n\t\tt.Fatalf(\"Expected store.c to be %q, got %q\", \"test3\", v)\n\t}\n}\n\nfunc TestMarshalJSON(t *testing.T) {\n\ts := &store.Store[string, string]{}\n\ts.Set(\"a\", \"test1\")\n\ts.Set(\"b\", \"test2\")\n\n\texpected := []byte(`{\"a\":\"test1\", \"b\":\"test2\"}`)\n\n\tresult, err := json.Marshal(s)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tif bytes.Equal(result, expected) {\n\t\tt.Fatalf(\"Expected\\n%s\\ngot\\n%s\", expected, result)\n\t}\n}\n\nfunc TestShrink(t *testing.T) {\n\ts := &store.Store[string, int]{}\n\n\ttotal := 1000\n\n\tfor i := 0; i < total; i++ {\n\t\ts.Set(strconv.Itoa(i), i)\n\t}\n\n\tif s.Length() != total {\n\t\tt.Fatalf(\"Expected %d items, got %d\", total, s.Length())\n\t}\n\n\t// trigger map \"shrink\"\n\tfor i := 0; i < store.ShrinkThreshold; i++ {\n\t\ts.Remove(strconv.Itoa(i))\n\t}\n\n\t// ensure that after the deletion, the new map was copied properly\n\tif s.Length() != total-store.ShrinkThreshold {\n\t\tt.Fatalf(\"Expected %d items, got %d\", total-store.ShrinkThreshold, s.Length())\n\t}\n\n\tfor k := range s.GetAll() {\n\t\tkInt, err := strconv.Atoi(k)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"failed to convert %s into int: %v\", k, err)\n\t\t}\n\t\tif kInt < store.ShrinkThreshold {\n\t\t\tt.Fatalf(\"Key %q should have been deleted\", k)\n\t\t}\n\t}\n}\n\n```"}]}
{"messages": [{"role": "system", "content": "You are an AI agent expert in writing unit tests. Your task is to write unit tests for the given code files of the repository. Make sure the tests can be executed without lint or compile errors."}, {"role": "user", "content": "### Task Information\nBased on the source code, write/rewrite tests to cover the source code.\nRepository: pocketbase\nSource File Path(s): ['pocketbase\\\\tagged_test.go\\\\source_files\\\\tagged.go']\nTest File Path: pocketbase\\tagged_test.go\\tagged_test.go\nProgramming Language: Go\nTesting Framework: go testing\n### Source File: pocketbase\\tagged_test.go\\source_files\\tagged.go\npackage hook\n\nimport (\n\t\"github.com/pocketbase/pocketbase/tools/list\"\n)\n\n// Tagger defines an interface for event data structs that support tags/groups/categories/etc.\n// Usually used together with TaggedHook.\ntype Tagger interface {\n\tResolver\n\n\tTags() []string\n}\n\n// wrapped local Hook embedded struct to limit the public API surface.\ntype mainHook[T Tagger] struct {\n\t*Hook[T]\n}\n\n// NewTaggedHook creates a new TaggedHook with the provided main hook and optional tags.\nfunc NewTaggedHook[T Tagger](hook *Hook[T], tags ...string) *TaggedHook[T] {\n\treturn &TaggedHook[T]{\n\t\tmainHook[T]{hook},\n\t\ttags,\n\t}\n}\n\n// TaggedHook defines a proxy hook which register handlers that are triggered only\n// if the TaggedHook.tags are empty or includes at least one of the event data tag(s).\ntype TaggedHook[T Tagger] struct {\n\tmainHook[T]\n\n\ttags []string\n}\n\n// CanTriggerOn checks if the current TaggedHook can be triggered with\n// the provided event data tags.\n//\n// It returns always true if the hook doens't have any tags.\nfunc (h *TaggedHook[T]) CanTriggerOn(tagsToCheck []string) bool {\n\tif len(h.tags) == 0 {\n\t\treturn true // match all\n\t}\n\n\tfor _, t := range tagsToCheck {\n\t\tif list.ExistInSlice(t, h.tags) {\n\t\t\treturn true\n\t\t}\n\t}\n\n\treturn false\n}\n\n// Bind registers the provided handler to the current hooks queue.\n//\n// It is similar to [Hook.Bind] with the difference that the handler\n// function is invoked only if the event data tags satisfy h.CanTriggerOn.\nfunc (h *TaggedHook[T]) Bind(handler *Handler[T]) string {\n\tfn := handler.Func\n\n\thandler.Func = func(e T) error {\n\t\tif h.CanTriggerOn(e.Tags()) {\n\t\t\treturn fn(e)\n\t\t}\n\n\t\treturn e.Next()\n\t}\n\n\treturn h.mainHook.Bind(handler)\n}\n\n// BindFunc registers a new handler with the specified function.\n//\n// It is similar to [Hook.Bind] with the difference that the handler\n// function is invoked only if the event data tags satisfy h.CanTriggerOn.\nfunc (h *TaggedHook[T]) BindFunc(fn func(e T) error) string {\n\treturn h.mainHook.BindFunc(func(e T) error {\n\t\tif h.CanTriggerOn(e.Tags()) {\n\t\t\treturn fn(e)\n\t\t}\n\n\t\treturn e.Next()\n\t})\n}\n\n### Source File Dependency Files Content\n### Dependency File: pocketbase\\tagged_test.go\\dependent_files\\list.go\npackage list\n\nimport (\n\t\"encoding/json\"\n\t\"regexp\"\n\t\"strings\"\n\n\t\"github.com/pocketbase/pocketbase/tools/store\"\n\t\"github.com/spf13/cast\"\n)\n\nvar cachedPatterns = store.New[string, *regexp.Regexp](nil)\n\n// SubtractSlice returns a new slice with only the \"base\" elements\n// that don't exist in \"subtract\".\nfunc SubtractSlice[T comparable](base []T, subtract []T) []T {\n\tvar result = make([]T, 0, len(base))\n\n\tfor _, b := range base {\n\t\tif !ExistInSlice(b, subtract) {\n\t\t\tresult = append(result, b)\n\t\t}\n\t}\n\n\treturn result\n}\n\n// ExistInSlice checks whether a comparable element exists in a slice of the same type.\nfunc ExistInSlice[T comparable](item T, list []T) bool {\n\tfor _, v := range list {\n\t\tif v == item {\n\t\t\treturn true\n\t\t}\n\t}\n\n\treturn false\n}\n\n// ExistInSliceWithRegex checks whether a string exists in a slice\n// either by direct match, or by a regular expression (eg. `^\\w+$`).\n//\n// Note: Only list items starting with '^' and ending with '$' are treated as regular expressions!\nfunc ExistInSliceWithRegex(str string, list []string) bool {\n\tfor _, field := range list {\n\t\tisRegex := strings.HasPrefix(field, \"^\") && strings.HasSuffix(field, \"$\")\n\n\t\tif !isRegex {\n\t\t\t// check for direct match\n\t\t\tif str == field {\n\t\t\t\treturn true\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\t// check for regex match\n\t\tpattern := cachedPatterns.Get(field)\n\t\tif pattern == nil {\n\t\t\tvar err error\n\t\t\tpattern, err = regexp.Compile(field)\n\t\t\tif err != nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// \"cache\" the pattern to avoid compiling it every time\n\t\t\t// (the limit size is arbitrary and it is there to prevent the cache growing too big)\n\t\t\t//\n\t\t\t// @todo consider replacing with TTL or LRU type cache\n\t\t\tcachedPatterns.SetIfLessThanLimit(field, pattern, 500)\n\t\t}\n\n\t\tif pattern != nil && pattern.MatchString(str) {\n\t\t\treturn true\n\t\t}\n\t}\n\n\treturn false\n}\n\n// ToInterfaceSlice converts a generic slice to slice of interfaces.\nfunc ToInterfaceSlice[T any](list []T) []any {\n\tresult := make([]any, len(list))\n\n\tfor i := range list {\n\t\tresult[i] = list[i]\n\t}\n\n\treturn result\n}\n\n// NonzeroUniques returns only the nonzero unique values from a slice.\nfunc NonzeroUniques[T comparable](list []T) []T {\n\tresult := make([]T, 0, len(list))\n\texistMap := make(map[T]struct{}, len(list))\n\n\tvar zeroVal T\n\n\tfor _, val := range list {\n\t\tif val == zeroVal {\n\t\t\tcontinue\n\t\t}\n\t\tif _, ok := existMap[val]; ok {\n\t\t\tcontinue\n\t\t}\n\t\texistMap[val] = struct{}{}\n\t\tresult = append(result, val)\n\t}\n\n\treturn result\n}\n\n// ToUniqueStringSlice casts `value` to a slice of non-zero unique strings.\nfunc ToUniqueStringSlice(value any) (result []string) {\n\tswitch val := value.(type) {\n\tcase nil:\n\t\t// nothing to cast\n\tcase []string:\n\t\tresult = val\n\tcase string:\n\t\tif val == \"\" {\n\t\t\tbreak\n\t\t}\n\n\t\t// check if it is a json encoded array of strings\n\t\tif strings.Contains(val, \"[\") {\n\t\t\tif err := json.Unmarshal([]byte(val), &result); err != nil {\n\t\t\t\t// not a json array, just add the string as single array element\n\t\t\t\tresult = append(result, val)\n\t\t\t}\n\t\t} else {\n\t\t\t// just add the string as single array element\n\t\t\tresult = append(result, val)\n\t\t}\n\tcase json.Marshaler: // eg. JSONArray\n\t\traw, _ := val.MarshalJSON()\n\t\t_ = json.Unmarshal(raw, &result)\n\tdefault:\n\t\tresult = cast.ToStringSlice(value)\n\t}\n\n\treturn NonzeroUniques(result)\n}\n\n// ToChunks splits list into chunks.\n//\n// Zero or negative chunkSize argument is normalized to 1.\n//\n// See https://go.dev/wiki/SliceTricks#batching-with-minimal-allocation.\nfunc ToChunks[T any](list []T, chunkSize int) [][]T {\n\tif chunkSize <= 0 {\n\t\tchunkSize = 1\n\t}\n\n\tchunks := make([][]T, 0, (len(list)+chunkSize-1)/chunkSize)\n\n\tif len(list) == 0 {\n\t\treturn chunks\n\t}\n\n\tfor chunkSize < len(list) {\n\t\tlist, chunks = list[chunkSize:], append(chunks, list[0:chunkSize:chunkSize])\n\t}\n\n\treturn append(chunks, list)\n}\n\nOutput the complete test file, code only, no explanations.\n### Time\nCurrent time: 2025-03-22 20:52:20"}, {"role": "assistant", "content": "```go\npackage hook\n\nimport (\n\t\"strings\"\n\t\"testing\"\n)\n\ntype mockTagsEvent struct {\n\tEvent\n\ttags []string\n}\n\nfunc (m mockTagsEvent) Tags() []string {\n\treturn m.tags\n}\n\nfunc TestTaggedHook(t *testing.T) {\n\tcalls := \"\"\n\n\tbase := &Hook[*mockTagsEvent]{}\n\tbase.BindFunc(func(e *mockTagsEvent) error { calls += \"f0\"; return e.Next() })\n\n\thA := NewTaggedHook(base)\n\thA.BindFunc(func(e *mockTagsEvent) error { calls += \"a1\"; return e.Next() })\n\thA.Bind(&Handler[*mockTagsEvent]{\n\t\tFunc:     func(e *mockTagsEvent) error { calls += \"a2\"; return e.Next() },\n\t\tPriority: -1,\n\t})\n\n\thB := NewTaggedHook(base, \"b1\", \"b2\")\n\thB.BindFunc(func(e *mockTagsEvent) error { calls += \"b1\"; return e.Next() })\n\thB.Bind(&Handler[*mockTagsEvent]{\n\t\tFunc:     func(e *mockTagsEvent) error { calls += \"b2\"; return e.Next() },\n\t\tPriority: -2,\n\t})\n\n\thC := NewTaggedHook(base, \"c1\", \"c2\")\n\thC.BindFunc(func(e *mockTagsEvent) error { calls += \"c1\"; return e.Next() })\n\thC.Bind(&Handler[*mockTagsEvent]{\n\t\tFunc:     func(e *mockTagsEvent) error { calls += \"c2\"; return e.Next() },\n\t\tPriority: -3,\n\t})\n\n\tscenarios := []struct {\n\t\tevent         *mockTagsEvent\n\t\texpectedCalls string\n\t}{\n\t\t{\n\t\t\t&mockTagsEvent{},\n\t\t\t\"a2f0a1\",\n\t\t},\n\t\t{\n\t\t\t&mockTagsEvent{tags: []string{\"missing\"}},\n\t\t\t\"a2f0a1\",\n\t\t},\n\t\t{\n\t\t\t&mockTagsEvent{tags: []string{\"b2\"}},\n\t\t\t\"b2a2f0a1b1\",\n\t\t},\n\t\t{\n\t\t\t&mockTagsEvent{tags: []string{\"c1\"}},\n\t\t\t\"c2a2f0a1c1\",\n\t\t},\n\t\t{\n\t\t\t&mockTagsEvent{tags: []string{\"b1\", \"c2\"}},\n\t\t\t\"c2b2a2f0a1b1c1\",\n\t\t},\n\t}\n\n\tfor _, s := range scenarios {\n\t\tt.Run(strings.Join(s.event.tags, \"_\"), func(t *testing.T) {\n\t\t\tcalls = \"\" // reset\n\n\t\t\terr := base.Trigger(s.event)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Unexpected trigger error: %v\", err)\n\t\t\t}\n\n\t\t\tif calls != s.expectedCalls {\n\t\t\t\tt.Fatalf(\"Expected calls sequence %q, got %q\", s.expectedCalls, calls)\n\t\t\t}\n\t\t})\n\t}\n}\n\n```"}]}
